{
  "metadata": {
    "version": "2.0",
    "created": "2026-01-04T04:02:40.232868",
    "description": "Het-Benchmark Model Dataset with Complete Operator Data",
    "total_models": 34,
    "total_operators": 6244
  },
  "models": [
    {
      "model_id": "Qwen2.5_7B",
      "name": "Qwen2.5-7B",
      "source": "huggingface/qwen2.5-7b",
      "category": "LLM",
      "architecture": "llm_decoder",
      "num_params": 7000000000,
      "num_layers": 28,
      "hidden_size": 3584,
      "vocab_size": 151936,
      "intermediate_size": 18944,
      "operator_count": 339,
      "operators": [
        {
          "op_id": "Qwen2.5_7B_op_1",
          "type": "Embedding",
          "name": "embed_tokens",
          "layer": "global",
          "parameters": 544538624,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_2",
          "type": "RMSNorm",
          "name": "norm",
          "layer": "global",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_3",
          "type": "Linear",
          "name": "lm_head",
          "layer": "global",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_4",
          "type": "Linear",
          "name": "layer0_q_proj",
          "layer": "layer_0",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_5",
          "type": "Linear",
          "name": "layer0_k_proj",
          "layer": "layer_0",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_6",
          "type": "Linear",
          "name": "layer0_v_proj",
          "layer": "layer_0",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_7",
          "type": "Linear",
          "name": "layer0_o_proj",
          "layer": "layer_0",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_8",
          "type": "Linear",
          "name": "layer0_gate_proj",
          "layer": "layer_0",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_9",
          "type": "Linear",
          "name": "layer0_up_proj",
          "layer": "layer_0",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_10",
          "type": "Linear",
          "name": "layer0_down_proj",
          "layer": "layer_0",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_11",
          "type": "RMSNorm",
          "name": "layer0_input_layernorm",
          "layer": "layer_0",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_12",
          "type": "RMSNorm",
          "name": "layer0_post_attention_layernorm",
          "layer": "layer_0",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_13",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_14",
          "type": "SiLU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_15",
          "type": "Dropout",
          "name": "layer0_attention_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_16",
          "type": "Linear",
          "name": "layer1_q_proj",
          "layer": "layer_1",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_17",
          "type": "Linear",
          "name": "layer1_k_proj",
          "layer": "layer_1",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_18",
          "type": "Linear",
          "name": "layer1_v_proj",
          "layer": "layer_1",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_19",
          "type": "Linear",
          "name": "layer1_o_proj",
          "layer": "layer_1",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_20",
          "type": "Linear",
          "name": "layer1_gate_proj",
          "layer": "layer_1",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_21",
          "type": "Linear",
          "name": "layer1_up_proj",
          "layer": "layer_1",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_22",
          "type": "Linear",
          "name": "layer1_down_proj",
          "layer": "layer_1",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_23",
          "type": "RMSNorm",
          "name": "layer1_input_layernorm",
          "layer": "layer_1",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_24",
          "type": "RMSNorm",
          "name": "layer1_post_attention_layernorm",
          "layer": "layer_1",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_25",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_26",
          "type": "SiLU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_27",
          "type": "Dropout",
          "name": "layer1_attention_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_28",
          "type": "Linear",
          "name": "layer2_q_proj",
          "layer": "layer_2",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_29",
          "type": "Linear",
          "name": "layer2_k_proj",
          "layer": "layer_2",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_30",
          "type": "Linear",
          "name": "layer2_v_proj",
          "layer": "layer_2",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_31",
          "type": "Linear",
          "name": "layer2_o_proj",
          "layer": "layer_2",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_32",
          "type": "Linear",
          "name": "layer2_gate_proj",
          "layer": "layer_2",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_33",
          "type": "Linear",
          "name": "layer2_up_proj",
          "layer": "layer_2",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_34",
          "type": "Linear",
          "name": "layer2_down_proj",
          "layer": "layer_2",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_35",
          "type": "RMSNorm",
          "name": "layer2_input_layernorm",
          "layer": "layer_2",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_36",
          "type": "RMSNorm",
          "name": "layer2_post_attention_layernorm",
          "layer": "layer_2",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_37",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_38",
          "type": "SiLU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_39",
          "type": "Dropout",
          "name": "layer2_attention_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_40",
          "type": "Linear",
          "name": "layer3_q_proj",
          "layer": "layer_3",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_41",
          "type": "Linear",
          "name": "layer3_k_proj",
          "layer": "layer_3",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_42",
          "type": "Linear",
          "name": "layer3_v_proj",
          "layer": "layer_3",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_43",
          "type": "Linear",
          "name": "layer3_o_proj",
          "layer": "layer_3",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_44",
          "type": "Linear",
          "name": "layer3_gate_proj",
          "layer": "layer_3",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_45",
          "type": "Linear",
          "name": "layer3_up_proj",
          "layer": "layer_3",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_46",
          "type": "Linear",
          "name": "layer3_down_proj",
          "layer": "layer_3",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_47",
          "type": "RMSNorm",
          "name": "layer3_input_layernorm",
          "layer": "layer_3",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_48",
          "type": "RMSNorm",
          "name": "layer3_post_attention_layernorm",
          "layer": "layer_3",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_49",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_50",
          "type": "SiLU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_51",
          "type": "Dropout",
          "name": "layer3_attention_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_52",
          "type": "Linear",
          "name": "layer4_q_proj",
          "layer": "layer_4",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_53",
          "type": "Linear",
          "name": "layer4_k_proj",
          "layer": "layer_4",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_54",
          "type": "Linear",
          "name": "layer4_v_proj",
          "layer": "layer_4",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_55",
          "type": "Linear",
          "name": "layer4_o_proj",
          "layer": "layer_4",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_56",
          "type": "Linear",
          "name": "layer4_gate_proj",
          "layer": "layer_4",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_57",
          "type": "Linear",
          "name": "layer4_up_proj",
          "layer": "layer_4",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_58",
          "type": "Linear",
          "name": "layer4_down_proj",
          "layer": "layer_4",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_59",
          "type": "RMSNorm",
          "name": "layer4_input_layernorm",
          "layer": "layer_4",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_60",
          "type": "RMSNorm",
          "name": "layer4_post_attention_layernorm",
          "layer": "layer_4",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_61",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_62",
          "type": "SiLU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_63",
          "type": "Dropout",
          "name": "layer4_attention_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_64",
          "type": "Linear",
          "name": "layer5_q_proj",
          "layer": "layer_5",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_65",
          "type": "Linear",
          "name": "layer5_k_proj",
          "layer": "layer_5",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_66",
          "type": "Linear",
          "name": "layer5_v_proj",
          "layer": "layer_5",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_67",
          "type": "Linear",
          "name": "layer5_o_proj",
          "layer": "layer_5",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_68",
          "type": "Linear",
          "name": "layer5_gate_proj",
          "layer": "layer_5",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_69",
          "type": "Linear",
          "name": "layer5_up_proj",
          "layer": "layer_5",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_70",
          "type": "Linear",
          "name": "layer5_down_proj",
          "layer": "layer_5",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_71",
          "type": "RMSNorm",
          "name": "layer5_input_layernorm",
          "layer": "layer_5",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_72",
          "type": "RMSNorm",
          "name": "layer5_post_attention_layernorm",
          "layer": "layer_5",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_73",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_74",
          "type": "SiLU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_75",
          "type": "Dropout",
          "name": "layer5_attention_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_76",
          "type": "Linear",
          "name": "layer6_q_proj",
          "layer": "layer_6",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_77",
          "type": "Linear",
          "name": "layer6_k_proj",
          "layer": "layer_6",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_78",
          "type": "Linear",
          "name": "layer6_v_proj",
          "layer": "layer_6",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_79",
          "type": "Linear",
          "name": "layer6_o_proj",
          "layer": "layer_6",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_80",
          "type": "Linear",
          "name": "layer6_gate_proj",
          "layer": "layer_6",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_81",
          "type": "Linear",
          "name": "layer6_up_proj",
          "layer": "layer_6",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_82",
          "type": "Linear",
          "name": "layer6_down_proj",
          "layer": "layer_6",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_83",
          "type": "RMSNorm",
          "name": "layer6_input_layernorm",
          "layer": "layer_6",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_84",
          "type": "RMSNorm",
          "name": "layer6_post_attention_layernorm",
          "layer": "layer_6",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_85",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_86",
          "type": "SiLU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_87",
          "type": "Dropout",
          "name": "layer6_attention_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_88",
          "type": "Linear",
          "name": "layer7_q_proj",
          "layer": "layer_7",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_89",
          "type": "Linear",
          "name": "layer7_k_proj",
          "layer": "layer_7",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_90",
          "type": "Linear",
          "name": "layer7_v_proj",
          "layer": "layer_7",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_91",
          "type": "Linear",
          "name": "layer7_o_proj",
          "layer": "layer_7",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_92",
          "type": "Linear",
          "name": "layer7_gate_proj",
          "layer": "layer_7",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_93",
          "type": "Linear",
          "name": "layer7_up_proj",
          "layer": "layer_7",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_94",
          "type": "Linear",
          "name": "layer7_down_proj",
          "layer": "layer_7",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_95",
          "type": "RMSNorm",
          "name": "layer7_input_layernorm",
          "layer": "layer_7",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_96",
          "type": "RMSNorm",
          "name": "layer7_post_attention_layernorm",
          "layer": "layer_7",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_97",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_98",
          "type": "SiLU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_99",
          "type": "Dropout",
          "name": "layer7_attention_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_100",
          "type": "Linear",
          "name": "layer8_q_proj",
          "layer": "layer_8",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_101",
          "type": "Linear",
          "name": "layer8_k_proj",
          "layer": "layer_8",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_102",
          "type": "Linear",
          "name": "layer8_v_proj",
          "layer": "layer_8",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_103",
          "type": "Linear",
          "name": "layer8_o_proj",
          "layer": "layer_8",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_104",
          "type": "Linear",
          "name": "layer8_gate_proj",
          "layer": "layer_8",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_105",
          "type": "Linear",
          "name": "layer8_up_proj",
          "layer": "layer_8",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_106",
          "type": "Linear",
          "name": "layer8_down_proj",
          "layer": "layer_8",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_107",
          "type": "RMSNorm",
          "name": "layer8_input_layernorm",
          "layer": "layer_8",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_108",
          "type": "RMSNorm",
          "name": "layer8_post_attention_layernorm",
          "layer": "layer_8",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_109",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_110",
          "type": "SiLU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_111",
          "type": "Dropout",
          "name": "layer8_attention_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_112",
          "type": "Linear",
          "name": "layer9_q_proj",
          "layer": "layer_9",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_113",
          "type": "Linear",
          "name": "layer9_k_proj",
          "layer": "layer_9",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_114",
          "type": "Linear",
          "name": "layer9_v_proj",
          "layer": "layer_9",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_115",
          "type": "Linear",
          "name": "layer9_o_proj",
          "layer": "layer_9",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_116",
          "type": "Linear",
          "name": "layer9_gate_proj",
          "layer": "layer_9",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_117",
          "type": "Linear",
          "name": "layer9_up_proj",
          "layer": "layer_9",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_118",
          "type": "Linear",
          "name": "layer9_down_proj",
          "layer": "layer_9",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_119",
          "type": "RMSNorm",
          "name": "layer9_input_layernorm",
          "layer": "layer_9",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_120",
          "type": "RMSNorm",
          "name": "layer9_post_attention_layernorm",
          "layer": "layer_9",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_121",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_122",
          "type": "SiLU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_123",
          "type": "Dropout",
          "name": "layer9_attention_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_124",
          "type": "Linear",
          "name": "layer10_q_proj",
          "layer": "layer_10",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_125",
          "type": "Linear",
          "name": "layer10_k_proj",
          "layer": "layer_10",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_126",
          "type": "Linear",
          "name": "layer10_v_proj",
          "layer": "layer_10",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_127",
          "type": "Linear",
          "name": "layer10_o_proj",
          "layer": "layer_10",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_128",
          "type": "Linear",
          "name": "layer10_gate_proj",
          "layer": "layer_10",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_129",
          "type": "Linear",
          "name": "layer10_up_proj",
          "layer": "layer_10",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_130",
          "type": "Linear",
          "name": "layer10_down_proj",
          "layer": "layer_10",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_131",
          "type": "RMSNorm",
          "name": "layer10_input_layernorm",
          "layer": "layer_10",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_132",
          "type": "RMSNorm",
          "name": "layer10_post_attention_layernorm",
          "layer": "layer_10",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_133",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_134",
          "type": "SiLU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_135",
          "type": "Dropout",
          "name": "layer10_attention_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_136",
          "type": "Linear",
          "name": "layer11_q_proj",
          "layer": "layer_11",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_137",
          "type": "Linear",
          "name": "layer11_k_proj",
          "layer": "layer_11",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_138",
          "type": "Linear",
          "name": "layer11_v_proj",
          "layer": "layer_11",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_139",
          "type": "Linear",
          "name": "layer11_o_proj",
          "layer": "layer_11",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_140",
          "type": "Linear",
          "name": "layer11_gate_proj",
          "layer": "layer_11",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_141",
          "type": "Linear",
          "name": "layer11_up_proj",
          "layer": "layer_11",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_142",
          "type": "Linear",
          "name": "layer11_down_proj",
          "layer": "layer_11",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_143",
          "type": "RMSNorm",
          "name": "layer11_input_layernorm",
          "layer": "layer_11",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_144",
          "type": "RMSNorm",
          "name": "layer11_post_attention_layernorm",
          "layer": "layer_11",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_145",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_146",
          "type": "SiLU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_147",
          "type": "Dropout",
          "name": "layer11_attention_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_148",
          "type": "Linear",
          "name": "layer12_q_proj",
          "layer": "layer_12",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_149",
          "type": "Linear",
          "name": "layer12_k_proj",
          "layer": "layer_12",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_150",
          "type": "Linear",
          "name": "layer12_v_proj",
          "layer": "layer_12",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_151",
          "type": "Linear",
          "name": "layer12_o_proj",
          "layer": "layer_12",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_152",
          "type": "Linear",
          "name": "layer12_gate_proj",
          "layer": "layer_12",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_153",
          "type": "Linear",
          "name": "layer12_up_proj",
          "layer": "layer_12",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_154",
          "type": "Linear",
          "name": "layer12_down_proj",
          "layer": "layer_12",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_155",
          "type": "RMSNorm",
          "name": "layer12_input_layernorm",
          "layer": "layer_12",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_156",
          "type": "RMSNorm",
          "name": "layer12_post_attention_layernorm",
          "layer": "layer_12",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_157",
          "type": "Softmax",
          "name": "layer12_attention_softmax",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_158",
          "type": "SiLU",
          "name": "layer12_activation",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_159",
          "type": "Dropout",
          "name": "layer12_attention_dropout",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_160",
          "type": "Linear",
          "name": "layer13_q_proj",
          "layer": "layer_13",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_161",
          "type": "Linear",
          "name": "layer13_k_proj",
          "layer": "layer_13",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_162",
          "type": "Linear",
          "name": "layer13_v_proj",
          "layer": "layer_13",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_163",
          "type": "Linear",
          "name": "layer13_o_proj",
          "layer": "layer_13",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_164",
          "type": "Linear",
          "name": "layer13_gate_proj",
          "layer": "layer_13",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_165",
          "type": "Linear",
          "name": "layer13_up_proj",
          "layer": "layer_13",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_166",
          "type": "Linear",
          "name": "layer13_down_proj",
          "layer": "layer_13",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_167",
          "type": "RMSNorm",
          "name": "layer13_input_layernorm",
          "layer": "layer_13",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_168",
          "type": "RMSNorm",
          "name": "layer13_post_attention_layernorm",
          "layer": "layer_13",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_169",
          "type": "Softmax",
          "name": "layer13_attention_softmax",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_170",
          "type": "SiLU",
          "name": "layer13_activation",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_171",
          "type": "Dropout",
          "name": "layer13_attention_dropout",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_172",
          "type": "Linear",
          "name": "layer14_q_proj",
          "layer": "layer_14",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_173",
          "type": "Linear",
          "name": "layer14_k_proj",
          "layer": "layer_14",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_174",
          "type": "Linear",
          "name": "layer14_v_proj",
          "layer": "layer_14",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_175",
          "type": "Linear",
          "name": "layer14_o_proj",
          "layer": "layer_14",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_176",
          "type": "Linear",
          "name": "layer14_gate_proj",
          "layer": "layer_14",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_177",
          "type": "Linear",
          "name": "layer14_up_proj",
          "layer": "layer_14",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_178",
          "type": "Linear",
          "name": "layer14_down_proj",
          "layer": "layer_14",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_179",
          "type": "RMSNorm",
          "name": "layer14_input_layernorm",
          "layer": "layer_14",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_180",
          "type": "RMSNorm",
          "name": "layer14_post_attention_layernorm",
          "layer": "layer_14",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_181",
          "type": "Softmax",
          "name": "layer14_attention_softmax",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_182",
          "type": "SiLU",
          "name": "layer14_activation",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_183",
          "type": "Dropout",
          "name": "layer14_attention_dropout",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_184",
          "type": "Linear",
          "name": "layer15_q_proj",
          "layer": "layer_15",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_185",
          "type": "Linear",
          "name": "layer15_k_proj",
          "layer": "layer_15",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_186",
          "type": "Linear",
          "name": "layer15_v_proj",
          "layer": "layer_15",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_187",
          "type": "Linear",
          "name": "layer15_o_proj",
          "layer": "layer_15",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_188",
          "type": "Linear",
          "name": "layer15_gate_proj",
          "layer": "layer_15",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_189",
          "type": "Linear",
          "name": "layer15_up_proj",
          "layer": "layer_15",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_190",
          "type": "Linear",
          "name": "layer15_down_proj",
          "layer": "layer_15",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_191",
          "type": "RMSNorm",
          "name": "layer15_input_layernorm",
          "layer": "layer_15",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_192",
          "type": "RMSNorm",
          "name": "layer15_post_attention_layernorm",
          "layer": "layer_15",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_193",
          "type": "Softmax",
          "name": "layer15_attention_softmax",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_194",
          "type": "SiLU",
          "name": "layer15_activation",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_195",
          "type": "Dropout",
          "name": "layer15_attention_dropout",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_196",
          "type": "Linear",
          "name": "layer16_q_proj",
          "layer": "layer_16",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_197",
          "type": "Linear",
          "name": "layer16_k_proj",
          "layer": "layer_16",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_198",
          "type": "Linear",
          "name": "layer16_v_proj",
          "layer": "layer_16",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_199",
          "type": "Linear",
          "name": "layer16_o_proj",
          "layer": "layer_16",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_200",
          "type": "Linear",
          "name": "layer16_gate_proj",
          "layer": "layer_16",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_201",
          "type": "Linear",
          "name": "layer16_up_proj",
          "layer": "layer_16",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_202",
          "type": "Linear",
          "name": "layer16_down_proj",
          "layer": "layer_16",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_203",
          "type": "RMSNorm",
          "name": "layer16_input_layernorm",
          "layer": "layer_16",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_204",
          "type": "RMSNorm",
          "name": "layer16_post_attention_layernorm",
          "layer": "layer_16",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_205",
          "type": "Softmax",
          "name": "layer16_attention_softmax",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_206",
          "type": "SiLU",
          "name": "layer16_activation",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_207",
          "type": "Dropout",
          "name": "layer16_attention_dropout",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_208",
          "type": "Linear",
          "name": "layer17_q_proj",
          "layer": "layer_17",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_209",
          "type": "Linear",
          "name": "layer17_k_proj",
          "layer": "layer_17",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_210",
          "type": "Linear",
          "name": "layer17_v_proj",
          "layer": "layer_17",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_211",
          "type": "Linear",
          "name": "layer17_o_proj",
          "layer": "layer_17",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_212",
          "type": "Linear",
          "name": "layer17_gate_proj",
          "layer": "layer_17",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_213",
          "type": "Linear",
          "name": "layer17_up_proj",
          "layer": "layer_17",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_214",
          "type": "Linear",
          "name": "layer17_down_proj",
          "layer": "layer_17",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_215",
          "type": "RMSNorm",
          "name": "layer17_input_layernorm",
          "layer": "layer_17",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_216",
          "type": "RMSNorm",
          "name": "layer17_post_attention_layernorm",
          "layer": "layer_17",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_217",
          "type": "Softmax",
          "name": "layer17_attention_softmax",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_218",
          "type": "SiLU",
          "name": "layer17_activation",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_219",
          "type": "Dropout",
          "name": "layer17_attention_dropout",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_220",
          "type": "Linear",
          "name": "layer18_q_proj",
          "layer": "layer_18",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_221",
          "type": "Linear",
          "name": "layer18_k_proj",
          "layer": "layer_18",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_222",
          "type": "Linear",
          "name": "layer18_v_proj",
          "layer": "layer_18",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_223",
          "type": "Linear",
          "name": "layer18_o_proj",
          "layer": "layer_18",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_224",
          "type": "Linear",
          "name": "layer18_gate_proj",
          "layer": "layer_18",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_225",
          "type": "Linear",
          "name": "layer18_up_proj",
          "layer": "layer_18",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_226",
          "type": "Linear",
          "name": "layer18_down_proj",
          "layer": "layer_18",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_227",
          "type": "RMSNorm",
          "name": "layer18_input_layernorm",
          "layer": "layer_18",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_228",
          "type": "RMSNorm",
          "name": "layer18_post_attention_layernorm",
          "layer": "layer_18",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_229",
          "type": "Softmax",
          "name": "layer18_attention_softmax",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_230",
          "type": "SiLU",
          "name": "layer18_activation",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_231",
          "type": "Dropout",
          "name": "layer18_attention_dropout",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_232",
          "type": "Linear",
          "name": "layer19_q_proj",
          "layer": "layer_19",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_233",
          "type": "Linear",
          "name": "layer19_k_proj",
          "layer": "layer_19",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_234",
          "type": "Linear",
          "name": "layer19_v_proj",
          "layer": "layer_19",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_235",
          "type": "Linear",
          "name": "layer19_o_proj",
          "layer": "layer_19",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_236",
          "type": "Linear",
          "name": "layer19_gate_proj",
          "layer": "layer_19",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_237",
          "type": "Linear",
          "name": "layer19_up_proj",
          "layer": "layer_19",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_238",
          "type": "Linear",
          "name": "layer19_down_proj",
          "layer": "layer_19",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_239",
          "type": "RMSNorm",
          "name": "layer19_input_layernorm",
          "layer": "layer_19",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_240",
          "type": "RMSNorm",
          "name": "layer19_post_attention_layernorm",
          "layer": "layer_19",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_241",
          "type": "Softmax",
          "name": "layer19_attention_softmax",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_242",
          "type": "SiLU",
          "name": "layer19_activation",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_243",
          "type": "Dropout",
          "name": "layer19_attention_dropout",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_244",
          "type": "Linear",
          "name": "layer20_q_proj",
          "layer": "layer_20",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_245",
          "type": "Linear",
          "name": "layer20_k_proj",
          "layer": "layer_20",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_246",
          "type": "Linear",
          "name": "layer20_v_proj",
          "layer": "layer_20",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_247",
          "type": "Linear",
          "name": "layer20_o_proj",
          "layer": "layer_20",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_248",
          "type": "Linear",
          "name": "layer20_gate_proj",
          "layer": "layer_20",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_249",
          "type": "Linear",
          "name": "layer20_up_proj",
          "layer": "layer_20",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_250",
          "type": "Linear",
          "name": "layer20_down_proj",
          "layer": "layer_20",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_251",
          "type": "RMSNorm",
          "name": "layer20_input_layernorm",
          "layer": "layer_20",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_252",
          "type": "RMSNorm",
          "name": "layer20_post_attention_layernorm",
          "layer": "layer_20",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_253",
          "type": "Softmax",
          "name": "layer20_attention_softmax",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_254",
          "type": "SiLU",
          "name": "layer20_activation",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_255",
          "type": "Dropout",
          "name": "layer20_attention_dropout",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_256",
          "type": "Linear",
          "name": "layer21_q_proj",
          "layer": "layer_21",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_257",
          "type": "Linear",
          "name": "layer21_k_proj",
          "layer": "layer_21",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_258",
          "type": "Linear",
          "name": "layer21_v_proj",
          "layer": "layer_21",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_259",
          "type": "Linear",
          "name": "layer21_o_proj",
          "layer": "layer_21",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_260",
          "type": "Linear",
          "name": "layer21_gate_proj",
          "layer": "layer_21",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_261",
          "type": "Linear",
          "name": "layer21_up_proj",
          "layer": "layer_21",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_262",
          "type": "Linear",
          "name": "layer21_down_proj",
          "layer": "layer_21",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_263",
          "type": "RMSNorm",
          "name": "layer21_input_layernorm",
          "layer": "layer_21",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_264",
          "type": "RMSNorm",
          "name": "layer21_post_attention_layernorm",
          "layer": "layer_21",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_265",
          "type": "Softmax",
          "name": "layer21_attention_softmax",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_266",
          "type": "SiLU",
          "name": "layer21_activation",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_267",
          "type": "Dropout",
          "name": "layer21_attention_dropout",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_268",
          "type": "Linear",
          "name": "layer22_q_proj",
          "layer": "layer_22",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_269",
          "type": "Linear",
          "name": "layer22_k_proj",
          "layer": "layer_22",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_270",
          "type": "Linear",
          "name": "layer22_v_proj",
          "layer": "layer_22",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_271",
          "type": "Linear",
          "name": "layer22_o_proj",
          "layer": "layer_22",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_272",
          "type": "Linear",
          "name": "layer22_gate_proj",
          "layer": "layer_22",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_273",
          "type": "Linear",
          "name": "layer22_up_proj",
          "layer": "layer_22",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_274",
          "type": "Linear",
          "name": "layer22_down_proj",
          "layer": "layer_22",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_275",
          "type": "RMSNorm",
          "name": "layer22_input_layernorm",
          "layer": "layer_22",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_276",
          "type": "RMSNorm",
          "name": "layer22_post_attention_layernorm",
          "layer": "layer_22",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_277",
          "type": "Softmax",
          "name": "layer22_attention_softmax",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_278",
          "type": "SiLU",
          "name": "layer22_activation",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_279",
          "type": "Dropout",
          "name": "layer22_attention_dropout",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_280",
          "type": "Linear",
          "name": "layer23_q_proj",
          "layer": "layer_23",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_281",
          "type": "Linear",
          "name": "layer23_k_proj",
          "layer": "layer_23",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_282",
          "type": "Linear",
          "name": "layer23_v_proj",
          "layer": "layer_23",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_283",
          "type": "Linear",
          "name": "layer23_o_proj",
          "layer": "layer_23",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_284",
          "type": "Linear",
          "name": "layer23_gate_proj",
          "layer": "layer_23",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_285",
          "type": "Linear",
          "name": "layer23_up_proj",
          "layer": "layer_23",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_286",
          "type": "Linear",
          "name": "layer23_down_proj",
          "layer": "layer_23",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_287",
          "type": "RMSNorm",
          "name": "layer23_input_layernorm",
          "layer": "layer_23",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_288",
          "type": "RMSNorm",
          "name": "layer23_post_attention_layernorm",
          "layer": "layer_23",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_289",
          "type": "Softmax",
          "name": "layer23_attention_softmax",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_290",
          "type": "SiLU",
          "name": "layer23_activation",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_291",
          "type": "Dropout",
          "name": "layer23_attention_dropout",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_292",
          "type": "Linear",
          "name": "layer24_q_proj",
          "layer": "layer_24",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_293",
          "type": "Linear",
          "name": "layer24_k_proj",
          "layer": "layer_24",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_294",
          "type": "Linear",
          "name": "layer24_v_proj",
          "layer": "layer_24",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_295",
          "type": "Linear",
          "name": "layer24_o_proj",
          "layer": "layer_24",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_296",
          "type": "Linear",
          "name": "layer24_gate_proj",
          "layer": "layer_24",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_297",
          "type": "Linear",
          "name": "layer24_up_proj",
          "layer": "layer_24",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_298",
          "type": "Linear",
          "name": "layer24_down_proj",
          "layer": "layer_24",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_299",
          "type": "RMSNorm",
          "name": "layer24_input_layernorm",
          "layer": "layer_24",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_300",
          "type": "RMSNorm",
          "name": "layer24_post_attention_layernorm",
          "layer": "layer_24",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_301",
          "type": "Softmax",
          "name": "layer24_attention_softmax",
          "layer": "layer_24",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_302",
          "type": "SiLU",
          "name": "layer24_activation",
          "layer": "layer_24",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_303",
          "type": "Dropout",
          "name": "layer24_attention_dropout",
          "layer": "layer_24",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_304",
          "type": "Linear",
          "name": "layer25_q_proj",
          "layer": "layer_25",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_305",
          "type": "Linear",
          "name": "layer25_k_proj",
          "layer": "layer_25",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_306",
          "type": "Linear",
          "name": "layer25_v_proj",
          "layer": "layer_25",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_307",
          "type": "Linear",
          "name": "layer25_o_proj",
          "layer": "layer_25",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_308",
          "type": "Linear",
          "name": "layer25_gate_proj",
          "layer": "layer_25",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_309",
          "type": "Linear",
          "name": "layer25_up_proj",
          "layer": "layer_25",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_310",
          "type": "Linear",
          "name": "layer25_down_proj",
          "layer": "layer_25",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_311",
          "type": "RMSNorm",
          "name": "layer25_input_layernorm",
          "layer": "layer_25",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_312",
          "type": "RMSNorm",
          "name": "layer25_post_attention_layernorm",
          "layer": "layer_25",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_313",
          "type": "Softmax",
          "name": "layer25_attention_softmax",
          "layer": "layer_25",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_314",
          "type": "SiLU",
          "name": "layer25_activation",
          "layer": "layer_25",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_315",
          "type": "Dropout",
          "name": "layer25_attention_dropout",
          "layer": "layer_25",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_316",
          "type": "Linear",
          "name": "layer26_q_proj",
          "layer": "layer_26",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_317",
          "type": "Linear",
          "name": "layer26_k_proj",
          "layer": "layer_26",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_318",
          "type": "Linear",
          "name": "layer26_v_proj",
          "layer": "layer_26",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_319",
          "type": "Linear",
          "name": "layer26_o_proj",
          "layer": "layer_26",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_320",
          "type": "Linear",
          "name": "layer26_gate_proj",
          "layer": "layer_26",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_321",
          "type": "Linear",
          "name": "layer26_up_proj",
          "layer": "layer_26",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_322",
          "type": "Linear",
          "name": "layer26_down_proj",
          "layer": "layer_26",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_323",
          "type": "RMSNorm",
          "name": "layer26_input_layernorm",
          "layer": "layer_26",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_324",
          "type": "RMSNorm",
          "name": "layer26_post_attention_layernorm",
          "layer": "layer_26",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_325",
          "type": "Softmax",
          "name": "layer26_attention_softmax",
          "layer": "layer_26",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_326",
          "type": "SiLU",
          "name": "layer26_activation",
          "layer": "layer_26",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_327",
          "type": "Dropout",
          "name": "layer26_attention_dropout",
          "layer": "layer_26",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_328",
          "type": "Linear",
          "name": "layer27_q_proj",
          "layer": "layer_27",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_329",
          "type": "Linear",
          "name": "layer27_k_proj",
          "layer": "layer_27",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_330",
          "type": "Linear",
          "name": "layer27_v_proj",
          "layer": "layer_27",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_331",
          "type": "Linear",
          "name": "layer27_o_proj",
          "layer": "layer_27",
          "parameters": 12845056,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_332",
          "type": "Linear",
          "name": "layer27_gate_proj",
          "layer": "layer_27",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_333",
          "type": "Linear",
          "name": "layer27_up_proj",
          "layer": "layer_27",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_334",
          "type": "Linear",
          "name": "layer27_down_proj",
          "layer": "layer_27",
          "parameters": 67895296,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Qwen2.5_7B_op_335",
          "type": "RMSNorm",
          "name": "layer27_input_layernorm",
          "layer": "layer_27",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_336",
          "type": "RMSNorm",
          "name": "layer27_post_attention_layernorm",
          "layer": "layer_27",
          "parameters": 3584,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        },
        {
          "op_id": "Qwen2.5_7B_op_337",
          "type": "Softmax",
          "name": "layer27_attention_softmax",
          "layer": "layer_27",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Qwen2.5_7B_op_338",
          "type": "SiLU",
          "name": "layer27_activation",
          "layer": "layer_27",
          "parameters": 0,
          "input_shape": "[batch, seq, 18944]",
          "output_shape": "[batch, seq, 18944]"
        },
        {
          "op_id": "Qwen2.5_7B_op_339",
          "type": "Dropout",
          "name": "layer27_attention_dropout",
          "layer": "layer_27",
          "parameters": 0,
          "input_shape": "[batch, seq, 3584]",
          "output_shape": "[batch, seq, 3584]"
        }
      ],
      "parameters": 7000000000
    },
    {
      "model_id": "Mistral_7B",
      "name": "Mistral-7B",
      "source": "huggingface/mistral-7b",
      "category": "LLM",
      "architecture": "llm_decoder",
      "num_params": 7000000000,
      "num_layers": 32,
      "hidden_size": 4096,
      "vocab_size": 32000,
      "intermediate_size": 14336,
      "operator_count": 387,
      "operators": [
        {
          "op_id": "Mistral_7B_op_1",
          "type": "Embedding",
          "name": "embed_tokens",
          "layer": "global",
          "parameters": 131072000,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_2",
          "type": "RMSNorm",
          "name": "norm",
          "layer": "global",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_3",
          "type": "Linear",
          "name": "lm_head",
          "layer": "global",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_4",
          "type": "Linear",
          "name": "layer0_q_proj",
          "layer": "layer_0",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_5",
          "type": "Linear",
          "name": "layer0_k_proj",
          "layer": "layer_0",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_6",
          "type": "Linear",
          "name": "layer0_v_proj",
          "layer": "layer_0",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_7",
          "type": "Linear",
          "name": "layer0_o_proj",
          "layer": "layer_0",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_8",
          "type": "Linear",
          "name": "layer0_gate_proj",
          "layer": "layer_0",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_9",
          "type": "Linear",
          "name": "layer0_up_proj",
          "layer": "layer_0",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_10",
          "type": "Linear",
          "name": "layer0_down_proj",
          "layer": "layer_0",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_11",
          "type": "RMSNorm",
          "name": "layer0_input_layernorm",
          "layer": "layer_0",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_12",
          "type": "RMSNorm",
          "name": "layer0_post_attention_layernorm",
          "layer": "layer_0",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_13",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_14",
          "type": "SiLU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_15",
          "type": "Dropout",
          "name": "layer0_attention_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_16",
          "type": "Linear",
          "name": "layer1_q_proj",
          "layer": "layer_1",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_17",
          "type": "Linear",
          "name": "layer1_k_proj",
          "layer": "layer_1",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_18",
          "type": "Linear",
          "name": "layer1_v_proj",
          "layer": "layer_1",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_19",
          "type": "Linear",
          "name": "layer1_o_proj",
          "layer": "layer_1",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_20",
          "type": "Linear",
          "name": "layer1_gate_proj",
          "layer": "layer_1",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_21",
          "type": "Linear",
          "name": "layer1_up_proj",
          "layer": "layer_1",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_22",
          "type": "Linear",
          "name": "layer1_down_proj",
          "layer": "layer_1",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_23",
          "type": "RMSNorm",
          "name": "layer1_input_layernorm",
          "layer": "layer_1",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_24",
          "type": "RMSNorm",
          "name": "layer1_post_attention_layernorm",
          "layer": "layer_1",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_25",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_26",
          "type": "SiLU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_27",
          "type": "Dropout",
          "name": "layer1_attention_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_28",
          "type": "Linear",
          "name": "layer2_q_proj",
          "layer": "layer_2",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_29",
          "type": "Linear",
          "name": "layer2_k_proj",
          "layer": "layer_2",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_30",
          "type": "Linear",
          "name": "layer2_v_proj",
          "layer": "layer_2",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_31",
          "type": "Linear",
          "name": "layer2_o_proj",
          "layer": "layer_2",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_32",
          "type": "Linear",
          "name": "layer2_gate_proj",
          "layer": "layer_2",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_33",
          "type": "Linear",
          "name": "layer2_up_proj",
          "layer": "layer_2",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_34",
          "type": "Linear",
          "name": "layer2_down_proj",
          "layer": "layer_2",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_35",
          "type": "RMSNorm",
          "name": "layer2_input_layernorm",
          "layer": "layer_2",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_36",
          "type": "RMSNorm",
          "name": "layer2_post_attention_layernorm",
          "layer": "layer_2",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_37",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_38",
          "type": "SiLU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_39",
          "type": "Dropout",
          "name": "layer2_attention_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_40",
          "type": "Linear",
          "name": "layer3_q_proj",
          "layer": "layer_3",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_41",
          "type": "Linear",
          "name": "layer3_k_proj",
          "layer": "layer_3",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_42",
          "type": "Linear",
          "name": "layer3_v_proj",
          "layer": "layer_3",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_43",
          "type": "Linear",
          "name": "layer3_o_proj",
          "layer": "layer_3",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_44",
          "type": "Linear",
          "name": "layer3_gate_proj",
          "layer": "layer_3",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_45",
          "type": "Linear",
          "name": "layer3_up_proj",
          "layer": "layer_3",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_46",
          "type": "Linear",
          "name": "layer3_down_proj",
          "layer": "layer_3",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_47",
          "type": "RMSNorm",
          "name": "layer3_input_layernorm",
          "layer": "layer_3",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_48",
          "type": "RMSNorm",
          "name": "layer3_post_attention_layernorm",
          "layer": "layer_3",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_49",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_50",
          "type": "SiLU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_51",
          "type": "Dropout",
          "name": "layer3_attention_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_52",
          "type": "Linear",
          "name": "layer4_q_proj",
          "layer": "layer_4",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_53",
          "type": "Linear",
          "name": "layer4_k_proj",
          "layer": "layer_4",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_54",
          "type": "Linear",
          "name": "layer4_v_proj",
          "layer": "layer_4",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_55",
          "type": "Linear",
          "name": "layer4_o_proj",
          "layer": "layer_4",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_56",
          "type": "Linear",
          "name": "layer4_gate_proj",
          "layer": "layer_4",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_57",
          "type": "Linear",
          "name": "layer4_up_proj",
          "layer": "layer_4",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_58",
          "type": "Linear",
          "name": "layer4_down_proj",
          "layer": "layer_4",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_59",
          "type": "RMSNorm",
          "name": "layer4_input_layernorm",
          "layer": "layer_4",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_60",
          "type": "RMSNorm",
          "name": "layer4_post_attention_layernorm",
          "layer": "layer_4",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_61",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_62",
          "type": "SiLU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_63",
          "type": "Dropout",
          "name": "layer4_attention_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_64",
          "type": "Linear",
          "name": "layer5_q_proj",
          "layer": "layer_5",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_65",
          "type": "Linear",
          "name": "layer5_k_proj",
          "layer": "layer_5",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_66",
          "type": "Linear",
          "name": "layer5_v_proj",
          "layer": "layer_5",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_67",
          "type": "Linear",
          "name": "layer5_o_proj",
          "layer": "layer_5",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_68",
          "type": "Linear",
          "name": "layer5_gate_proj",
          "layer": "layer_5",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_69",
          "type": "Linear",
          "name": "layer5_up_proj",
          "layer": "layer_5",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_70",
          "type": "Linear",
          "name": "layer5_down_proj",
          "layer": "layer_5",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_71",
          "type": "RMSNorm",
          "name": "layer5_input_layernorm",
          "layer": "layer_5",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_72",
          "type": "RMSNorm",
          "name": "layer5_post_attention_layernorm",
          "layer": "layer_5",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_73",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_74",
          "type": "SiLU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_75",
          "type": "Dropout",
          "name": "layer5_attention_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_76",
          "type": "Linear",
          "name": "layer6_q_proj",
          "layer": "layer_6",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_77",
          "type": "Linear",
          "name": "layer6_k_proj",
          "layer": "layer_6",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_78",
          "type": "Linear",
          "name": "layer6_v_proj",
          "layer": "layer_6",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_79",
          "type": "Linear",
          "name": "layer6_o_proj",
          "layer": "layer_6",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_80",
          "type": "Linear",
          "name": "layer6_gate_proj",
          "layer": "layer_6",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_81",
          "type": "Linear",
          "name": "layer6_up_proj",
          "layer": "layer_6",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_82",
          "type": "Linear",
          "name": "layer6_down_proj",
          "layer": "layer_6",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_83",
          "type": "RMSNorm",
          "name": "layer6_input_layernorm",
          "layer": "layer_6",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_84",
          "type": "RMSNorm",
          "name": "layer6_post_attention_layernorm",
          "layer": "layer_6",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_85",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_86",
          "type": "SiLU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_87",
          "type": "Dropout",
          "name": "layer6_attention_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_88",
          "type": "Linear",
          "name": "layer7_q_proj",
          "layer": "layer_7",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_89",
          "type": "Linear",
          "name": "layer7_k_proj",
          "layer": "layer_7",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_90",
          "type": "Linear",
          "name": "layer7_v_proj",
          "layer": "layer_7",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_91",
          "type": "Linear",
          "name": "layer7_o_proj",
          "layer": "layer_7",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_92",
          "type": "Linear",
          "name": "layer7_gate_proj",
          "layer": "layer_7",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_93",
          "type": "Linear",
          "name": "layer7_up_proj",
          "layer": "layer_7",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_94",
          "type": "Linear",
          "name": "layer7_down_proj",
          "layer": "layer_7",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_95",
          "type": "RMSNorm",
          "name": "layer7_input_layernorm",
          "layer": "layer_7",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_96",
          "type": "RMSNorm",
          "name": "layer7_post_attention_layernorm",
          "layer": "layer_7",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_97",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_98",
          "type": "SiLU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_99",
          "type": "Dropout",
          "name": "layer7_attention_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_100",
          "type": "Linear",
          "name": "layer8_q_proj",
          "layer": "layer_8",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_101",
          "type": "Linear",
          "name": "layer8_k_proj",
          "layer": "layer_8",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_102",
          "type": "Linear",
          "name": "layer8_v_proj",
          "layer": "layer_8",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_103",
          "type": "Linear",
          "name": "layer8_o_proj",
          "layer": "layer_8",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_104",
          "type": "Linear",
          "name": "layer8_gate_proj",
          "layer": "layer_8",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_105",
          "type": "Linear",
          "name": "layer8_up_proj",
          "layer": "layer_8",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_106",
          "type": "Linear",
          "name": "layer8_down_proj",
          "layer": "layer_8",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_107",
          "type": "RMSNorm",
          "name": "layer8_input_layernorm",
          "layer": "layer_8",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_108",
          "type": "RMSNorm",
          "name": "layer8_post_attention_layernorm",
          "layer": "layer_8",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_109",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_110",
          "type": "SiLU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_111",
          "type": "Dropout",
          "name": "layer8_attention_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_112",
          "type": "Linear",
          "name": "layer9_q_proj",
          "layer": "layer_9",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_113",
          "type": "Linear",
          "name": "layer9_k_proj",
          "layer": "layer_9",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_114",
          "type": "Linear",
          "name": "layer9_v_proj",
          "layer": "layer_9",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_115",
          "type": "Linear",
          "name": "layer9_o_proj",
          "layer": "layer_9",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_116",
          "type": "Linear",
          "name": "layer9_gate_proj",
          "layer": "layer_9",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_117",
          "type": "Linear",
          "name": "layer9_up_proj",
          "layer": "layer_9",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_118",
          "type": "Linear",
          "name": "layer9_down_proj",
          "layer": "layer_9",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_119",
          "type": "RMSNorm",
          "name": "layer9_input_layernorm",
          "layer": "layer_9",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_120",
          "type": "RMSNorm",
          "name": "layer9_post_attention_layernorm",
          "layer": "layer_9",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_121",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_122",
          "type": "SiLU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_123",
          "type": "Dropout",
          "name": "layer9_attention_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_124",
          "type": "Linear",
          "name": "layer10_q_proj",
          "layer": "layer_10",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_125",
          "type": "Linear",
          "name": "layer10_k_proj",
          "layer": "layer_10",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_126",
          "type": "Linear",
          "name": "layer10_v_proj",
          "layer": "layer_10",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_127",
          "type": "Linear",
          "name": "layer10_o_proj",
          "layer": "layer_10",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_128",
          "type": "Linear",
          "name": "layer10_gate_proj",
          "layer": "layer_10",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_129",
          "type": "Linear",
          "name": "layer10_up_proj",
          "layer": "layer_10",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_130",
          "type": "Linear",
          "name": "layer10_down_proj",
          "layer": "layer_10",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_131",
          "type": "RMSNorm",
          "name": "layer10_input_layernorm",
          "layer": "layer_10",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_132",
          "type": "RMSNorm",
          "name": "layer10_post_attention_layernorm",
          "layer": "layer_10",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_133",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_134",
          "type": "SiLU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_135",
          "type": "Dropout",
          "name": "layer10_attention_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_136",
          "type": "Linear",
          "name": "layer11_q_proj",
          "layer": "layer_11",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_137",
          "type": "Linear",
          "name": "layer11_k_proj",
          "layer": "layer_11",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_138",
          "type": "Linear",
          "name": "layer11_v_proj",
          "layer": "layer_11",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_139",
          "type": "Linear",
          "name": "layer11_o_proj",
          "layer": "layer_11",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_140",
          "type": "Linear",
          "name": "layer11_gate_proj",
          "layer": "layer_11",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_141",
          "type": "Linear",
          "name": "layer11_up_proj",
          "layer": "layer_11",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_142",
          "type": "Linear",
          "name": "layer11_down_proj",
          "layer": "layer_11",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_143",
          "type": "RMSNorm",
          "name": "layer11_input_layernorm",
          "layer": "layer_11",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_144",
          "type": "RMSNorm",
          "name": "layer11_post_attention_layernorm",
          "layer": "layer_11",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_145",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_146",
          "type": "SiLU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_147",
          "type": "Dropout",
          "name": "layer11_attention_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_148",
          "type": "Linear",
          "name": "layer12_q_proj",
          "layer": "layer_12",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_149",
          "type": "Linear",
          "name": "layer12_k_proj",
          "layer": "layer_12",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_150",
          "type": "Linear",
          "name": "layer12_v_proj",
          "layer": "layer_12",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_151",
          "type": "Linear",
          "name": "layer12_o_proj",
          "layer": "layer_12",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_152",
          "type": "Linear",
          "name": "layer12_gate_proj",
          "layer": "layer_12",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_153",
          "type": "Linear",
          "name": "layer12_up_proj",
          "layer": "layer_12",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_154",
          "type": "Linear",
          "name": "layer12_down_proj",
          "layer": "layer_12",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_155",
          "type": "RMSNorm",
          "name": "layer12_input_layernorm",
          "layer": "layer_12",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_156",
          "type": "RMSNorm",
          "name": "layer12_post_attention_layernorm",
          "layer": "layer_12",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_157",
          "type": "Softmax",
          "name": "layer12_attention_softmax",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_158",
          "type": "SiLU",
          "name": "layer12_activation",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_159",
          "type": "Dropout",
          "name": "layer12_attention_dropout",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_160",
          "type": "Linear",
          "name": "layer13_q_proj",
          "layer": "layer_13",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_161",
          "type": "Linear",
          "name": "layer13_k_proj",
          "layer": "layer_13",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_162",
          "type": "Linear",
          "name": "layer13_v_proj",
          "layer": "layer_13",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_163",
          "type": "Linear",
          "name": "layer13_o_proj",
          "layer": "layer_13",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_164",
          "type": "Linear",
          "name": "layer13_gate_proj",
          "layer": "layer_13",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_165",
          "type": "Linear",
          "name": "layer13_up_proj",
          "layer": "layer_13",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_166",
          "type": "Linear",
          "name": "layer13_down_proj",
          "layer": "layer_13",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_167",
          "type": "RMSNorm",
          "name": "layer13_input_layernorm",
          "layer": "layer_13",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_168",
          "type": "RMSNorm",
          "name": "layer13_post_attention_layernorm",
          "layer": "layer_13",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_169",
          "type": "Softmax",
          "name": "layer13_attention_softmax",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_170",
          "type": "SiLU",
          "name": "layer13_activation",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_171",
          "type": "Dropout",
          "name": "layer13_attention_dropout",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_172",
          "type": "Linear",
          "name": "layer14_q_proj",
          "layer": "layer_14",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_173",
          "type": "Linear",
          "name": "layer14_k_proj",
          "layer": "layer_14",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_174",
          "type": "Linear",
          "name": "layer14_v_proj",
          "layer": "layer_14",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_175",
          "type": "Linear",
          "name": "layer14_o_proj",
          "layer": "layer_14",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_176",
          "type": "Linear",
          "name": "layer14_gate_proj",
          "layer": "layer_14",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_177",
          "type": "Linear",
          "name": "layer14_up_proj",
          "layer": "layer_14",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_178",
          "type": "Linear",
          "name": "layer14_down_proj",
          "layer": "layer_14",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_179",
          "type": "RMSNorm",
          "name": "layer14_input_layernorm",
          "layer": "layer_14",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_180",
          "type": "RMSNorm",
          "name": "layer14_post_attention_layernorm",
          "layer": "layer_14",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_181",
          "type": "Softmax",
          "name": "layer14_attention_softmax",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_182",
          "type": "SiLU",
          "name": "layer14_activation",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_183",
          "type": "Dropout",
          "name": "layer14_attention_dropout",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_184",
          "type": "Linear",
          "name": "layer15_q_proj",
          "layer": "layer_15",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_185",
          "type": "Linear",
          "name": "layer15_k_proj",
          "layer": "layer_15",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_186",
          "type": "Linear",
          "name": "layer15_v_proj",
          "layer": "layer_15",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_187",
          "type": "Linear",
          "name": "layer15_o_proj",
          "layer": "layer_15",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_188",
          "type": "Linear",
          "name": "layer15_gate_proj",
          "layer": "layer_15",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_189",
          "type": "Linear",
          "name": "layer15_up_proj",
          "layer": "layer_15",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_190",
          "type": "Linear",
          "name": "layer15_down_proj",
          "layer": "layer_15",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_191",
          "type": "RMSNorm",
          "name": "layer15_input_layernorm",
          "layer": "layer_15",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_192",
          "type": "RMSNorm",
          "name": "layer15_post_attention_layernorm",
          "layer": "layer_15",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_193",
          "type": "Softmax",
          "name": "layer15_attention_softmax",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_194",
          "type": "SiLU",
          "name": "layer15_activation",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_195",
          "type": "Dropout",
          "name": "layer15_attention_dropout",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_196",
          "type": "Linear",
          "name": "layer16_q_proj",
          "layer": "layer_16",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_197",
          "type": "Linear",
          "name": "layer16_k_proj",
          "layer": "layer_16",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_198",
          "type": "Linear",
          "name": "layer16_v_proj",
          "layer": "layer_16",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_199",
          "type": "Linear",
          "name": "layer16_o_proj",
          "layer": "layer_16",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_200",
          "type": "Linear",
          "name": "layer16_gate_proj",
          "layer": "layer_16",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_201",
          "type": "Linear",
          "name": "layer16_up_proj",
          "layer": "layer_16",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_202",
          "type": "Linear",
          "name": "layer16_down_proj",
          "layer": "layer_16",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_203",
          "type": "RMSNorm",
          "name": "layer16_input_layernorm",
          "layer": "layer_16",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_204",
          "type": "RMSNorm",
          "name": "layer16_post_attention_layernorm",
          "layer": "layer_16",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_205",
          "type": "Softmax",
          "name": "layer16_attention_softmax",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_206",
          "type": "SiLU",
          "name": "layer16_activation",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_207",
          "type": "Dropout",
          "name": "layer16_attention_dropout",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_208",
          "type": "Linear",
          "name": "layer17_q_proj",
          "layer": "layer_17",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_209",
          "type": "Linear",
          "name": "layer17_k_proj",
          "layer": "layer_17",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_210",
          "type": "Linear",
          "name": "layer17_v_proj",
          "layer": "layer_17",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_211",
          "type": "Linear",
          "name": "layer17_o_proj",
          "layer": "layer_17",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_212",
          "type": "Linear",
          "name": "layer17_gate_proj",
          "layer": "layer_17",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_213",
          "type": "Linear",
          "name": "layer17_up_proj",
          "layer": "layer_17",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_214",
          "type": "Linear",
          "name": "layer17_down_proj",
          "layer": "layer_17",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_215",
          "type": "RMSNorm",
          "name": "layer17_input_layernorm",
          "layer": "layer_17",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_216",
          "type": "RMSNorm",
          "name": "layer17_post_attention_layernorm",
          "layer": "layer_17",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_217",
          "type": "Softmax",
          "name": "layer17_attention_softmax",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_218",
          "type": "SiLU",
          "name": "layer17_activation",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_219",
          "type": "Dropout",
          "name": "layer17_attention_dropout",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_220",
          "type": "Linear",
          "name": "layer18_q_proj",
          "layer": "layer_18",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_221",
          "type": "Linear",
          "name": "layer18_k_proj",
          "layer": "layer_18",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_222",
          "type": "Linear",
          "name": "layer18_v_proj",
          "layer": "layer_18",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_223",
          "type": "Linear",
          "name": "layer18_o_proj",
          "layer": "layer_18",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_224",
          "type": "Linear",
          "name": "layer18_gate_proj",
          "layer": "layer_18",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_225",
          "type": "Linear",
          "name": "layer18_up_proj",
          "layer": "layer_18",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_226",
          "type": "Linear",
          "name": "layer18_down_proj",
          "layer": "layer_18",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_227",
          "type": "RMSNorm",
          "name": "layer18_input_layernorm",
          "layer": "layer_18",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_228",
          "type": "RMSNorm",
          "name": "layer18_post_attention_layernorm",
          "layer": "layer_18",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_229",
          "type": "Softmax",
          "name": "layer18_attention_softmax",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_230",
          "type": "SiLU",
          "name": "layer18_activation",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_231",
          "type": "Dropout",
          "name": "layer18_attention_dropout",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_232",
          "type": "Linear",
          "name": "layer19_q_proj",
          "layer": "layer_19",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_233",
          "type": "Linear",
          "name": "layer19_k_proj",
          "layer": "layer_19",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_234",
          "type": "Linear",
          "name": "layer19_v_proj",
          "layer": "layer_19",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_235",
          "type": "Linear",
          "name": "layer19_o_proj",
          "layer": "layer_19",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_236",
          "type": "Linear",
          "name": "layer19_gate_proj",
          "layer": "layer_19",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_237",
          "type": "Linear",
          "name": "layer19_up_proj",
          "layer": "layer_19",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_238",
          "type": "Linear",
          "name": "layer19_down_proj",
          "layer": "layer_19",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_239",
          "type": "RMSNorm",
          "name": "layer19_input_layernorm",
          "layer": "layer_19",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_240",
          "type": "RMSNorm",
          "name": "layer19_post_attention_layernorm",
          "layer": "layer_19",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_241",
          "type": "Softmax",
          "name": "layer19_attention_softmax",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_242",
          "type": "SiLU",
          "name": "layer19_activation",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_243",
          "type": "Dropout",
          "name": "layer19_attention_dropout",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_244",
          "type": "Linear",
          "name": "layer20_q_proj",
          "layer": "layer_20",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_245",
          "type": "Linear",
          "name": "layer20_k_proj",
          "layer": "layer_20",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_246",
          "type": "Linear",
          "name": "layer20_v_proj",
          "layer": "layer_20",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_247",
          "type": "Linear",
          "name": "layer20_o_proj",
          "layer": "layer_20",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_248",
          "type": "Linear",
          "name": "layer20_gate_proj",
          "layer": "layer_20",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_249",
          "type": "Linear",
          "name": "layer20_up_proj",
          "layer": "layer_20",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_250",
          "type": "Linear",
          "name": "layer20_down_proj",
          "layer": "layer_20",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_251",
          "type": "RMSNorm",
          "name": "layer20_input_layernorm",
          "layer": "layer_20",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_252",
          "type": "RMSNorm",
          "name": "layer20_post_attention_layernorm",
          "layer": "layer_20",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_253",
          "type": "Softmax",
          "name": "layer20_attention_softmax",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_254",
          "type": "SiLU",
          "name": "layer20_activation",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_255",
          "type": "Dropout",
          "name": "layer20_attention_dropout",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_256",
          "type": "Linear",
          "name": "layer21_q_proj",
          "layer": "layer_21",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_257",
          "type": "Linear",
          "name": "layer21_k_proj",
          "layer": "layer_21",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_258",
          "type": "Linear",
          "name": "layer21_v_proj",
          "layer": "layer_21",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_259",
          "type": "Linear",
          "name": "layer21_o_proj",
          "layer": "layer_21",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_260",
          "type": "Linear",
          "name": "layer21_gate_proj",
          "layer": "layer_21",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_261",
          "type": "Linear",
          "name": "layer21_up_proj",
          "layer": "layer_21",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_262",
          "type": "Linear",
          "name": "layer21_down_proj",
          "layer": "layer_21",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_263",
          "type": "RMSNorm",
          "name": "layer21_input_layernorm",
          "layer": "layer_21",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_264",
          "type": "RMSNorm",
          "name": "layer21_post_attention_layernorm",
          "layer": "layer_21",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_265",
          "type": "Softmax",
          "name": "layer21_attention_softmax",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_266",
          "type": "SiLU",
          "name": "layer21_activation",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_267",
          "type": "Dropout",
          "name": "layer21_attention_dropout",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_268",
          "type": "Linear",
          "name": "layer22_q_proj",
          "layer": "layer_22",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_269",
          "type": "Linear",
          "name": "layer22_k_proj",
          "layer": "layer_22",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_270",
          "type": "Linear",
          "name": "layer22_v_proj",
          "layer": "layer_22",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_271",
          "type": "Linear",
          "name": "layer22_o_proj",
          "layer": "layer_22",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_272",
          "type": "Linear",
          "name": "layer22_gate_proj",
          "layer": "layer_22",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_273",
          "type": "Linear",
          "name": "layer22_up_proj",
          "layer": "layer_22",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_274",
          "type": "Linear",
          "name": "layer22_down_proj",
          "layer": "layer_22",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_275",
          "type": "RMSNorm",
          "name": "layer22_input_layernorm",
          "layer": "layer_22",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_276",
          "type": "RMSNorm",
          "name": "layer22_post_attention_layernorm",
          "layer": "layer_22",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_277",
          "type": "Softmax",
          "name": "layer22_attention_softmax",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_278",
          "type": "SiLU",
          "name": "layer22_activation",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_279",
          "type": "Dropout",
          "name": "layer22_attention_dropout",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_280",
          "type": "Linear",
          "name": "layer23_q_proj",
          "layer": "layer_23",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_281",
          "type": "Linear",
          "name": "layer23_k_proj",
          "layer": "layer_23",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_282",
          "type": "Linear",
          "name": "layer23_v_proj",
          "layer": "layer_23",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_283",
          "type": "Linear",
          "name": "layer23_o_proj",
          "layer": "layer_23",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_284",
          "type": "Linear",
          "name": "layer23_gate_proj",
          "layer": "layer_23",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_285",
          "type": "Linear",
          "name": "layer23_up_proj",
          "layer": "layer_23",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_286",
          "type": "Linear",
          "name": "layer23_down_proj",
          "layer": "layer_23",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_287",
          "type": "RMSNorm",
          "name": "layer23_input_layernorm",
          "layer": "layer_23",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_288",
          "type": "RMSNorm",
          "name": "layer23_post_attention_layernorm",
          "layer": "layer_23",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_289",
          "type": "Softmax",
          "name": "layer23_attention_softmax",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_290",
          "type": "SiLU",
          "name": "layer23_activation",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_291",
          "type": "Dropout",
          "name": "layer23_attention_dropout",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_292",
          "type": "Linear",
          "name": "layer24_q_proj",
          "layer": "layer_24",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_293",
          "type": "Linear",
          "name": "layer24_k_proj",
          "layer": "layer_24",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_294",
          "type": "Linear",
          "name": "layer24_v_proj",
          "layer": "layer_24",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_295",
          "type": "Linear",
          "name": "layer24_o_proj",
          "layer": "layer_24",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_296",
          "type": "Linear",
          "name": "layer24_gate_proj",
          "layer": "layer_24",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_297",
          "type": "Linear",
          "name": "layer24_up_proj",
          "layer": "layer_24",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_298",
          "type": "Linear",
          "name": "layer24_down_proj",
          "layer": "layer_24",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_299",
          "type": "RMSNorm",
          "name": "layer24_input_layernorm",
          "layer": "layer_24",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_300",
          "type": "RMSNorm",
          "name": "layer24_post_attention_layernorm",
          "layer": "layer_24",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_301",
          "type": "Softmax",
          "name": "layer24_attention_softmax",
          "layer": "layer_24",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_302",
          "type": "SiLU",
          "name": "layer24_activation",
          "layer": "layer_24",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_303",
          "type": "Dropout",
          "name": "layer24_attention_dropout",
          "layer": "layer_24",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_304",
          "type": "Linear",
          "name": "layer25_q_proj",
          "layer": "layer_25",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_305",
          "type": "Linear",
          "name": "layer25_k_proj",
          "layer": "layer_25",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_306",
          "type": "Linear",
          "name": "layer25_v_proj",
          "layer": "layer_25",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_307",
          "type": "Linear",
          "name": "layer25_o_proj",
          "layer": "layer_25",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_308",
          "type": "Linear",
          "name": "layer25_gate_proj",
          "layer": "layer_25",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_309",
          "type": "Linear",
          "name": "layer25_up_proj",
          "layer": "layer_25",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_310",
          "type": "Linear",
          "name": "layer25_down_proj",
          "layer": "layer_25",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_311",
          "type": "RMSNorm",
          "name": "layer25_input_layernorm",
          "layer": "layer_25",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_312",
          "type": "RMSNorm",
          "name": "layer25_post_attention_layernorm",
          "layer": "layer_25",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_313",
          "type": "Softmax",
          "name": "layer25_attention_softmax",
          "layer": "layer_25",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_314",
          "type": "SiLU",
          "name": "layer25_activation",
          "layer": "layer_25",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_315",
          "type": "Dropout",
          "name": "layer25_attention_dropout",
          "layer": "layer_25",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_316",
          "type": "Linear",
          "name": "layer26_q_proj",
          "layer": "layer_26",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_317",
          "type": "Linear",
          "name": "layer26_k_proj",
          "layer": "layer_26",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_318",
          "type": "Linear",
          "name": "layer26_v_proj",
          "layer": "layer_26",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_319",
          "type": "Linear",
          "name": "layer26_o_proj",
          "layer": "layer_26",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_320",
          "type": "Linear",
          "name": "layer26_gate_proj",
          "layer": "layer_26",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_321",
          "type": "Linear",
          "name": "layer26_up_proj",
          "layer": "layer_26",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_322",
          "type": "Linear",
          "name": "layer26_down_proj",
          "layer": "layer_26",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_323",
          "type": "RMSNorm",
          "name": "layer26_input_layernorm",
          "layer": "layer_26",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_324",
          "type": "RMSNorm",
          "name": "layer26_post_attention_layernorm",
          "layer": "layer_26",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_325",
          "type": "Softmax",
          "name": "layer26_attention_softmax",
          "layer": "layer_26",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_326",
          "type": "SiLU",
          "name": "layer26_activation",
          "layer": "layer_26",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_327",
          "type": "Dropout",
          "name": "layer26_attention_dropout",
          "layer": "layer_26",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_328",
          "type": "Linear",
          "name": "layer27_q_proj",
          "layer": "layer_27",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_329",
          "type": "Linear",
          "name": "layer27_k_proj",
          "layer": "layer_27",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_330",
          "type": "Linear",
          "name": "layer27_v_proj",
          "layer": "layer_27",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_331",
          "type": "Linear",
          "name": "layer27_o_proj",
          "layer": "layer_27",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_332",
          "type": "Linear",
          "name": "layer27_gate_proj",
          "layer": "layer_27",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_333",
          "type": "Linear",
          "name": "layer27_up_proj",
          "layer": "layer_27",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_334",
          "type": "Linear",
          "name": "layer27_down_proj",
          "layer": "layer_27",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_335",
          "type": "RMSNorm",
          "name": "layer27_input_layernorm",
          "layer": "layer_27",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_336",
          "type": "RMSNorm",
          "name": "layer27_post_attention_layernorm",
          "layer": "layer_27",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_337",
          "type": "Softmax",
          "name": "layer27_attention_softmax",
          "layer": "layer_27",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_338",
          "type": "SiLU",
          "name": "layer27_activation",
          "layer": "layer_27",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_339",
          "type": "Dropout",
          "name": "layer27_attention_dropout",
          "layer": "layer_27",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_340",
          "type": "Linear",
          "name": "layer28_q_proj",
          "layer": "layer_28",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_341",
          "type": "Linear",
          "name": "layer28_k_proj",
          "layer": "layer_28",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_342",
          "type": "Linear",
          "name": "layer28_v_proj",
          "layer": "layer_28",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_343",
          "type": "Linear",
          "name": "layer28_o_proj",
          "layer": "layer_28",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_344",
          "type": "Linear",
          "name": "layer28_gate_proj",
          "layer": "layer_28",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_345",
          "type": "Linear",
          "name": "layer28_up_proj",
          "layer": "layer_28",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_346",
          "type": "Linear",
          "name": "layer28_down_proj",
          "layer": "layer_28",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_347",
          "type": "RMSNorm",
          "name": "layer28_input_layernorm",
          "layer": "layer_28",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_348",
          "type": "RMSNorm",
          "name": "layer28_post_attention_layernorm",
          "layer": "layer_28",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_349",
          "type": "Softmax",
          "name": "layer28_attention_softmax",
          "layer": "layer_28",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_350",
          "type": "SiLU",
          "name": "layer28_activation",
          "layer": "layer_28",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_351",
          "type": "Dropout",
          "name": "layer28_attention_dropout",
          "layer": "layer_28",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_352",
          "type": "Linear",
          "name": "layer29_q_proj",
          "layer": "layer_29",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_353",
          "type": "Linear",
          "name": "layer29_k_proj",
          "layer": "layer_29",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_354",
          "type": "Linear",
          "name": "layer29_v_proj",
          "layer": "layer_29",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_355",
          "type": "Linear",
          "name": "layer29_o_proj",
          "layer": "layer_29",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_356",
          "type": "Linear",
          "name": "layer29_gate_proj",
          "layer": "layer_29",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_357",
          "type": "Linear",
          "name": "layer29_up_proj",
          "layer": "layer_29",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_358",
          "type": "Linear",
          "name": "layer29_down_proj",
          "layer": "layer_29",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_359",
          "type": "RMSNorm",
          "name": "layer29_input_layernorm",
          "layer": "layer_29",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_360",
          "type": "RMSNorm",
          "name": "layer29_post_attention_layernorm",
          "layer": "layer_29",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_361",
          "type": "Softmax",
          "name": "layer29_attention_softmax",
          "layer": "layer_29",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_362",
          "type": "SiLU",
          "name": "layer29_activation",
          "layer": "layer_29",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_363",
          "type": "Dropout",
          "name": "layer29_attention_dropout",
          "layer": "layer_29",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_364",
          "type": "Linear",
          "name": "layer30_q_proj",
          "layer": "layer_30",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_365",
          "type": "Linear",
          "name": "layer30_k_proj",
          "layer": "layer_30",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_366",
          "type": "Linear",
          "name": "layer30_v_proj",
          "layer": "layer_30",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_367",
          "type": "Linear",
          "name": "layer30_o_proj",
          "layer": "layer_30",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_368",
          "type": "Linear",
          "name": "layer30_gate_proj",
          "layer": "layer_30",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_369",
          "type": "Linear",
          "name": "layer30_up_proj",
          "layer": "layer_30",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_370",
          "type": "Linear",
          "name": "layer30_down_proj",
          "layer": "layer_30",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_371",
          "type": "RMSNorm",
          "name": "layer30_input_layernorm",
          "layer": "layer_30",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_372",
          "type": "RMSNorm",
          "name": "layer30_post_attention_layernorm",
          "layer": "layer_30",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_373",
          "type": "Softmax",
          "name": "layer30_attention_softmax",
          "layer": "layer_30",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_374",
          "type": "SiLU",
          "name": "layer30_activation",
          "layer": "layer_30",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_375",
          "type": "Dropout",
          "name": "layer30_attention_dropout",
          "layer": "layer_30",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_376",
          "type": "Linear",
          "name": "layer31_q_proj",
          "layer": "layer_31",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_377",
          "type": "Linear",
          "name": "layer31_k_proj",
          "layer": "layer_31",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_378",
          "type": "Linear",
          "name": "layer31_v_proj",
          "layer": "layer_31",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_379",
          "type": "Linear",
          "name": "layer31_o_proj",
          "layer": "layer_31",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_380",
          "type": "Linear",
          "name": "layer31_gate_proj",
          "layer": "layer_31",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_381",
          "type": "Linear",
          "name": "layer31_up_proj",
          "layer": "layer_31",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_382",
          "type": "Linear",
          "name": "layer31_down_proj",
          "layer": "layer_31",
          "parameters": 58720256,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Mistral_7B_op_383",
          "type": "RMSNorm",
          "name": "layer31_input_layernorm",
          "layer": "layer_31",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_384",
          "type": "RMSNorm",
          "name": "layer31_post_attention_layernorm",
          "layer": "layer_31",
          "parameters": 4096,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Mistral_7B_op_385",
          "type": "Softmax",
          "name": "layer31_attention_softmax",
          "layer": "layer_31",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Mistral_7B_op_386",
          "type": "SiLU",
          "name": "layer31_activation",
          "layer": "layer_31",
          "parameters": 0,
          "input_shape": "[batch, seq, 14336]",
          "output_shape": "[batch, seq, 14336]"
        },
        {
          "op_id": "Mistral_7B_op_387",
          "type": "Dropout",
          "name": "layer31_attention_dropout",
          "layer": "layer_31",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        }
      ],
      "parameters": 7000000000
    },
    {
      "model_id": "Phi_3_mini",
      "name": "Phi-3-mini",
      "source": "huggingface/phi-3-mini",
      "category": "LLM",
      "architecture": "llm_decoder",
      "num_params": 3800000000,
      "num_layers": 32,
      "hidden_size": 3072,
      "vocab_size": 32064,
      "intermediate_size": 8192,
      "operator_count": 387,
      "operators": [
        {
          "op_id": "Phi_3_mini_op_1",
          "type": "Embedding",
          "name": "embed_tokens",
          "layer": "global",
          "parameters": 98500608,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_2",
          "type": "RMSNorm",
          "name": "norm",
          "layer": "global",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_3",
          "type": "Linear",
          "name": "lm_head",
          "layer": "global",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_4",
          "type": "Linear",
          "name": "layer0_q_proj",
          "layer": "layer_0",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_5",
          "type": "Linear",
          "name": "layer0_k_proj",
          "layer": "layer_0",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_6",
          "type": "Linear",
          "name": "layer0_v_proj",
          "layer": "layer_0",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_7",
          "type": "Linear",
          "name": "layer0_o_proj",
          "layer": "layer_0",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_8",
          "type": "Linear",
          "name": "layer0_gate_proj",
          "layer": "layer_0",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_9",
          "type": "Linear",
          "name": "layer0_up_proj",
          "layer": "layer_0",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_10",
          "type": "Linear",
          "name": "layer0_down_proj",
          "layer": "layer_0",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_11",
          "type": "RMSNorm",
          "name": "layer0_input_layernorm",
          "layer": "layer_0",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_12",
          "type": "RMSNorm",
          "name": "layer0_post_attention_layernorm",
          "layer": "layer_0",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_13",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_14",
          "type": "SiLU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_15",
          "type": "Dropout",
          "name": "layer0_attention_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_16",
          "type": "Linear",
          "name": "layer1_q_proj",
          "layer": "layer_1",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_17",
          "type": "Linear",
          "name": "layer1_k_proj",
          "layer": "layer_1",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_18",
          "type": "Linear",
          "name": "layer1_v_proj",
          "layer": "layer_1",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_19",
          "type": "Linear",
          "name": "layer1_o_proj",
          "layer": "layer_1",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_20",
          "type": "Linear",
          "name": "layer1_gate_proj",
          "layer": "layer_1",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_21",
          "type": "Linear",
          "name": "layer1_up_proj",
          "layer": "layer_1",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_22",
          "type": "Linear",
          "name": "layer1_down_proj",
          "layer": "layer_1",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_23",
          "type": "RMSNorm",
          "name": "layer1_input_layernorm",
          "layer": "layer_1",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_24",
          "type": "RMSNorm",
          "name": "layer1_post_attention_layernorm",
          "layer": "layer_1",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_25",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_26",
          "type": "SiLU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_27",
          "type": "Dropout",
          "name": "layer1_attention_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_28",
          "type": "Linear",
          "name": "layer2_q_proj",
          "layer": "layer_2",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_29",
          "type": "Linear",
          "name": "layer2_k_proj",
          "layer": "layer_2",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_30",
          "type": "Linear",
          "name": "layer2_v_proj",
          "layer": "layer_2",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_31",
          "type": "Linear",
          "name": "layer2_o_proj",
          "layer": "layer_2",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_32",
          "type": "Linear",
          "name": "layer2_gate_proj",
          "layer": "layer_2",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_33",
          "type": "Linear",
          "name": "layer2_up_proj",
          "layer": "layer_2",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_34",
          "type": "Linear",
          "name": "layer2_down_proj",
          "layer": "layer_2",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_35",
          "type": "RMSNorm",
          "name": "layer2_input_layernorm",
          "layer": "layer_2",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_36",
          "type": "RMSNorm",
          "name": "layer2_post_attention_layernorm",
          "layer": "layer_2",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_37",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_38",
          "type": "SiLU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_39",
          "type": "Dropout",
          "name": "layer2_attention_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_40",
          "type": "Linear",
          "name": "layer3_q_proj",
          "layer": "layer_3",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_41",
          "type": "Linear",
          "name": "layer3_k_proj",
          "layer": "layer_3",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_42",
          "type": "Linear",
          "name": "layer3_v_proj",
          "layer": "layer_3",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_43",
          "type": "Linear",
          "name": "layer3_o_proj",
          "layer": "layer_3",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_44",
          "type": "Linear",
          "name": "layer3_gate_proj",
          "layer": "layer_3",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_45",
          "type": "Linear",
          "name": "layer3_up_proj",
          "layer": "layer_3",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_46",
          "type": "Linear",
          "name": "layer3_down_proj",
          "layer": "layer_3",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_47",
          "type": "RMSNorm",
          "name": "layer3_input_layernorm",
          "layer": "layer_3",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_48",
          "type": "RMSNorm",
          "name": "layer3_post_attention_layernorm",
          "layer": "layer_3",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_49",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_50",
          "type": "SiLU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_51",
          "type": "Dropout",
          "name": "layer3_attention_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_52",
          "type": "Linear",
          "name": "layer4_q_proj",
          "layer": "layer_4",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_53",
          "type": "Linear",
          "name": "layer4_k_proj",
          "layer": "layer_4",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_54",
          "type": "Linear",
          "name": "layer4_v_proj",
          "layer": "layer_4",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_55",
          "type": "Linear",
          "name": "layer4_o_proj",
          "layer": "layer_4",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_56",
          "type": "Linear",
          "name": "layer4_gate_proj",
          "layer": "layer_4",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_57",
          "type": "Linear",
          "name": "layer4_up_proj",
          "layer": "layer_4",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_58",
          "type": "Linear",
          "name": "layer4_down_proj",
          "layer": "layer_4",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_59",
          "type": "RMSNorm",
          "name": "layer4_input_layernorm",
          "layer": "layer_4",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_60",
          "type": "RMSNorm",
          "name": "layer4_post_attention_layernorm",
          "layer": "layer_4",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_61",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_62",
          "type": "SiLU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_63",
          "type": "Dropout",
          "name": "layer4_attention_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_64",
          "type": "Linear",
          "name": "layer5_q_proj",
          "layer": "layer_5",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_65",
          "type": "Linear",
          "name": "layer5_k_proj",
          "layer": "layer_5",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_66",
          "type": "Linear",
          "name": "layer5_v_proj",
          "layer": "layer_5",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_67",
          "type": "Linear",
          "name": "layer5_o_proj",
          "layer": "layer_5",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_68",
          "type": "Linear",
          "name": "layer5_gate_proj",
          "layer": "layer_5",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_69",
          "type": "Linear",
          "name": "layer5_up_proj",
          "layer": "layer_5",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_70",
          "type": "Linear",
          "name": "layer5_down_proj",
          "layer": "layer_5",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_71",
          "type": "RMSNorm",
          "name": "layer5_input_layernorm",
          "layer": "layer_5",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_72",
          "type": "RMSNorm",
          "name": "layer5_post_attention_layernorm",
          "layer": "layer_5",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_73",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_74",
          "type": "SiLU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_75",
          "type": "Dropout",
          "name": "layer5_attention_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_76",
          "type": "Linear",
          "name": "layer6_q_proj",
          "layer": "layer_6",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_77",
          "type": "Linear",
          "name": "layer6_k_proj",
          "layer": "layer_6",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_78",
          "type": "Linear",
          "name": "layer6_v_proj",
          "layer": "layer_6",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_79",
          "type": "Linear",
          "name": "layer6_o_proj",
          "layer": "layer_6",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_80",
          "type": "Linear",
          "name": "layer6_gate_proj",
          "layer": "layer_6",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_81",
          "type": "Linear",
          "name": "layer6_up_proj",
          "layer": "layer_6",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_82",
          "type": "Linear",
          "name": "layer6_down_proj",
          "layer": "layer_6",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_83",
          "type": "RMSNorm",
          "name": "layer6_input_layernorm",
          "layer": "layer_6",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_84",
          "type": "RMSNorm",
          "name": "layer6_post_attention_layernorm",
          "layer": "layer_6",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_85",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_86",
          "type": "SiLU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_87",
          "type": "Dropout",
          "name": "layer6_attention_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_88",
          "type": "Linear",
          "name": "layer7_q_proj",
          "layer": "layer_7",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_89",
          "type": "Linear",
          "name": "layer7_k_proj",
          "layer": "layer_7",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_90",
          "type": "Linear",
          "name": "layer7_v_proj",
          "layer": "layer_7",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_91",
          "type": "Linear",
          "name": "layer7_o_proj",
          "layer": "layer_7",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_92",
          "type": "Linear",
          "name": "layer7_gate_proj",
          "layer": "layer_7",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_93",
          "type": "Linear",
          "name": "layer7_up_proj",
          "layer": "layer_7",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_94",
          "type": "Linear",
          "name": "layer7_down_proj",
          "layer": "layer_7",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_95",
          "type": "RMSNorm",
          "name": "layer7_input_layernorm",
          "layer": "layer_7",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_96",
          "type": "RMSNorm",
          "name": "layer7_post_attention_layernorm",
          "layer": "layer_7",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_97",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_98",
          "type": "SiLU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_99",
          "type": "Dropout",
          "name": "layer7_attention_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_100",
          "type": "Linear",
          "name": "layer8_q_proj",
          "layer": "layer_8",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_101",
          "type": "Linear",
          "name": "layer8_k_proj",
          "layer": "layer_8",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_102",
          "type": "Linear",
          "name": "layer8_v_proj",
          "layer": "layer_8",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_103",
          "type": "Linear",
          "name": "layer8_o_proj",
          "layer": "layer_8",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_104",
          "type": "Linear",
          "name": "layer8_gate_proj",
          "layer": "layer_8",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_105",
          "type": "Linear",
          "name": "layer8_up_proj",
          "layer": "layer_8",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_106",
          "type": "Linear",
          "name": "layer8_down_proj",
          "layer": "layer_8",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_107",
          "type": "RMSNorm",
          "name": "layer8_input_layernorm",
          "layer": "layer_8",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_108",
          "type": "RMSNorm",
          "name": "layer8_post_attention_layernorm",
          "layer": "layer_8",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_109",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_110",
          "type": "SiLU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_111",
          "type": "Dropout",
          "name": "layer8_attention_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_112",
          "type": "Linear",
          "name": "layer9_q_proj",
          "layer": "layer_9",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_113",
          "type": "Linear",
          "name": "layer9_k_proj",
          "layer": "layer_9",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_114",
          "type": "Linear",
          "name": "layer9_v_proj",
          "layer": "layer_9",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_115",
          "type": "Linear",
          "name": "layer9_o_proj",
          "layer": "layer_9",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_116",
          "type": "Linear",
          "name": "layer9_gate_proj",
          "layer": "layer_9",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_117",
          "type": "Linear",
          "name": "layer9_up_proj",
          "layer": "layer_9",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_118",
          "type": "Linear",
          "name": "layer9_down_proj",
          "layer": "layer_9",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_119",
          "type": "RMSNorm",
          "name": "layer9_input_layernorm",
          "layer": "layer_9",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_120",
          "type": "RMSNorm",
          "name": "layer9_post_attention_layernorm",
          "layer": "layer_9",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_121",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_122",
          "type": "SiLU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_123",
          "type": "Dropout",
          "name": "layer9_attention_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_124",
          "type": "Linear",
          "name": "layer10_q_proj",
          "layer": "layer_10",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_125",
          "type": "Linear",
          "name": "layer10_k_proj",
          "layer": "layer_10",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_126",
          "type": "Linear",
          "name": "layer10_v_proj",
          "layer": "layer_10",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_127",
          "type": "Linear",
          "name": "layer10_o_proj",
          "layer": "layer_10",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_128",
          "type": "Linear",
          "name": "layer10_gate_proj",
          "layer": "layer_10",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_129",
          "type": "Linear",
          "name": "layer10_up_proj",
          "layer": "layer_10",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_130",
          "type": "Linear",
          "name": "layer10_down_proj",
          "layer": "layer_10",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_131",
          "type": "RMSNorm",
          "name": "layer10_input_layernorm",
          "layer": "layer_10",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_132",
          "type": "RMSNorm",
          "name": "layer10_post_attention_layernorm",
          "layer": "layer_10",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_133",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_134",
          "type": "SiLU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_135",
          "type": "Dropout",
          "name": "layer10_attention_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_136",
          "type": "Linear",
          "name": "layer11_q_proj",
          "layer": "layer_11",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_137",
          "type": "Linear",
          "name": "layer11_k_proj",
          "layer": "layer_11",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_138",
          "type": "Linear",
          "name": "layer11_v_proj",
          "layer": "layer_11",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_139",
          "type": "Linear",
          "name": "layer11_o_proj",
          "layer": "layer_11",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_140",
          "type": "Linear",
          "name": "layer11_gate_proj",
          "layer": "layer_11",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_141",
          "type": "Linear",
          "name": "layer11_up_proj",
          "layer": "layer_11",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_142",
          "type": "Linear",
          "name": "layer11_down_proj",
          "layer": "layer_11",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_143",
          "type": "RMSNorm",
          "name": "layer11_input_layernorm",
          "layer": "layer_11",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_144",
          "type": "RMSNorm",
          "name": "layer11_post_attention_layernorm",
          "layer": "layer_11",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_145",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_146",
          "type": "SiLU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_147",
          "type": "Dropout",
          "name": "layer11_attention_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_148",
          "type": "Linear",
          "name": "layer12_q_proj",
          "layer": "layer_12",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_149",
          "type": "Linear",
          "name": "layer12_k_proj",
          "layer": "layer_12",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_150",
          "type": "Linear",
          "name": "layer12_v_proj",
          "layer": "layer_12",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_151",
          "type": "Linear",
          "name": "layer12_o_proj",
          "layer": "layer_12",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_152",
          "type": "Linear",
          "name": "layer12_gate_proj",
          "layer": "layer_12",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_153",
          "type": "Linear",
          "name": "layer12_up_proj",
          "layer": "layer_12",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_154",
          "type": "Linear",
          "name": "layer12_down_proj",
          "layer": "layer_12",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_155",
          "type": "RMSNorm",
          "name": "layer12_input_layernorm",
          "layer": "layer_12",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_156",
          "type": "RMSNorm",
          "name": "layer12_post_attention_layernorm",
          "layer": "layer_12",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_157",
          "type": "Softmax",
          "name": "layer12_attention_softmax",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_158",
          "type": "SiLU",
          "name": "layer12_activation",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_159",
          "type": "Dropout",
          "name": "layer12_attention_dropout",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_160",
          "type": "Linear",
          "name": "layer13_q_proj",
          "layer": "layer_13",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_161",
          "type": "Linear",
          "name": "layer13_k_proj",
          "layer": "layer_13",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_162",
          "type": "Linear",
          "name": "layer13_v_proj",
          "layer": "layer_13",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_163",
          "type": "Linear",
          "name": "layer13_o_proj",
          "layer": "layer_13",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_164",
          "type": "Linear",
          "name": "layer13_gate_proj",
          "layer": "layer_13",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_165",
          "type": "Linear",
          "name": "layer13_up_proj",
          "layer": "layer_13",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_166",
          "type": "Linear",
          "name": "layer13_down_proj",
          "layer": "layer_13",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_167",
          "type": "RMSNorm",
          "name": "layer13_input_layernorm",
          "layer": "layer_13",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_168",
          "type": "RMSNorm",
          "name": "layer13_post_attention_layernorm",
          "layer": "layer_13",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_169",
          "type": "Softmax",
          "name": "layer13_attention_softmax",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_170",
          "type": "SiLU",
          "name": "layer13_activation",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_171",
          "type": "Dropout",
          "name": "layer13_attention_dropout",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_172",
          "type": "Linear",
          "name": "layer14_q_proj",
          "layer": "layer_14",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_173",
          "type": "Linear",
          "name": "layer14_k_proj",
          "layer": "layer_14",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_174",
          "type": "Linear",
          "name": "layer14_v_proj",
          "layer": "layer_14",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_175",
          "type": "Linear",
          "name": "layer14_o_proj",
          "layer": "layer_14",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_176",
          "type": "Linear",
          "name": "layer14_gate_proj",
          "layer": "layer_14",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_177",
          "type": "Linear",
          "name": "layer14_up_proj",
          "layer": "layer_14",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_178",
          "type": "Linear",
          "name": "layer14_down_proj",
          "layer": "layer_14",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_179",
          "type": "RMSNorm",
          "name": "layer14_input_layernorm",
          "layer": "layer_14",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_180",
          "type": "RMSNorm",
          "name": "layer14_post_attention_layernorm",
          "layer": "layer_14",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_181",
          "type": "Softmax",
          "name": "layer14_attention_softmax",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_182",
          "type": "SiLU",
          "name": "layer14_activation",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_183",
          "type": "Dropout",
          "name": "layer14_attention_dropout",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_184",
          "type": "Linear",
          "name": "layer15_q_proj",
          "layer": "layer_15",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_185",
          "type": "Linear",
          "name": "layer15_k_proj",
          "layer": "layer_15",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_186",
          "type": "Linear",
          "name": "layer15_v_proj",
          "layer": "layer_15",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_187",
          "type": "Linear",
          "name": "layer15_o_proj",
          "layer": "layer_15",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_188",
          "type": "Linear",
          "name": "layer15_gate_proj",
          "layer": "layer_15",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_189",
          "type": "Linear",
          "name": "layer15_up_proj",
          "layer": "layer_15",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_190",
          "type": "Linear",
          "name": "layer15_down_proj",
          "layer": "layer_15",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_191",
          "type": "RMSNorm",
          "name": "layer15_input_layernorm",
          "layer": "layer_15",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_192",
          "type": "RMSNorm",
          "name": "layer15_post_attention_layernorm",
          "layer": "layer_15",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_193",
          "type": "Softmax",
          "name": "layer15_attention_softmax",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_194",
          "type": "SiLU",
          "name": "layer15_activation",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_195",
          "type": "Dropout",
          "name": "layer15_attention_dropout",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_196",
          "type": "Linear",
          "name": "layer16_q_proj",
          "layer": "layer_16",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_197",
          "type": "Linear",
          "name": "layer16_k_proj",
          "layer": "layer_16",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_198",
          "type": "Linear",
          "name": "layer16_v_proj",
          "layer": "layer_16",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_199",
          "type": "Linear",
          "name": "layer16_o_proj",
          "layer": "layer_16",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_200",
          "type": "Linear",
          "name": "layer16_gate_proj",
          "layer": "layer_16",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_201",
          "type": "Linear",
          "name": "layer16_up_proj",
          "layer": "layer_16",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_202",
          "type": "Linear",
          "name": "layer16_down_proj",
          "layer": "layer_16",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_203",
          "type": "RMSNorm",
          "name": "layer16_input_layernorm",
          "layer": "layer_16",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_204",
          "type": "RMSNorm",
          "name": "layer16_post_attention_layernorm",
          "layer": "layer_16",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_205",
          "type": "Softmax",
          "name": "layer16_attention_softmax",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_206",
          "type": "SiLU",
          "name": "layer16_activation",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_207",
          "type": "Dropout",
          "name": "layer16_attention_dropout",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_208",
          "type": "Linear",
          "name": "layer17_q_proj",
          "layer": "layer_17",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_209",
          "type": "Linear",
          "name": "layer17_k_proj",
          "layer": "layer_17",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_210",
          "type": "Linear",
          "name": "layer17_v_proj",
          "layer": "layer_17",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_211",
          "type": "Linear",
          "name": "layer17_o_proj",
          "layer": "layer_17",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_212",
          "type": "Linear",
          "name": "layer17_gate_proj",
          "layer": "layer_17",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_213",
          "type": "Linear",
          "name": "layer17_up_proj",
          "layer": "layer_17",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_214",
          "type": "Linear",
          "name": "layer17_down_proj",
          "layer": "layer_17",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_215",
          "type": "RMSNorm",
          "name": "layer17_input_layernorm",
          "layer": "layer_17",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_216",
          "type": "RMSNorm",
          "name": "layer17_post_attention_layernorm",
          "layer": "layer_17",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_217",
          "type": "Softmax",
          "name": "layer17_attention_softmax",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_218",
          "type": "SiLU",
          "name": "layer17_activation",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_219",
          "type": "Dropout",
          "name": "layer17_attention_dropout",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_220",
          "type": "Linear",
          "name": "layer18_q_proj",
          "layer": "layer_18",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_221",
          "type": "Linear",
          "name": "layer18_k_proj",
          "layer": "layer_18",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_222",
          "type": "Linear",
          "name": "layer18_v_proj",
          "layer": "layer_18",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_223",
          "type": "Linear",
          "name": "layer18_o_proj",
          "layer": "layer_18",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_224",
          "type": "Linear",
          "name": "layer18_gate_proj",
          "layer": "layer_18",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_225",
          "type": "Linear",
          "name": "layer18_up_proj",
          "layer": "layer_18",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_226",
          "type": "Linear",
          "name": "layer18_down_proj",
          "layer": "layer_18",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_227",
          "type": "RMSNorm",
          "name": "layer18_input_layernorm",
          "layer": "layer_18",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_228",
          "type": "RMSNorm",
          "name": "layer18_post_attention_layernorm",
          "layer": "layer_18",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_229",
          "type": "Softmax",
          "name": "layer18_attention_softmax",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_230",
          "type": "SiLU",
          "name": "layer18_activation",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_231",
          "type": "Dropout",
          "name": "layer18_attention_dropout",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_232",
          "type": "Linear",
          "name": "layer19_q_proj",
          "layer": "layer_19",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_233",
          "type": "Linear",
          "name": "layer19_k_proj",
          "layer": "layer_19",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_234",
          "type": "Linear",
          "name": "layer19_v_proj",
          "layer": "layer_19",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_235",
          "type": "Linear",
          "name": "layer19_o_proj",
          "layer": "layer_19",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_236",
          "type": "Linear",
          "name": "layer19_gate_proj",
          "layer": "layer_19",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_237",
          "type": "Linear",
          "name": "layer19_up_proj",
          "layer": "layer_19",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_238",
          "type": "Linear",
          "name": "layer19_down_proj",
          "layer": "layer_19",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_239",
          "type": "RMSNorm",
          "name": "layer19_input_layernorm",
          "layer": "layer_19",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_240",
          "type": "RMSNorm",
          "name": "layer19_post_attention_layernorm",
          "layer": "layer_19",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_241",
          "type": "Softmax",
          "name": "layer19_attention_softmax",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_242",
          "type": "SiLU",
          "name": "layer19_activation",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_243",
          "type": "Dropout",
          "name": "layer19_attention_dropout",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_244",
          "type": "Linear",
          "name": "layer20_q_proj",
          "layer": "layer_20",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_245",
          "type": "Linear",
          "name": "layer20_k_proj",
          "layer": "layer_20",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_246",
          "type": "Linear",
          "name": "layer20_v_proj",
          "layer": "layer_20",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_247",
          "type": "Linear",
          "name": "layer20_o_proj",
          "layer": "layer_20",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_248",
          "type": "Linear",
          "name": "layer20_gate_proj",
          "layer": "layer_20",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_249",
          "type": "Linear",
          "name": "layer20_up_proj",
          "layer": "layer_20",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_250",
          "type": "Linear",
          "name": "layer20_down_proj",
          "layer": "layer_20",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_251",
          "type": "RMSNorm",
          "name": "layer20_input_layernorm",
          "layer": "layer_20",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_252",
          "type": "RMSNorm",
          "name": "layer20_post_attention_layernorm",
          "layer": "layer_20",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_253",
          "type": "Softmax",
          "name": "layer20_attention_softmax",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_254",
          "type": "SiLU",
          "name": "layer20_activation",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_255",
          "type": "Dropout",
          "name": "layer20_attention_dropout",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_256",
          "type": "Linear",
          "name": "layer21_q_proj",
          "layer": "layer_21",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_257",
          "type": "Linear",
          "name": "layer21_k_proj",
          "layer": "layer_21",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_258",
          "type": "Linear",
          "name": "layer21_v_proj",
          "layer": "layer_21",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_259",
          "type": "Linear",
          "name": "layer21_o_proj",
          "layer": "layer_21",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_260",
          "type": "Linear",
          "name": "layer21_gate_proj",
          "layer": "layer_21",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_261",
          "type": "Linear",
          "name": "layer21_up_proj",
          "layer": "layer_21",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_262",
          "type": "Linear",
          "name": "layer21_down_proj",
          "layer": "layer_21",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_263",
          "type": "RMSNorm",
          "name": "layer21_input_layernorm",
          "layer": "layer_21",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_264",
          "type": "RMSNorm",
          "name": "layer21_post_attention_layernorm",
          "layer": "layer_21",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_265",
          "type": "Softmax",
          "name": "layer21_attention_softmax",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_266",
          "type": "SiLU",
          "name": "layer21_activation",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_267",
          "type": "Dropout",
          "name": "layer21_attention_dropout",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_268",
          "type": "Linear",
          "name": "layer22_q_proj",
          "layer": "layer_22",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_269",
          "type": "Linear",
          "name": "layer22_k_proj",
          "layer": "layer_22",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_270",
          "type": "Linear",
          "name": "layer22_v_proj",
          "layer": "layer_22",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_271",
          "type": "Linear",
          "name": "layer22_o_proj",
          "layer": "layer_22",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_272",
          "type": "Linear",
          "name": "layer22_gate_proj",
          "layer": "layer_22",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_273",
          "type": "Linear",
          "name": "layer22_up_proj",
          "layer": "layer_22",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_274",
          "type": "Linear",
          "name": "layer22_down_proj",
          "layer": "layer_22",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_275",
          "type": "RMSNorm",
          "name": "layer22_input_layernorm",
          "layer": "layer_22",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_276",
          "type": "RMSNorm",
          "name": "layer22_post_attention_layernorm",
          "layer": "layer_22",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_277",
          "type": "Softmax",
          "name": "layer22_attention_softmax",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_278",
          "type": "SiLU",
          "name": "layer22_activation",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_279",
          "type": "Dropout",
          "name": "layer22_attention_dropout",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_280",
          "type": "Linear",
          "name": "layer23_q_proj",
          "layer": "layer_23",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_281",
          "type": "Linear",
          "name": "layer23_k_proj",
          "layer": "layer_23",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_282",
          "type": "Linear",
          "name": "layer23_v_proj",
          "layer": "layer_23",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_283",
          "type": "Linear",
          "name": "layer23_o_proj",
          "layer": "layer_23",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_284",
          "type": "Linear",
          "name": "layer23_gate_proj",
          "layer": "layer_23",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_285",
          "type": "Linear",
          "name": "layer23_up_proj",
          "layer": "layer_23",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_286",
          "type": "Linear",
          "name": "layer23_down_proj",
          "layer": "layer_23",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_287",
          "type": "RMSNorm",
          "name": "layer23_input_layernorm",
          "layer": "layer_23",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_288",
          "type": "RMSNorm",
          "name": "layer23_post_attention_layernorm",
          "layer": "layer_23",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_289",
          "type": "Softmax",
          "name": "layer23_attention_softmax",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_290",
          "type": "SiLU",
          "name": "layer23_activation",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_291",
          "type": "Dropout",
          "name": "layer23_attention_dropout",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_292",
          "type": "Linear",
          "name": "layer24_q_proj",
          "layer": "layer_24",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_293",
          "type": "Linear",
          "name": "layer24_k_proj",
          "layer": "layer_24",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_294",
          "type": "Linear",
          "name": "layer24_v_proj",
          "layer": "layer_24",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_295",
          "type": "Linear",
          "name": "layer24_o_proj",
          "layer": "layer_24",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_296",
          "type": "Linear",
          "name": "layer24_gate_proj",
          "layer": "layer_24",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_297",
          "type": "Linear",
          "name": "layer24_up_proj",
          "layer": "layer_24",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_298",
          "type": "Linear",
          "name": "layer24_down_proj",
          "layer": "layer_24",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_299",
          "type": "RMSNorm",
          "name": "layer24_input_layernorm",
          "layer": "layer_24",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_300",
          "type": "RMSNorm",
          "name": "layer24_post_attention_layernorm",
          "layer": "layer_24",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_301",
          "type": "Softmax",
          "name": "layer24_attention_softmax",
          "layer": "layer_24",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_302",
          "type": "SiLU",
          "name": "layer24_activation",
          "layer": "layer_24",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_303",
          "type": "Dropout",
          "name": "layer24_attention_dropout",
          "layer": "layer_24",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_304",
          "type": "Linear",
          "name": "layer25_q_proj",
          "layer": "layer_25",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_305",
          "type": "Linear",
          "name": "layer25_k_proj",
          "layer": "layer_25",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_306",
          "type": "Linear",
          "name": "layer25_v_proj",
          "layer": "layer_25",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_307",
          "type": "Linear",
          "name": "layer25_o_proj",
          "layer": "layer_25",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_308",
          "type": "Linear",
          "name": "layer25_gate_proj",
          "layer": "layer_25",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_309",
          "type": "Linear",
          "name": "layer25_up_proj",
          "layer": "layer_25",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_310",
          "type": "Linear",
          "name": "layer25_down_proj",
          "layer": "layer_25",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_311",
          "type": "RMSNorm",
          "name": "layer25_input_layernorm",
          "layer": "layer_25",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_312",
          "type": "RMSNorm",
          "name": "layer25_post_attention_layernorm",
          "layer": "layer_25",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_313",
          "type": "Softmax",
          "name": "layer25_attention_softmax",
          "layer": "layer_25",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_314",
          "type": "SiLU",
          "name": "layer25_activation",
          "layer": "layer_25",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_315",
          "type": "Dropout",
          "name": "layer25_attention_dropout",
          "layer": "layer_25",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_316",
          "type": "Linear",
          "name": "layer26_q_proj",
          "layer": "layer_26",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_317",
          "type": "Linear",
          "name": "layer26_k_proj",
          "layer": "layer_26",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_318",
          "type": "Linear",
          "name": "layer26_v_proj",
          "layer": "layer_26",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_319",
          "type": "Linear",
          "name": "layer26_o_proj",
          "layer": "layer_26",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_320",
          "type": "Linear",
          "name": "layer26_gate_proj",
          "layer": "layer_26",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_321",
          "type": "Linear",
          "name": "layer26_up_proj",
          "layer": "layer_26",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_322",
          "type": "Linear",
          "name": "layer26_down_proj",
          "layer": "layer_26",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_323",
          "type": "RMSNorm",
          "name": "layer26_input_layernorm",
          "layer": "layer_26",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_324",
          "type": "RMSNorm",
          "name": "layer26_post_attention_layernorm",
          "layer": "layer_26",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_325",
          "type": "Softmax",
          "name": "layer26_attention_softmax",
          "layer": "layer_26",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_326",
          "type": "SiLU",
          "name": "layer26_activation",
          "layer": "layer_26",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_327",
          "type": "Dropout",
          "name": "layer26_attention_dropout",
          "layer": "layer_26",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_328",
          "type": "Linear",
          "name": "layer27_q_proj",
          "layer": "layer_27",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_329",
          "type": "Linear",
          "name": "layer27_k_proj",
          "layer": "layer_27",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_330",
          "type": "Linear",
          "name": "layer27_v_proj",
          "layer": "layer_27",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_331",
          "type": "Linear",
          "name": "layer27_o_proj",
          "layer": "layer_27",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_332",
          "type": "Linear",
          "name": "layer27_gate_proj",
          "layer": "layer_27",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_333",
          "type": "Linear",
          "name": "layer27_up_proj",
          "layer": "layer_27",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_334",
          "type": "Linear",
          "name": "layer27_down_proj",
          "layer": "layer_27",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_335",
          "type": "RMSNorm",
          "name": "layer27_input_layernorm",
          "layer": "layer_27",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_336",
          "type": "RMSNorm",
          "name": "layer27_post_attention_layernorm",
          "layer": "layer_27",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_337",
          "type": "Softmax",
          "name": "layer27_attention_softmax",
          "layer": "layer_27",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_338",
          "type": "SiLU",
          "name": "layer27_activation",
          "layer": "layer_27",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_339",
          "type": "Dropout",
          "name": "layer27_attention_dropout",
          "layer": "layer_27",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_340",
          "type": "Linear",
          "name": "layer28_q_proj",
          "layer": "layer_28",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_341",
          "type": "Linear",
          "name": "layer28_k_proj",
          "layer": "layer_28",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_342",
          "type": "Linear",
          "name": "layer28_v_proj",
          "layer": "layer_28",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_343",
          "type": "Linear",
          "name": "layer28_o_proj",
          "layer": "layer_28",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_344",
          "type": "Linear",
          "name": "layer28_gate_proj",
          "layer": "layer_28",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_345",
          "type": "Linear",
          "name": "layer28_up_proj",
          "layer": "layer_28",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_346",
          "type": "Linear",
          "name": "layer28_down_proj",
          "layer": "layer_28",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_347",
          "type": "RMSNorm",
          "name": "layer28_input_layernorm",
          "layer": "layer_28",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_348",
          "type": "RMSNorm",
          "name": "layer28_post_attention_layernorm",
          "layer": "layer_28",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_349",
          "type": "Softmax",
          "name": "layer28_attention_softmax",
          "layer": "layer_28",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_350",
          "type": "SiLU",
          "name": "layer28_activation",
          "layer": "layer_28",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_351",
          "type": "Dropout",
          "name": "layer28_attention_dropout",
          "layer": "layer_28",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_352",
          "type": "Linear",
          "name": "layer29_q_proj",
          "layer": "layer_29",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_353",
          "type": "Linear",
          "name": "layer29_k_proj",
          "layer": "layer_29",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_354",
          "type": "Linear",
          "name": "layer29_v_proj",
          "layer": "layer_29",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_355",
          "type": "Linear",
          "name": "layer29_o_proj",
          "layer": "layer_29",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_356",
          "type": "Linear",
          "name": "layer29_gate_proj",
          "layer": "layer_29",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_357",
          "type": "Linear",
          "name": "layer29_up_proj",
          "layer": "layer_29",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_358",
          "type": "Linear",
          "name": "layer29_down_proj",
          "layer": "layer_29",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_359",
          "type": "RMSNorm",
          "name": "layer29_input_layernorm",
          "layer": "layer_29",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_360",
          "type": "RMSNorm",
          "name": "layer29_post_attention_layernorm",
          "layer": "layer_29",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_361",
          "type": "Softmax",
          "name": "layer29_attention_softmax",
          "layer": "layer_29",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_362",
          "type": "SiLU",
          "name": "layer29_activation",
          "layer": "layer_29",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_363",
          "type": "Dropout",
          "name": "layer29_attention_dropout",
          "layer": "layer_29",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_364",
          "type": "Linear",
          "name": "layer30_q_proj",
          "layer": "layer_30",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_365",
          "type": "Linear",
          "name": "layer30_k_proj",
          "layer": "layer_30",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_366",
          "type": "Linear",
          "name": "layer30_v_proj",
          "layer": "layer_30",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_367",
          "type": "Linear",
          "name": "layer30_o_proj",
          "layer": "layer_30",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_368",
          "type": "Linear",
          "name": "layer30_gate_proj",
          "layer": "layer_30",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_369",
          "type": "Linear",
          "name": "layer30_up_proj",
          "layer": "layer_30",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_370",
          "type": "Linear",
          "name": "layer30_down_proj",
          "layer": "layer_30",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_371",
          "type": "RMSNorm",
          "name": "layer30_input_layernorm",
          "layer": "layer_30",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_372",
          "type": "RMSNorm",
          "name": "layer30_post_attention_layernorm",
          "layer": "layer_30",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_373",
          "type": "Softmax",
          "name": "layer30_attention_softmax",
          "layer": "layer_30",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_374",
          "type": "SiLU",
          "name": "layer30_activation",
          "layer": "layer_30",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_375",
          "type": "Dropout",
          "name": "layer30_attention_dropout",
          "layer": "layer_30",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_376",
          "type": "Linear",
          "name": "layer31_q_proj",
          "layer": "layer_31",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_377",
          "type": "Linear",
          "name": "layer31_k_proj",
          "layer": "layer_31",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_378",
          "type": "Linear",
          "name": "layer31_v_proj",
          "layer": "layer_31",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_379",
          "type": "Linear",
          "name": "layer31_o_proj",
          "layer": "layer_31",
          "parameters": 9437184,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_380",
          "type": "Linear",
          "name": "layer31_gate_proj",
          "layer": "layer_31",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_381",
          "type": "Linear",
          "name": "layer31_up_proj",
          "layer": "layer_31",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_382",
          "type": "Linear",
          "name": "layer31_down_proj",
          "layer": "layer_31",
          "parameters": 25165824,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Phi_3_mini_op_383",
          "type": "RMSNorm",
          "name": "layer31_input_layernorm",
          "layer": "layer_31",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_384",
          "type": "RMSNorm",
          "name": "layer31_post_attention_layernorm",
          "layer": "layer_31",
          "parameters": 3072,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Phi_3_mini_op_385",
          "type": "Softmax",
          "name": "layer31_attention_softmax",
          "layer": "layer_31",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Phi_3_mini_op_386",
          "type": "SiLU",
          "name": "layer31_activation",
          "layer": "layer_31",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Phi_3_mini_op_387",
          "type": "Dropout",
          "name": "layer31_attention_dropout",
          "layer": "layer_31",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        }
      ],
      "parameters": 3800000000
    },
    {
      "model_id": "BLOOM_560M",
      "name": "BLOOM-560M",
      "source": "huggingface/bloom-560m",
      "category": "LLM",
      "architecture": "gpt2_style",
      "num_params": 560000000,
      "num_layers": 24,
      "hidden_size": 1024,
      "vocab_size": 250880,
      "intermediate_size": 4096,
      "operator_count": 243,
      "operators": [
        {
          "op_id": "BLOOM_560M_op_1",
          "type": "Embedding",
          "name": "wte",
          "layer": "global",
          "parameters": 1048576,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_2",
          "type": "Embedding",
          "name": "wpe",
          "layer": "global",
          "parameters": 1048576,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_3",
          "type": "LayerNorm",
          "name": "ln_f",
          "layer": "global",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_4",
          "type": "Linear",
          "name": "layer0_c_attn",
          "layer": "layer_0",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_5",
          "type": "Linear",
          "name": "layer0_c_proj",
          "layer": "layer_0",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_6",
          "type": "Linear",
          "name": "layer0_c_fc",
          "layer": "layer_0",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_7",
          "type": "Linear",
          "name": "layer0_c_proj_mlp",
          "layer": "layer_0",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_8",
          "type": "LayerNorm",
          "name": "layer0_ln_1",
          "layer": "layer_0",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_9",
          "type": "LayerNorm",
          "name": "layer0_ln_2",
          "layer": "layer_0",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_10",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_11",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_12",
          "type": "Dropout",
          "name": "layer0_attn_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_13",
          "type": "Dropout",
          "name": "layer0_resid_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_14",
          "type": "Linear",
          "name": "layer1_c_attn",
          "layer": "layer_1",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_15",
          "type": "Linear",
          "name": "layer1_c_proj",
          "layer": "layer_1",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_16",
          "type": "Linear",
          "name": "layer1_c_fc",
          "layer": "layer_1",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_17",
          "type": "Linear",
          "name": "layer1_c_proj_mlp",
          "layer": "layer_1",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_18",
          "type": "LayerNorm",
          "name": "layer1_ln_1",
          "layer": "layer_1",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_19",
          "type": "LayerNorm",
          "name": "layer1_ln_2",
          "layer": "layer_1",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_20",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_21",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_22",
          "type": "Dropout",
          "name": "layer1_attn_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_23",
          "type": "Dropout",
          "name": "layer1_resid_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_24",
          "type": "Linear",
          "name": "layer2_c_attn",
          "layer": "layer_2",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_25",
          "type": "Linear",
          "name": "layer2_c_proj",
          "layer": "layer_2",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_26",
          "type": "Linear",
          "name": "layer2_c_fc",
          "layer": "layer_2",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_27",
          "type": "Linear",
          "name": "layer2_c_proj_mlp",
          "layer": "layer_2",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_28",
          "type": "LayerNorm",
          "name": "layer2_ln_1",
          "layer": "layer_2",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_29",
          "type": "LayerNorm",
          "name": "layer2_ln_2",
          "layer": "layer_2",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_30",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_31",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_32",
          "type": "Dropout",
          "name": "layer2_attn_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_33",
          "type": "Dropout",
          "name": "layer2_resid_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_34",
          "type": "Linear",
          "name": "layer3_c_attn",
          "layer": "layer_3",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_35",
          "type": "Linear",
          "name": "layer3_c_proj",
          "layer": "layer_3",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_36",
          "type": "Linear",
          "name": "layer3_c_fc",
          "layer": "layer_3",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_37",
          "type": "Linear",
          "name": "layer3_c_proj_mlp",
          "layer": "layer_3",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_38",
          "type": "LayerNorm",
          "name": "layer3_ln_1",
          "layer": "layer_3",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_39",
          "type": "LayerNorm",
          "name": "layer3_ln_2",
          "layer": "layer_3",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_40",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_41",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_42",
          "type": "Dropout",
          "name": "layer3_attn_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_43",
          "type": "Dropout",
          "name": "layer3_resid_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_44",
          "type": "Linear",
          "name": "layer4_c_attn",
          "layer": "layer_4",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_45",
          "type": "Linear",
          "name": "layer4_c_proj",
          "layer": "layer_4",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_46",
          "type": "Linear",
          "name": "layer4_c_fc",
          "layer": "layer_4",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_47",
          "type": "Linear",
          "name": "layer4_c_proj_mlp",
          "layer": "layer_4",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_48",
          "type": "LayerNorm",
          "name": "layer4_ln_1",
          "layer": "layer_4",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_49",
          "type": "LayerNorm",
          "name": "layer4_ln_2",
          "layer": "layer_4",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_50",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_51",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_52",
          "type": "Dropout",
          "name": "layer4_attn_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_53",
          "type": "Dropout",
          "name": "layer4_resid_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_54",
          "type": "Linear",
          "name": "layer5_c_attn",
          "layer": "layer_5",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_55",
          "type": "Linear",
          "name": "layer5_c_proj",
          "layer": "layer_5",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_56",
          "type": "Linear",
          "name": "layer5_c_fc",
          "layer": "layer_5",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_57",
          "type": "Linear",
          "name": "layer5_c_proj_mlp",
          "layer": "layer_5",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_58",
          "type": "LayerNorm",
          "name": "layer5_ln_1",
          "layer": "layer_5",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_59",
          "type": "LayerNorm",
          "name": "layer5_ln_2",
          "layer": "layer_5",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_60",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_61",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_62",
          "type": "Dropout",
          "name": "layer5_attn_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_63",
          "type": "Dropout",
          "name": "layer5_resid_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_64",
          "type": "Linear",
          "name": "layer6_c_attn",
          "layer": "layer_6",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_65",
          "type": "Linear",
          "name": "layer6_c_proj",
          "layer": "layer_6",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_66",
          "type": "Linear",
          "name": "layer6_c_fc",
          "layer": "layer_6",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_67",
          "type": "Linear",
          "name": "layer6_c_proj_mlp",
          "layer": "layer_6",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_68",
          "type": "LayerNorm",
          "name": "layer6_ln_1",
          "layer": "layer_6",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_69",
          "type": "LayerNorm",
          "name": "layer6_ln_2",
          "layer": "layer_6",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_70",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_71",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_72",
          "type": "Dropout",
          "name": "layer6_attn_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_73",
          "type": "Dropout",
          "name": "layer6_resid_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_74",
          "type": "Linear",
          "name": "layer7_c_attn",
          "layer": "layer_7",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_75",
          "type": "Linear",
          "name": "layer7_c_proj",
          "layer": "layer_7",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_76",
          "type": "Linear",
          "name": "layer7_c_fc",
          "layer": "layer_7",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_77",
          "type": "Linear",
          "name": "layer7_c_proj_mlp",
          "layer": "layer_7",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_78",
          "type": "LayerNorm",
          "name": "layer7_ln_1",
          "layer": "layer_7",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_79",
          "type": "LayerNorm",
          "name": "layer7_ln_2",
          "layer": "layer_7",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_80",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_81",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_82",
          "type": "Dropout",
          "name": "layer7_attn_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_83",
          "type": "Dropout",
          "name": "layer7_resid_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_84",
          "type": "Linear",
          "name": "layer8_c_attn",
          "layer": "layer_8",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_85",
          "type": "Linear",
          "name": "layer8_c_proj",
          "layer": "layer_8",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_86",
          "type": "Linear",
          "name": "layer8_c_fc",
          "layer": "layer_8",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_87",
          "type": "Linear",
          "name": "layer8_c_proj_mlp",
          "layer": "layer_8",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_88",
          "type": "LayerNorm",
          "name": "layer8_ln_1",
          "layer": "layer_8",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_89",
          "type": "LayerNorm",
          "name": "layer8_ln_2",
          "layer": "layer_8",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_90",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_91",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_92",
          "type": "Dropout",
          "name": "layer8_attn_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_93",
          "type": "Dropout",
          "name": "layer8_resid_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_94",
          "type": "Linear",
          "name": "layer9_c_attn",
          "layer": "layer_9",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_95",
          "type": "Linear",
          "name": "layer9_c_proj",
          "layer": "layer_9",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_96",
          "type": "Linear",
          "name": "layer9_c_fc",
          "layer": "layer_9",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_97",
          "type": "Linear",
          "name": "layer9_c_proj_mlp",
          "layer": "layer_9",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_98",
          "type": "LayerNorm",
          "name": "layer9_ln_1",
          "layer": "layer_9",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_99",
          "type": "LayerNorm",
          "name": "layer9_ln_2",
          "layer": "layer_9",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_100",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_101",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_102",
          "type": "Dropout",
          "name": "layer9_attn_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_103",
          "type": "Dropout",
          "name": "layer9_resid_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_104",
          "type": "Linear",
          "name": "layer10_c_attn",
          "layer": "layer_10",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_105",
          "type": "Linear",
          "name": "layer10_c_proj",
          "layer": "layer_10",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_106",
          "type": "Linear",
          "name": "layer10_c_fc",
          "layer": "layer_10",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_107",
          "type": "Linear",
          "name": "layer10_c_proj_mlp",
          "layer": "layer_10",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_108",
          "type": "LayerNorm",
          "name": "layer10_ln_1",
          "layer": "layer_10",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_109",
          "type": "LayerNorm",
          "name": "layer10_ln_2",
          "layer": "layer_10",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_110",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_111",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_112",
          "type": "Dropout",
          "name": "layer10_attn_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_113",
          "type": "Dropout",
          "name": "layer10_resid_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_114",
          "type": "Linear",
          "name": "layer11_c_attn",
          "layer": "layer_11",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_115",
          "type": "Linear",
          "name": "layer11_c_proj",
          "layer": "layer_11",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_116",
          "type": "Linear",
          "name": "layer11_c_fc",
          "layer": "layer_11",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_117",
          "type": "Linear",
          "name": "layer11_c_proj_mlp",
          "layer": "layer_11",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_118",
          "type": "LayerNorm",
          "name": "layer11_ln_1",
          "layer": "layer_11",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_119",
          "type": "LayerNorm",
          "name": "layer11_ln_2",
          "layer": "layer_11",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_120",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_121",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_122",
          "type": "Dropout",
          "name": "layer11_attn_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_123",
          "type": "Dropout",
          "name": "layer11_resid_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_124",
          "type": "Linear",
          "name": "layer12_c_attn",
          "layer": "layer_12",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_125",
          "type": "Linear",
          "name": "layer12_c_proj",
          "layer": "layer_12",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_126",
          "type": "Linear",
          "name": "layer12_c_fc",
          "layer": "layer_12",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_127",
          "type": "Linear",
          "name": "layer12_c_proj_mlp",
          "layer": "layer_12",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_128",
          "type": "LayerNorm",
          "name": "layer12_ln_1",
          "layer": "layer_12",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_129",
          "type": "LayerNorm",
          "name": "layer12_ln_2",
          "layer": "layer_12",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_130",
          "type": "Softmax",
          "name": "layer12_attention_softmax",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_131",
          "type": "GELU",
          "name": "layer12_activation",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_132",
          "type": "Dropout",
          "name": "layer12_attn_dropout",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_133",
          "type": "Dropout",
          "name": "layer12_resid_dropout",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_134",
          "type": "Linear",
          "name": "layer13_c_attn",
          "layer": "layer_13",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_135",
          "type": "Linear",
          "name": "layer13_c_proj",
          "layer": "layer_13",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_136",
          "type": "Linear",
          "name": "layer13_c_fc",
          "layer": "layer_13",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_137",
          "type": "Linear",
          "name": "layer13_c_proj_mlp",
          "layer": "layer_13",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_138",
          "type": "LayerNorm",
          "name": "layer13_ln_1",
          "layer": "layer_13",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_139",
          "type": "LayerNorm",
          "name": "layer13_ln_2",
          "layer": "layer_13",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_140",
          "type": "Softmax",
          "name": "layer13_attention_softmax",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_141",
          "type": "GELU",
          "name": "layer13_activation",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_142",
          "type": "Dropout",
          "name": "layer13_attn_dropout",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_143",
          "type": "Dropout",
          "name": "layer13_resid_dropout",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_144",
          "type": "Linear",
          "name": "layer14_c_attn",
          "layer": "layer_14",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_145",
          "type": "Linear",
          "name": "layer14_c_proj",
          "layer": "layer_14",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_146",
          "type": "Linear",
          "name": "layer14_c_fc",
          "layer": "layer_14",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_147",
          "type": "Linear",
          "name": "layer14_c_proj_mlp",
          "layer": "layer_14",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_148",
          "type": "LayerNorm",
          "name": "layer14_ln_1",
          "layer": "layer_14",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_149",
          "type": "LayerNorm",
          "name": "layer14_ln_2",
          "layer": "layer_14",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_150",
          "type": "Softmax",
          "name": "layer14_attention_softmax",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_151",
          "type": "GELU",
          "name": "layer14_activation",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_152",
          "type": "Dropout",
          "name": "layer14_attn_dropout",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_153",
          "type": "Dropout",
          "name": "layer14_resid_dropout",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_154",
          "type": "Linear",
          "name": "layer15_c_attn",
          "layer": "layer_15",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_155",
          "type": "Linear",
          "name": "layer15_c_proj",
          "layer": "layer_15",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_156",
          "type": "Linear",
          "name": "layer15_c_fc",
          "layer": "layer_15",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_157",
          "type": "Linear",
          "name": "layer15_c_proj_mlp",
          "layer": "layer_15",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_158",
          "type": "LayerNorm",
          "name": "layer15_ln_1",
          "layer": "layer_15",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_159",
          "type": "LayerNorm",
          "name": "layer15_ln_2",
          "layer": "layer_15",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_160",
          "type": "Softmax",
          "name": "layer15_attention_softmax",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_161",
          "type": "GELU",
          "name": "layer15_activation",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_162",
          "type": "Dropout",
          "name": "layer15_attn_dropout",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_163",
          "type": "Dropout",
          "name": "layer15_resid_dropout",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_164",
          "type": "Linear",
          "name": "layer16_c_attn",
          "layer": "layer_16",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_165",
          "type": "Linear",
          "name": "layer16_c_proj",
          "layer": "layer_16",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_166",
          "type": "Linear",
          "name": "layer16_c_fc",
          "layer": "layer_16",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_167",
          "type": "Linear",
          "name": "layer16_c_proj_mlp",
          "layer": "layer_16",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_168",
          "type": "LayerNorm",
          "name": "layer16_ln_1",
          "layer": "layer_16",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_169",
          "type": "LayerNorm",
          "name": "layer16_ln_2",
          "layer": "layer_16",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_170",
          "type": "Softmax",
          "name": "layer16_attention_softmax",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_171",
          "type": "GELU",
          "name": "layer16_activation",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_172",
          "type": "Dropout",
          "name": "layer16_attn_dropout",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_173",
          "type": "Dropout",
          "name": "layer16_resid_dropout",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_174",
          "type": "Linear",
          "name": "layer17_c_attn",
          "layer": "layer_17",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_175",
          "type": "Linear",
          "name": "layer17_c_proj",
          "layer": "layer_17",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_176",
          "type": "Linear",
          "name": "layer17_c_fc",
          "layer": "layer_17",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_177",
          "type": "Linear",
          "name": "layer17_c_proj_mlp",
          "layer": "layer_17",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_178",
          "type": "LayerNorm",
          "name": "layer17_ln_1",
          "layer": "layer_17",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_179",
          "type": "LayerNorm",
          "name": "layer17_ln_2",
          "layer": "layer_17",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_180",
          "type": "Softmax",
          "name": "layer17_attention_softmax",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_181",
          "type": "GELU",
          "name": "layer17_activation",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_182",
          "type": "Dropout",
          "name": "layer17_attn_dropout",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_183",
          "type": "Dropout",
          "name": "layer17_resid_dropout",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_184",
          "type": "Linear",
          "name": "layer18_c_attn",
          "layer": "layer_18",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_185",
          "type": "Linear",
          "name": "layer18_c_proj",
          "layer": "layer_18",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_186",
          "type": "Linear",
          "name": "layer18_c_fc",
          "layer": "layer_18",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_187",
          "type": "Linear",
          "name": "layer18_c_proj_mlp",
          "layer": "layer_18",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_188",
          "type": "LayerNorm",
          "name": "layer18_ln_1",
          "layer": "layer_18",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_189",
          "type": "LayerNorm",
          "name": "layer18_ln_2",
          "layer": "layer_18",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_190",
          "type": "Softmax",
          "name": "layer18_attention_softmax",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_191",
          "type": "GELU",
          "name": "layer18_activation",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_192",
          "type": "Dropout",
          "name": "layer18_attn_dropout",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_193",
          "type": "Dropout",
          "name": "layer18_resid_dropout",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_194",
          "type": "Linear",
          "name": "layer19_c_attn",
          "layer": "layer_19",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_195",
          "type": "Linear",
          "name": "layer19_c_proj",
          "layer": "layer_19",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_196",
          "type": "Linear",
          "name": "layer19_c_fc",
          "layer": "layer_19",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_197",
          "type": "Linear",
          "name": "layer19_c_proj_mlp",
          "layer": "layer_19",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_198",
          "type": "LayerNorm",
          "name": "layer19_ln_1",
          "layer": "layer_19",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_199",
          "type": "LayerNorm",
          "name": "layer19_ln_2",
          "layer": "layer_19",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_200",
          "type": "Softmax",
          "name": "layer19_attention_softmax",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_201",
          "type": "GELU",
          "name": "layer19_activation",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_202",
          "type": "Dropout",
          "name": "layer19_attn_dropout",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_203",
          "type": "Dropout",
          "name": "layer19_resid_dropout",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_204",
          "type": "Linear",
          "name": "layer20_c_attn",
          "layer": "layer_20",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_205",
          "type": "Linear",
          "name": "layer20_c_proj",
          "layer": "layer_20",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_206",
          "type": "Linear",
          "name": "layer20_c_fc",
          "layer": "layer_20",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_207",
          "type": "Linear",
          "name": "layer20_c_proj_mlp",
          "layer": "layer_20",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_208",
          "type": "LayerNorm",
          "name": "layer20_ln_1",
          "layer": "layer_20",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_209",
          "type": "LayerNorm",
          "name": "layer20_ln_2",
          "layer": "layer_20",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_210",
          "type": "Softmax",
          "name": "layer20_attention_softmax",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_211",
          "type": "GELU",
          "name": "layer20_activation",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_212",
          "type": "Dropout",
          "name": "layer20_attn_dropout",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_213",
          "type": "Dropout",
          "name": "layer20_resid_dropout",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_214",
          "type": "Linear",
          "name": "layer21_c_attn",
          "layer": "layer_21",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_215",
          "type": "Linear",
          "name": "layer21_c_proj",
          "layer": "layer_21",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_216",
          "type": "Linear",
          "name": "layer21_c_fc",
          "layer": "layer_21",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_217",
          "type": "Linear",
          "name": "layer21_c_proj_mlp",
          "layer": "layer_21",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_218",
          "type": "LayerNorm",
          "name": "layer21_ln_1",
          "layer": "layer_21",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_219",
          "type": "LayerNorm",
          "name": "layer21_ln_2",
          "layer": "layer_21",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_220",
          "type": "Softmax",
          "name": "layer21_attention_softmax",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_221",
          "type": "GELU",
          "name": "layer21_activation",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_222",
          "type": "Dropout",
          "name": "layer21_attn_dropout",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_223",
          "type": "Dropout",
          "name": "layer21_resid_dropout",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_224",
          "type": "Linear",
          "name": "layer22_c_attn",
          "layer": "layer_22",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_225",
          "type": "Linear",
          "name": "layer22_c_proj",
          "layer": "layer_22",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_226",
          "type": "Linear",
          "name": "layer22_c_fc",
          "layer": "layer_22",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_227",
          "type": "Linear",
          "name": "layer22_c_proj_mlp",
          "layer": "layer_22",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_228",
          "type": "LayerNorm",
          "name": "layer22_ln_1",
          "layer": "layer_22",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_229",
          "type": "LayerNorm",
          "name": "layer22_ln_2",
          "layer": "layer_22",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_230",
          "type": "Softmax",
          "name": "layer22_attention_softmax",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_231",
          "type": "GELU",
          "name": "layer22_activation",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_232",
          "type": "Dropout",
          "name": "layer22_attn_dropout",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_233",
          "type": "Dropout",
          "name": "layer22_resid_dropout",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_234",
          "type": "Linear",
          "name": "layer23_c_attn",
          "layer": "layer_23",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_235",
          "type": "Linear",
          "name": "layer23_c_proj",
          "layer": "layer_23",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_236",
          "type": "Linear",
          "name": "layer23_c_fc",
          "layer": "layer_23",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_237",
          "type": "Linear",
          "name": "layer23_c_proj_mlp",
          "layer": "layer_23",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLOOM_560M_op_238",
          "type": "LayerNorm",
          "name": "layer23_ln_1",
          "layer": "layer_23",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_239",
          "type": "LayerNorm",
          "name": "layer23_ln_2",
          "layer": "layer_23",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_240",
          "type": "Softmax",
          "name": "layer23_attention_softmax",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLOOM_560M_op_241",
          "type": "GELU",
          "name": "layer23_activation",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "BLOOM_560M_op_242",
          "type": "Dropout",
          "name": "layer23_attn_dropout",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BLOOM_560M_op_243",
          "type": "Dropout",
          "name": "layer23_resid_dropout",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        }
      ],
      "parameters": 560000000
    },
    {
      "model_id": "GPT_2",
      "name": "GPT-2",
      "source": "huggingface/gpt-2",
      "category": "LLM",
      "architecture": "gpt2_style",
      "num_params": 124000000,
      "num_layers": 12,
      "hidden_size": 768,
      "vocab_size": 50257,
      "intermediate_size": 3072,
      "operator_count": 123,
      "operators": [
        {
          "op_id": "GPT_2_op_1",
          "type": "Embedding",
          "name": "wte",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_2",
          "type": "Embedding",
          "name": "wpe",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_3",
          "type": "LayerNorm",
          "name": "ln_f",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_4",
          "type": "Linear",
          "name": "layer0_c_attn",
          "layer": "layer_0",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_5",
          "type": "Linear",
          "name": "layer0_c_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_6",
          "type": "Linear",
          "name": "layer0_c_fc",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_7",
          "type": "Linear",
          "name": "layer0_c_proj_mlp",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_8",
          "type": "LayerNorm",
          "name": "layer0_ln_1",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_9",
          "type": "LayerNorm",
          "name": "layer0_ln_2",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_10",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_2_op_11",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "GPT_2_op_12",
          "type": "Dropout",
          "name": "layer0_attn_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_13",
          "type": "Dropout",
          "name": "layer0_resid_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_14",
          "type": "Linear",
          "name": "layer1_c_attn",
          "layer": "layer_1",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_15",
          "type": "Linear",
          "name": "layer1_c_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_16",
          "type": "Linear",
          "name": "layer1_c_fc",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_17",
          "type": "Linear",
          "name": "layer1_c_proj_mlp",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_18",
          "type": "LayerNorm",
          "name": "layer1_ln_1",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_19",
          "type": "LayerNorm",
          "name": "layer1_ln_2",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_20",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_2_op_21",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "GPT_2_op_22",
          "type": "Dropout",
          "name": "layer1_attn_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_23",
          "type": "Dropout",
          "name": "layer1_resid_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_24",
          "type": "Linear",
          "name": "layer2_c_attn",
          "layer": "layer_2",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_25",
          "type": "Linear",
          "name": "layer2_c_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_26",
          "type": "Linear",
          "name": "layer2_c_fc",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_27",
          "type": "Linear",
          "name": "layer2_c_proj_mlp",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_28",
          "type": "LayerNorm",
          "name": "layer2_ln_1",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_29",
          "type": "LayerNorm",
          "name": "layer2_ln_2",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_30",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_2_op_31",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "GPT_2_op_32",
          "type": "Dropout",
          "name": "layer2_attn_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_33",
          "type": "Dropout",
          "name": "layer2_resid_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_34",
          "type": "Linear",
          "name": "layer3_c_attn",
          "layer": "layer_3",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_35",
          "type": "Linear",
          "name": "layer3_c_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_36",
          "type": "Linear",
          "name": "layer3_c_fc",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_37",
          "type": "Linear",
          "name": "layer3_c_proj_mlp",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_38",
          "type": "LayerNorm",
          "name": "layer3_ln_1",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_39",
          "type": "LayerNorm",
          "name": "layer3_ln_2",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_40",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_2_op_41",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "GPT_2_op_42",
          "type": "Dropout",
          "name": "layer3_attn_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_43",
          "type": "Dropout",
          "name": "layer3_resid_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_44",
          "type": "Linear",
          "name": "layer4_c_attn",
          "layer": "layer_4",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_45",
          "type": "Linear",
          "name": "layer4_c_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_46",
          "type": "Linear",
          "name": "layer4_c_fc",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_47",
          "type": "Linear",
          "name": "layer4_c_proj_mlp",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_48",
          "type": "LayerNorm",
          "name": "layer4_ln_1",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_49",
          "type": "LayerNorm",
          "name": "layer4_ln_2",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_50",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_2_op_51",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "GPT_2_op_52",
          "type": "Dropout",
          "name": "layer4_attn_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_53",
          "type": "Dropout",
          "name": "layer4_resid_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_54",
          "type": "Linear",
          "name": "layer5_c_attn",
          "layer": "layer_5",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_55",
          "type": "Linear",
          "name": "layer5_c_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_56",
          "type": "Linear",
          "name": "layer5_c_fc",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_57",
          "type": "Linear",
          "name": "layer5_c_proj_mlp",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_58",
          "type": "LayerNorm",
          "name": "layer5_ln_1",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_59",
          "type": "LayerNorm",
          "name": "layer5_ln_2",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_60",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_2_op_61",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "GPT_2_op_62",
          "type": "Dropout",
          "name": "layer5_attn_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_63",
          "type": "Dropout",
          "name": "layer5_resid_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_64",
          "type": "Linear",
          "name": "layer6_c_attn",
          "layer": "layer_6",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_65",
          "type": "Linear",
          "name": "layer6_c_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_66",
          "type": "Linear",
          "name": "layer6_c_fc",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_67",
          "type": "Linear",
          "name": "layer6_c_proj_mlp",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_68",
          "type": "LayerNorm",
          "name": "layer6_ln_1",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_69",
          "type": "LayerNorm",
          "name": "layer6_ln_2",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_70",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_2_op_71",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "GPT_2_op_72",
          "type": "Dropout",
          "name": "layer6_attn_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_73",
          "type": "Dropout",
          "name": "layer6_resid_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_74",
          "type": "Linear",
          "name": "layer7_c_attn",
          "layer": "layer_7",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_75",
          "type": "Linear",
          "name": "layer7_c_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_76",
          "type": "Linear",
          "name": "layer7_c_fc",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_77",
          "type": "Linear",
          "name": "layer7_c_proj_mlp",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_78",
          "type": "LayerNorm",
          "name": "layer7_ln_1",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_79",
          "type": "LayerNorm",
          "name": "layer7_ln_2",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_80",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_2_op_81",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "GPT_2_op_82",
          "type": "Dropout",
          "name": "layer7_attn_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_83",
          "type": "Dropout",
          "name": "layer7_resid_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_84",
          "type": "Linear",
          "name": "layer8_c_attn",
          "layer": "layer_8",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_85",
          "type": "Linear",
          "name": "layer8_c_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_86",
          "type": "Linear",
          "name": "layer8_c_fc",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_87",
          "type": "Linear",
          "name": "layer8_c_proj_mlp",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_88",
          "type": "LayerNorm",
          "name": "layer8_ln_1",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_89",
          "type": "LayerNorm",
          "name": "layer8_ln_2",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_90",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_2_op_91",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "GPT_2_op_92",
          "type": "Dropout",
          "name": "layer8_attn_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_93",
          "type": "Dropout",
          "name": "layer8_resid_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_94",
          "type": "Linear",
          "name": "layer9_c_attn",
          "layer": "layer_9",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_95",
          "type": "Linear",
          "name": "layer9_c_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_96",
          "type": "Linear",
          "name": "layer9_c_fc",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_97",
          "type": "Linear",
          "name": "layer9_c_proj_mlp",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_98",
          "type": "LayerNorm",
          "name": "layer9_ln_1",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_99",
          "type": "LayerNorm",
          "name": "layer9_ln_2",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_100",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_2_op_101",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "GPT_2_op_102",
          "type": "Dropout",
          "name": "layer9_attn_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_103",
          "type": "Dropout",
          "name": "layer9_resid_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_104",
          "type": "Linear",
          "name": "layer10_c_attn",
          "layer": "layer_10",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_105",
          "type": "Linear",
          "name": "layer10_c_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_106",
          "type": "Linear",
          "name": "layer10_c_fc",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_107",
          "type": "Linear",
          "name": "layer10_c_proj_mlp",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_108",
          "type": "LayerNorm",
          "name": "layer10_ln_1",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_109",
          "type": "LayerNorm",
          "name": "layer10_ln_2",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_110",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_2_op_111",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "GPT_2_op_112",
          "type": "Dropout",
          "name": "layer10_attn_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_113",
          "type": "Dropout",
          "name": "layer10_resid_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_114",
          "type": "Linear",
          "name": "layer11_c_attn",
          "layer": "layer_11",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_115",
          "type": "Linear",
          "name": "layer11_c_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_116",
          "type": "Linear",
          "name": "layer11_c_fc",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_117",
          "type": "Linear",
          "name": "layer11_c_proj_mlp",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_2_op_118",
          "type": "LayerNorm",
          "name": "layer11_ln_1",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_119",
          "type": "LayerNorm",
          "name": "layer11_ln_2",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_120",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_2_op_121",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "GPT_2_op_122",
          "type": "Dropout",
          "name": "layer11_attn_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "GPT_2_op_123",
          "type": "Dropout",
          "name": "layer11_resid_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        }
      ],
      "parameters": 124000000
    },
    {
      "model_id": "OPT_1.3B",
      "name": "OPT-1.3B",
      "source": "huggingface/opt-1.3b",
      "category": "LLM",
      "architecture": "gpt2_style",
      "num_params": 1300000000,
      "num_layers": 24,
      "hidden_size": 2048,
      "vocab_size": 50272,
      "intermediate_size": 8192,
      "operator_count": 243,
      "operators": [
        {
          "op_id": "OPT_1.3B_op_1",
          "type": "Embedding",
          "name": "wte",
          "layer": "global",
          "parameters": 4194304,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_2",
          "type": "Embedding",
          "name": "wpe",
          "layer": "global",
          "parameters": 4194304,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_3",
          "type": "LayerNorm",
          "name": "ln_f",
          "layer": "global",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_4",
          "type": "Linear",
          "name": "layer0_c_attn",
          "layer": "layer_0",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_5",
          "type": "Linear",
          "name": "layer0_c_proj",
          "layer": "layer_0",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_6",
          "type": "Linear",
          "name": "layer0_c_fc",
          "layer": "layer_0",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_7",
          "type": "Linear",
          "name": "layer0_c_proj_mlp",
          "layer": "layer_0",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_8",
          "type": "LayerNorm",
          "name": "layer0_ln_1",
          "layer": "layer_0",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_9",
          "type": "LayerNorm",
          "name": "layer0_ln_2",
          "layer": "layer_0",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_10",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_11",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_12",
          "type": "Dropout",
          "name": "layer0_attn_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_13",
          "type": "Dropout",
          "name": "layer0_resid_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_14",
          "type": "Linear",
          "name": "layer1_c_attn",
          "layer": "layer_1",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_15",
          "type": "Linear",
          "name": "layer1_c_proj",
          "layer": "layer_1",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_16",
          "type": "Linear",
          "name": "layer1_c_fc",
          "layer": "layer_1",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_17",
          "type": "Linear",
          "name": "layer1_c_proj_mlp",
          "layer": "layer_1",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_18",
          "type": "LayerNorm",
          "name": "layer1_ln_1",
          "layer": "layer_1",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_19",
          "type": "LayerNorm",
          "name": "layer1_ln_2",
          "layer": "layer_1",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_20",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_21",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_22",
          "type": "Dropout",
          "name": "layer1_attn_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_23",
          "type": "Dropout",
          "name": "layer1_resid_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_24",
          "type": "Linear",
          "name": "layer2_c_attn",
          "layer": "layer_2",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_25",
          "type": "Linear",
          "name": "layer2_c_proj",
          "layer": "layer_2",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_26",
          "type": "Linear",
          "name": "layer2_c_fc",
          "layer": "layer_2",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_27",
          "type": "Linear",
          "name": "layer2_c_proj_mlp",
          "layer": "layer_2",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_28",
          "type": "LayerNorm",
          "name": "layer2_ln_1",
          "layer": "layer_2",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_29",
          "type": "LayerNorm",
          "name": "layer2_ln_2",
          "layer": "layer_2",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_30",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_31",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_32",
          "type": "Dropout",
          "name": "layer2_attn_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_33",
          "type": "Dropout",
          "name": "layer2_resid_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_34",
          "type": "Linear",
          "name": "layer3_c_attn",
          "layer": "layer_3",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_35",
          "type": "Linear",
          "name": "layer3_c_proj",
          "layer": "layer_3",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_36",
          "type": "Linear",
          "name": "layer3_c_fc",
          "layer": "layer_3",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_37",
          "type": "Linear",
          "name": "layer3_c_proj_mlp",
          "layer": "layer_3",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_38",
          "type": "LayerNorm",
          "name": "layer3_ln_1",
          "layer": "layer_3",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_39",
          "type": "LayerNorm",
          "name": "layer3_ln_2",
          "layer": "layer_3",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_40",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_41",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_42",
          "type": "Dropout",
          "name": "layer3_attn_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_43",
          "type": "Dropout",
          "name": "layer3_resid_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_44",
          "type": "Linear",
          "name": "layer4_c_attn",
          "layer": "layer_4",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_45",
          "type": "Linear",
          "name": "layer4_c_proj",
          "layer": "layer_4",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_46",
          "type": "Linear",
          "name": "layer4_c_fc",
          "layer": "layer_4",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_47",
          "type": "Linear",
          "name": "layer4_c_proj_mlp",
          "layer": "layer_4",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_48",
          "type": "LayerNorm",
          "name": "layer4_ln_1",
          "layer": "layer_4",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_49",
          "type": "LayerNorm",
          "name": "layer4_ln_2",
          "layer": "layer_4",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_50",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_51",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_52",
          "type": "Dropout",
          "name": "layer4_attn_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_53",
          "type": "Dropout",
          "name": "layer4_resid_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_54",
          "type": "Linear",
          "name": "layer5_c_attn",
          "layer": "layer_5",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_55",
          "type": "Linear",
          "name": "layer5_c_proj",
          "layer": "layer_5",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_56",
          "type": "Linear",
          "name": "layer5_c_fc",
          "layer": "layer_5",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_57",
          "type": "Linear",
          "name": "layer5_c_proj_mlp",
          "layer": "layer_5",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_58",
          "type": "LayerNorm",
          "name": "layer5_ln_1",
          "layer": "layer_5",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_59",
          "type": "LayerNorm",
          "name": "layer5_ln_2",
          "layer": "layer_5",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_60",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_61",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_62",
          "type": "Dropout",
          "name": "layer5_attn_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_63",
          "type": "Dropout",
          "name": "layer5_resid_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_64",
          "type": "Linear",
          "name": "layer6_c_attn",
          "layer": "layer_6",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_65",
          "type": "Linear",
          "name": "layer6_c_proj",
          "layer": "layer_6",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_66",
          "type": "Linear",
          "name": "layer6_c_fc",
          "layer": "layer_6",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_67",
          "type": "Linear",
          "name": "layer6_c_proj_mlp",
          "layer": "layer_6",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_68",
          "type": "LayerNorm",
          "name": "layer6_ln_1",
          "layer": "layer_6",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_69",
          "type": "LayerNorm",
          "name": "layer6_ln_2",
          "layer": "layer_6",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_70",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_71",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_72",
          "type": "Dropout",
          "name": "layer6_attn_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_73",
          "type": "Dropout",
          "name": "layer6_resid_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_74",
          "type": "Linear",
          "name": "layer7_c_attn",
          "layer": "layer_7",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_75",
          "type": "Linear",
          "name": "layer7_c_proj",
          "layer": "layer_7",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_76",
          "type": "Linear",
          "name": "layer7_c_fc",
          "layer": "layer_7",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_77",
          "type": "Linear",
          "name": "layer7_c_proj_mlp",
          "layer": "layer_7",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_78",
          "type": "LayerNorm",
          "name": "layer7_ln_1",
          "layer": "layer_7",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_79",
          "type": "LayerNorm",
          "name": "layer7_ln_2",
          "layer": "layer_7",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_80",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_81",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_82",
          "type": "Dropout",
          "name": "layer7_attn_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_83",
          "type": "Dropout",
          "name": "layer7_resid_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_84",
          "type": "Linear",
          "name": "layer8_c_attn",
          "layer": "layer_8",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_85",
          "type": "Linear",
          "name": "layer8_c_proj",
          "layer": "layer_8",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_86",
          "type": "Linear",
          "name": "layer8_c_fc",
          "layer": "layer_8",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_87",
          "type": "Linear",
          "name": "layer8_c_proj_mlp",
          "layer": "layer_8",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_88",
          "type": "LayerNorm",
          "name": "layer8_ln_1",
          "layer": "layer_8",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_89",
          "type": "LayerNorm",
          "name": "layer8_ln_2",
          "layer": "layer_8",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_90",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_91",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_92",
          "type": "Dropout",
          "name": "layer8_attn_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_93",
          "type": "Dropout",
          "name": "layer8_resid_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_94",
          "type": "Linear",
          "name": "layer9_c_attn",
          "layer": "layer_9",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_95",
          "type": "Linear",
          "name": "layer9_c_proj",
          "layer": "layer_9",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_96",
          "type": "Linear",
          "name": "layer9_c_fc",
          "layer": "layer_9",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_97",
          "type": "Linear",
          "name": "layer9_c_proj_mlp",
          "layer": "layer_9",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_98",
          "type": "LayerNorm",
          "name": "layer9_ln_1",
          "layer": "layer_9",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_99",
          "type": "LayerNorm",
          "name": "layer9_ln_2",
          "layer": "layer_9",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_100",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_101",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_102",
          "type": "Dropout",
          "name": "layer9_attn_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_103",
          "type": "Dropout",
          "name": "layer9_resid_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_104",
          "type": "Linear",
          "name": "layer10_c_attn",
          "layer": "layer_10",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_105",
          "type": "Linear",
          "name": "layer10_c_proj",
          "layer": "layer_10",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_106",
          "type": "Linear",
          "name": "layer10_c_fc",
          "layer": "layer_10",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_107",
          "type": "Linear",
          "name": "layer10_c_proj_mlp",
          "layer": "layer_10",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_108",
          "type": "LayerNorm",
          "name": "layer10_ln_1",
          "layer": "layer_10",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_109",
          "type": "LayerNorm",
          "name": "layer10_ln_2",
          "layer": "layer_10",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_110",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_111",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_112",
          "type": "Dropout",
          "name": "layer10_attn_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_113",
          "type": "Dropout",
          "name": "layer10_resid_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_114",
          "type": "Linear",
          "name": "layer11_c_attn",
          "layer": "layer_11",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_115",
          "type": "Linear",
          "name": "layer11_c_proj",
          "layer": "layer_11",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_116",
          "type": "Linear",
          "name": "layer11_c_fc",
          "layer": "layer_11",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_117",
          "type": "Linear",
          "name": "layer11_c_proj_mlp",
          "layer": "layer_11",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_118",
          "type": "LayerNorm",
          "name": "layer11_ln_1",
          "layer": "layer_11",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_119",
          "type": "LayerNorm",
          "name": "layer11_ln_2",
          "layer": "layer_11",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_120",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_121",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_122",
          "type": "Dropout",
          "name": "layer11_attn_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_123",
          "type": "Dropout",
          "name": "layer11_resid_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_124",
          "type": "Linear",
          "name": "layer12_c_attn",
          "layer": "layer_12",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_125",
          "type": "Linear",
          "name": "layer12_c_proj",
          "layer": "layer_12",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_126",
          "type": "Linear",
          "name": "layer12_c_fc",
          "layer": "layer_12",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_127",
          "type": "Linear",
          "name": "layer12_c_proj_mlp",
          "layer": "layer_12",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_128",
          "type": "LayerNorm",
          "name": "layer12_ln_1",
          "layer": "layer_12",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_129",
          "type": "LayerNorm",
          "name": "layer12_ln_2",
          "layer": "layer_12",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_130",
          "type": "Softmax",
          "name": "layer12_attention_softmax",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_131",
          "type": "GELU",
          "name": "layer12_activation",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_132",
          "type": "Dropout",
          "name": "layer12_attn_dropout",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_133",
          "type": "Dropout",
          "name": "layer12_resid_dropout",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_134",
          "type": "Linear",
          "name": "layer13_c_attn",
          "layer": "layer_13",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_135",
          "type": "Linear",
          "name": "layer13_c_proj",
          "layer": "layer_13",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_136",
          "type": "Linear",
          "name": "layer13_c_fc",
          "layer": "layer_13",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_137",
          "type": "Linear",
          "name": "layer13_c_proj_mlp",
          "layer": "layer_13",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_138",
          "type": "LayerNorm",
          "name": "layer13_ln_1",
          "layer": "layer_13",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_139",
          "type": "LayerNorm",
          "name": "layer13_ln_2",
          "layer": "layer_13",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_140",
          "type": "Softmax",
          "name": "layer13_attention_softmax",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_141",
          "type": "GELU",
          "name": "layer13_activation",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_142",
          "type": "Dropout",
          "name": "layer13_attn_dropout",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_143",
          "type": "Dropout",
          "name": "layer13_resid_dropout",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_144",
          "type": "Linear",
          "name": "layer14_c_attn",
          "layer": "layer_14",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_145",
          "type": "Linear",
          "name": "layer14_c_proj",
          "layer": "layer_14",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_146",
          "type": "Linear",
          "name": "layer14_c_fc",
          "layer": "layer_14",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_147",
          "type": "Linear",
          "name": "layer14_c_proj_mlp",
          "layer": "layer_14",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_148",
          "type": "LayerNorm",
          "name": "layer14_ln_1",
          "layer": "layer_14",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_149",
          "type": "LayerNorm",
          "name": "layer14_ln_2",
          "layer": "layer_14",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_150",
          "type": "Softmax",
          "name": "layer14_attention_softmax",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_151",
          "type": "GELU",
          "name": "layer14_activation",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_152",
          "type": "Dropout",
          "name": "layer14_attn_dropout",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_153",
          "type": "Dropout",
          "name": "layer14_resid_dropout",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_154",
          "type": "Linear",
          "name": "layer15_c_attn",
          "layer": "layer_15",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_155",
          "type": "Linear",
          "name": "layer15_c_proj",
          "layer": "layer_15",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_156",
          "type": "Linear",
          "name": "layer15_c_fc",
          "layer": "layer_15",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_157",
          "type": "Linear",
          "name": "layer15_c_proj_mlp",
          "layer": "layer_15",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_158",
          "type": "LayerNorm",
          "name": "layer15_ln_1",
          "layer": "layer_15",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_159",
          "type": "LayerNorm",
          "name": "layer15_ln_2",
          "layer": "layer_15",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_160",
          "type": "Softmax",
          "name": "layer15_attention_softmax",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_161",
          "type": "GELU",
          "name": "layer15_activation",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_162",
          "type": "Dropout",
          "name": "layer15_attn_dropout",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_163",
          "type": "Dropout",
          "name": "layer15_resid_dropout",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_164",
          "type": "Linear",
          "name": "layer16_c_attn",
          "layer": "layer_16",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_165",
          "type": "Linear",
          "name": "layer16_c_proj",
          "layer": "layer_16",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_166",
          "type": "Linear",
          "name": "layer16_c_fc",
          "layer": "layer_16",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_167",
          "type": "Linear",
          "name": "layer16_c_proj_mlp",
          "layer": "layer_16",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_168",
          "type": "LayerNorm",
          "name": "layer16_ln_1",
          "layer": "layer_16",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_169",
          "type": "LayerNorm",
          "name": "layer16_ln_2",
          "layer": "layer_16",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_170",
          "type": "Softmax",
          "name": "layer16_attention_softmax",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_171",
          "type": "GELU",
          "name": "layer16_activation",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_172",
          "type": "Dropout",
          "name": "layer16_attn_dropout",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_173",
          "type": "Dropout",
          "name": "layer16_resid_dropout",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_174",
          "type": "Linear",
          "name": "layer17_c_attn",
          "layer": "layer_17",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_175",
          "type": "Linear",
          "name": "layer17_c_proj",
          "layer": "layer_17",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_176",
          "type": "Linear",
          "name": "layer17_c_fc",
          "layer": "layer_17",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_177",
          "type": "Linear",
          "name": "layer17_c_proj_mlp",
          "layer": "layer_17",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_178",
          "type": "LayerNorm",
          "name": "layer17_ln_1",
          "layer": "layer_17",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_179",
          "type": "LayerNorm",
          "name": "layer17_ln_2",
          "layer": "layer_17",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_180",
          "type": "Softmax",
          "name": "layer17_attention_softmax",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_181",
          "type": "GELU",
          "name": "layer17_activation",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_182",
          "type": "Dropout",
          "name": "layer17_attn_dropout",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_183",
          "type": "Dropout",
          "name": "layer17_resid_dropout",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_184",
          "type": "Linear",
          "name": "layer18_c_attn",
          "layer": "layer_18",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_185",
          "type": "Linear",
          "name": "layer18_c_proj",
          "layer": "layer_18",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_186",
          "type": "Linear",
          "name": "layer18_c_fc",
          "layer": "layer_18",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_187",
          "type": "Linear",
          "name": "layer18_c_proj_mlp",
          "layer": "layer_18",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_188",
          "type": "LayerNorm",
          "name": "layer18_ln_1",
          "layer": "layer_18",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_189",
          "type": "LayerNorm",
          "name": "layer18_ln_2",
          "layer": "layer_18",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_190",
          "type": "Softmax",
          "name": "layer18_attention_softmax",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_191",
          "type": "GELU",
          "name": "layer18_activation",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_192",
          "type": "Dropout",
          "name": "layer18_attn_dropout",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_193",
          "type": "Dropout",
          "name": "layer18_resid_dropout",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_194",
          "type": "Linear",
          "name": "layer19_c_attn",
          "layer": "layer_19",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_195",
          "type": "Linear",
          "name": "layer19_c_proj",
          "layer": "layer_19",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_196",
          "type": "Linear",
          "name": "layer19_c_fc",
          "layer": "layer_19",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_197",
          "type": "Linear",
          "name": "layer19_c_proj_mlp",
          "layer": "layer_19",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_198",
          "type": "LayerNorm",
          "name": "layer19_ln_1",
          "layer": "layer_19",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_199",
          "type": "LayerNorm",
          "name": "layer19_ln_2",
          "layer": "layer_19",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_200",
          "type": "Softmax",
          "name": "layer19_attention_softmax",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_201",
          "type": "GELU",
          "name": "layer19_activation",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_202",
          "type": "Dropout",
          "name": "layer19_attn_dropout",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_203",
          "type": "Dropout",
          "name": "layer19_resid_dropout",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_204",
          "type": "Linear",
          "name": "layer20_c_attn",
          "layer": "layer_20",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_205",
          "type": "Linear",
          "name": "layer20_c_proj",
          "layer": "layer_20",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_206",
          "type": "Linear",
          "name": "layer20_c_fc",
          "layer": "layer_20",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_207",
          "type": "Linear",
          "name": "layer20_c_proj_mlp",
          "layer": "layer_20",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_208",
          "type": "LayerNorm",
          "name": "layer20_ln_1",
          "layer": "layer_20",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_209",
          "type": "LayerNorm",
          "name": "layer20_ln_2",
          "layer": "layer_20",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_210",
          "type": "Softmax",
          "name": "layer20_attention_softmax",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_211",
          "type": "GELU",
          "name": "layer20_activation",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_212",
          "type": "Dropout",
          "name": "layer20_attn_dropout",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_213",
          "type": "Dropout",
          "name": "layer20_resid_dropout",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_214",
          "type": "Linear",
          "name": "layer21_c_attn",
          "layer": "layer_21",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_215",
          "type": "Linear",
          "name": "layer21_c_proj",
          "layer": "layer_21",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_216",
          "type": "Linear",
          "name": "layer21_c_fc",
          "layer": "layer_21",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_217",
          "type": "Linear",
          "name": "layer21_c_proj_mlp",
          "layer": "layer_21",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_218",
          "type": "LayerNorm",
          "name": "layer21_ln_1",
          "layer": "layer_21",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_219",
          "type": "LayerNorm",
          "name": "layer21_ln_2",
          "layer": "layer_21",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_220",
          "type": "Softmax",
          "name": "layer21_attention_softmax",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_221",
          "type": "GELU",
          "name": "layer21_activation",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_222",
          "type": "Dropout",
          "name": "layer21_attn_dropout",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_223",
          "type": "Dropout",
          "name": "layer21_resid_dropout",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_224",
          "type": "Linear",
          "name": "layer22_c_attn",
          "layer": "layer_22",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_225",
          "type": "Linear",
          "name": "layer22_c_proj",
          "layer": "layer_22",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_226",
          "type": "Linear",
          "name": "layer22_c_fc",
          "layer": "layer_22",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_227",
          "type": "Linear",
          "name": "layer22_c_proj_mlp",
          "layer": "layer_22",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_228",
          "type": "LayerNorm",
          "name": "layer22_ln_1",
          "layer": "layer_22",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_229",
          "type": "LayerNorm",
          "name": "layer22_ln_2",
          "layer": "layer_22",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_230",
          "type": "Softmax",
          "name": "layer22_attention_softmax",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_231",
          "type": "GELU",
          "name": "layer22_activation",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_232",
          "type": "Dropout",
          "name": "layer22_attn_dropout",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_233",
          "type": "Dropout",
          "name": "layer22_resid_dropout",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_234",
          "type": "Linear",
          "name": "layer23_c_attn",
          "layer": "layer_23",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_235",
          "type": "Linear",
          "name": "layer23_c_proj",
          "layer": "layer_23",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_236",
          "type": "Linear",
          "name": "layer23_c_fc",
          "layer": "layer_23",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_237",
          "type": "Linear",
          "name": "layer23_c_proj_mlp",
          "layer": "layer_23",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "OPT_1.3B_op_238",
          "type": "LayerNorm",
          "name": "layer23_ln_1",
          "layer": "layer_23",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_239",
          "type": "LayerNorm",
          "name": "layer23_ln_2",
          "layer": "layer_23",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_240",
          "type": "Softmax",
          "name": "layer23_attention_softmax",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "OPT_1.3B_op_241",
          "type": "GELU",
          "name": "layer23_activation",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "OPT_1.3B_op_242",
          "type": "Dropout",
          "name": "layer23_attn_dropout",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "OPT_1.3B_op_243",
          "type": "Dropout",
          "name": "layer23_resid_dropout",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        }
      ],
      "parameters": 1300000000
    },
    {
      "model_id": "Falcon_7B",
      "name": "Falcon-7B",
      "source": "huggingface/falcon-7b",
      "category": "LLM",
      "architecture": "llm_decoder",
      "num_params": 7000000000,
      "num_layers": 32,
      "hidden_size": 4544,
      "vocab_size": 65024,
      "intermediate_size": 18176,
      "operator_count": 387,
      "operators": [
        {
          "op_id": "Falcon_7B_op_1",
          "type": "Embedding",
          "name": "embed_tokens",
          "layer": "global",
          "parameters": 295469056,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_2",
          "type": "RMSNorm",
          "name": "norm",
          "layer": "global",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_3",
          "type": "Linear",
          "name": "lm_head",
          "layer": "global",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_4",
          "type": "Linear",
          "name": "layer0_q_proj",
          "layer": "layer_0",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_5",
          "type": "Linear",
          "name": "layer0_k_proj",
          "layer": "layer_0",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_6",
          "type": "Linear",
          "name": "layer0_v_proj",
          "layer": "layer_0",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_7",
          "type": "Linear",
          "name": "layer0_o_proj",
          "layer": "layer_0",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_8",
          "type": "Linear",
          "name": "layer0_gate_proj",
          "layer": "layer_0",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_9",
          "type": "Linear",
          "name": "layer0_up_proj",
          "layer": "layer_0",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_10",
          "type": "Linear",
          "name": "layer0_down_proj",
          "layer": "layer_0",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_11",
          "type": "RMSNorm",
          "name": "layer0_input_layernorm",
          "layer": "layer_0",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_12",
          "type": "RMSNorm",
          "name": "layer0_post_attention_layernorm",
          "layer": "layer_0",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_13",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_14",
          "type": "SiLU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_15",
          "type": "Dropout",
          "name": "layer0_attention_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_16",
          "type": "Linear",
          "name": "layer1_q_proj",
          "layer": "layer_1",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_17",
          "type": "Linear",
          "name": "layer1_k_proj",
          "layer": "layer_1",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_18",
          "type": "Linear",
          "name": "layer1_v_proj",
          "layer": "layer_1",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_19",
          "type": "Linear",
          "name": "layer1_o_proj",
          "layer": "layer_1",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_20",
          "type": "Linear",
          "name": "layer1_gate_proj",
          "layer": "layer_1",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_21",
          "type": "Linear",
          "name": "layer1_up_proj",
          "layer": "layer_1",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_22",
          "type": "Linear",
          "name": "layer1_down_proj",
          "layer": "layer_1",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_23",
          "type": "RMSNorm",
          "name": "layer1_input_layernorm",
          "layer": "layer_1",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_24",
          "type": "RMSNorm",
          "name": "layer1_post_attention_layernorm",
          "layer": "layer_1",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_25",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_26",
          "type": "SiLU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_27",
          "type": "Dropout",
          "name": "layer1_attention_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_28",
          "type": "Linear",
          "name": "layer2_q_proj",
          "layer": "layer_2",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_29",
          "type": "Linear",
          "name": "layer2_k_proj",
          "layer": "layer_2",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_30",
          "type": "Linear",
          "name": "layer2_v_proj",
          "layer": "layer_2",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_31",
          "type": "Linear",
          "name": "layer2_o_proj",
          "layer": "layer_2",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_32",
          "type": "Linear",
          "name": "layer2_gate_proj",
          "layer": "layer_2",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_33",
          "type": "Linear",
          "name": "layer2_up_proj",
          "layer": "layer_2",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_34",
          "type": "Linear",
          "name": "layer2_down_proj",
          "layer": "layer_2",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_35",
          "type": "RMSNorm",
          "name": "layer2_input_layernorm",
          "layer": "layer_2",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_36",
          "type": "RMSNorm",
          "name": "layer2_post_attention_layernorm",
          "layer": "layer_2",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_37",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_38",
          "type": "SiLU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_39",
          "type": "Dropout",
          "name": "layer2_attention_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_40",
          "type": "Linear",
          "name": "layer3_q_proj",
          "layer": "layer_3",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_41",
          "type": "Linear",
          "name": "layer3_k_proj",
          "layer": "layer_3",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_42",
          "type": "Linear",
          "name": "layer3_v_proj",
          "layer": "layer_3",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_43",
          "type": "Linear",
          "name": "layer3_o_proj",
          "layer": "layer_3",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_44",
          "type": "Linear",
          "name": "layer3_gate_proj",
          "layer": "layer_3",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_45",
          "type": "Linear",
          "name": "layer3_up_proj",
          "layer": "layer_3",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_46",
          "type": "Linear",
          "name": "layer3_down_proj",
          "layer": "layer_3",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_47",
          "type": "RMSNorm",
          "name": "layer3_input_layernorm",
          "layer": "layer_3",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_48",
          "type": "RMSNorm",
          "name": "layer3_post_attention_layernorm",
          "layer": "layer_3",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_49",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_50",
          "type": "SiLU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_51",
          "type": "Dropout",
          "name": "layer3_attention_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_52",
          "type": "Linear",
          "name": "layer4_q_proj",
          "layer": "layer_4",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_53",
          "type": "Linear",
          "name": "layer4_k_proj",
          "layer": "layer_4",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_54",
          "type": "Linear",
          "name": "layer4_v_proj",
          "layer": "layer_4",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_55",
          "type": "Linear",
          "name": "layer4_o_proj",
          "layer": "layer_4",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_56",
          "type": "Linear",
          "name": "layer4_gate_proj",
          "layer": "layer_4",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_57",
          "type": "Linear",
          "name": "layer4_up_proj",
          "layer": "layer_4",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_58",
          "type": "Linear",
          "name": "layer4_down_proj",
          "layer": "layer_4",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_59",
          "type": "RMSNorm",
          "name": "layer4_input_layernorm",
          "layer": "layer_4",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_60",
          "type": "RMSNorm",
          "name": "layer4_post_attention_layernorm",
          "layer": "layer_4",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_61",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_62",
          "type": "SiLU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_63",
          "type": "Dropout",
          "name": "layer4_attention_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_64",
          "type": "Linear",
          "name": "layer5_q_proj",
          "layer": "layer_5",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_65",
          "type": "Linear",
          "name": "layer5_k_proj",
          "layer": "layer_5",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_66",
          "type": "Linear",
          "name": "layer5_v_proj",
          "layer": "layer_5",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_67",
          "type": "Linear",
          "name": "layer5_o_proj",
          "layer": "layer_5",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_68",
          "type": "Linear",
          "name": "layer5_gate_proj",
          "layer": "layer_5",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_69",
          "type": "Linear",
          "name": "layer5_up_proj",
          "layer": "layer_5",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_70",
          "type": "Linear",
          "name": "layer5_down_proj",
          "layer": "layer_5",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_71",
          "type": "RMSNorm",
          "name": "layer5_input_layernorm",
          "layer": "layer_5",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_72",
          "type": "RMSNorm",
          "name": "layer5_post_attention_layernorm",
          "layer": "layer_5",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_73",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_74",
          "type": "SiLU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_75",
          "type": "Dropout",
          "name": "layer5_attention_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_76",
          "type": "Linear",
          "name": "layer6_q_proj",
          "layer": "layer_6",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_77",
          "type": "Linear",
          "name": "layer6_k_proj",
          "layer": "layer_6",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_78",
          "type": "Linear",
          "name": "layer6_v_proj",
          "layer": "layer_6",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_79",
          "type": "Linear",
          "name": "layer6_o_proj",
          "layer": "layer_6",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_80",
          "type": "Linear",
          "name": "layer6_gate_proj",
          "layer": "layer_6",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_81",
          "type": "Linear",
          "name": "layer6_up_proj",
          "layer": "layer_6",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_82",
          "type": "Linear",
          "name": "layer6_down_proj",
          "layer": "layer_6",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_83",
          "type": "RMSNorm",
          "name": "layer6_input_layernorm",
          "layer": "layer_6",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_84",
          "type": "RMSNorm",
          "name": "layer6_post_attention_layernorm",
          "layer": "layer_6",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_85",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_86",
          "type": "SiLU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_87",
          "type": "Dropout",
          "name": "layer6_attention_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_88",
          "type": "Linear",
          "name": "layer7_q_proj",
          "layer": "layer_7",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_89",
          "type": "Linear",
          "name": "layer7_k_proj",
          "layer": "layer_7",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_90",
          "type": "Linear",
          "name": "layer7_v_proj",
          "layer": "layer_7",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_91",
          "type": "Linear",
          "name": "layer7_o_proj",
          "layer": "layer_7",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_92",
          "type": "Linear",
          "name": "layer7_gate_proj",
          "layer": "layer_7",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_93",
          "type": "Linear",
          "name": "layer7_up_proj",
          "layer": "layer_7",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_94",
          "type": "Linear",
          "name": "layer7_down_proj",
          "layer": "layer_7",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_95",
          "type": "RMSNorm",
          "name": "layer7_input_layernorm",
          "layer": "layer_7",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_96",
          "type": "RMSNorm",
          "name": "layer7_post_attention_layernorm",
          "layer": "layer_7",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_97",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_98",
          "type": "SiLU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_99",
          "type": "Dropout",
          "name": "layer7_attention_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_100",
          "type": "Linear",
          "name": "layer8_q_proj",
          "layer": "layer_8",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_101",
          "type": "Linear",
          "name": "layer8_k_proj",
          "layer": "layer_8",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_102",
          "type": "Linear",
          "name": "layer8_v_proj",
          "layer": "layer_8",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_103",
          "type": "Linear",
          "name": "layer8_o_proj",
          "layer": "layer_8",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_104",
          "type": "Linear",
          "name": "layer8_gate_proj",
          "layer": "layer_8",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_105",
          "type": "Linear",
          "name": "layer8_up_proj",
          "layer": "layer_8",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_106",
          "type": "Linear",
          "name": "layer8_down_proj",
          "layer": "layer_8",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_107",
          "type": "RMSNorm",
          "name": "layer8_input_layernorm",
          "layer": "layer_8",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_108",
          "type": "RMSNorm",
          "name": "layer8_post_attention_layernorm",
          "layer": "layer_8",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_109",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_110",
          "type": "SiLU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_111",
          "type": "Dropout",
          "name": "layer8_attention_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_112",
          "type": "Linear",
          "name": "layer9_q_proj",
          "layer": "layer_9",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_113",
          "type": "Linear",
          "name": "layer9_k_proj",
          "layer": "layer_9",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_114",
          "type": "Linear",
          "name": "layer9_v_proj",
          "layer": "layer_9",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_115",
          "type": "Linear",
          "name": "layer9_o_proj",
          "layer": "layer_9",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_116",
          "type": "Linear",
          "name": "layer9_gate_proj",
          "layer": "layer_9",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_117",
          "type": "Linear",
          "name": "layer9_up_proj",
          "layer": "layer_9",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_118",
          "type": "Linear",
          "name": "layer9_down_proj",
          "layer": "layer_9",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_119",
          "type": "RMSNorm",
          "name": "layer9_input_layernorm",
          "layer": "layer_9",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_120",
          "type": "RMSNorm",
          "name": "layer9_post_attention_layernorm",
          "layer": "layer_9",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_121",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_122",
          "type": "SiLU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_123",
          "type": "Dropout",
          "name": "layer9_attention_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_124",
          "type": "Linear",
          "name": "layer10_q_proj",
          "layer": "layer_10",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_125",
          "type": "Linear",
          "name": "layer10_k_proj",
          "layer": "layer_10",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_126",
          "type": "Linear",
          "name": "layer10_v_proj",
          "layer": "layer_10",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_127",
          "type": "Linear",
          "name": "layer10_o_proj",
          "layer": "layer_10",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_128",
          "type": "Linear",
          "name": "layer10_gate_proj",
          "layer": "layer_10",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_129",
          "type": "Linear",
          "name": "layer10_up_proj",
          "layer": "layer_10",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_130",
          "type": "Linear",
          "name": "layer10_down_proj",
          "layer": "layer_10",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_131",
          "type": "RMSNorm",
          "name": "layer10_input_layernorm",
          "layer": "layer_10",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_132",
          "type": "RMSNorm",
          "name": "layer10_post_attention_layernorm",
          "layer": "layer_10",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_133",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_134",
          "type": "SiLU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_135",
          "type": "Dropout",
          "name": "layer10_attention_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_136",
          "type": "Linear",
          "name": "layer11_q_proj",
          "layer": "layer_11",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_137",
          "type": "Linear",
          "name": "layer11_k_proj",
          "layer": "layer_11",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_138",
          "type": "Linear",
          "name": "layer11_v_proj",
          "layer": "layer_11",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_139",
          "type": "Linear",
          "name": "layer11_o_proj",
          "layer": "layer_11",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_140",
          "type": "Linear",
          "name": "layer11_gate_proj",
          "layer": "layer_11",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_141",
          "type": "Linear",
          "name": "layer11_up_proj",
          "layer": "layer_11",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_142",
          "type": "Linear",
          "name": "layer11_down_proj",
          "layer": "layer_11",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_143",
          "type": "RMSNorm",
          "name": "layer11_input_layernorm",
          "layer": "layer_11",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_144",
          "type": "RMSNorm",
          "name": "layer11_post_attention_layernorm",
          "layer": "layer_11",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_145",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_146",
          "type": "SiLU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_147",
          "type": "Dropout",
          "name": "layer11_attention_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_148",
          "type": "Linear",
          "name": "layer12_q_proj",
          "layer": "layer_12",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_149",
          "type": "Linear",
          "name": "layer12_k_proj",
          "layer": "layer_12",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_150",
          "type": "Linear",
          "name": "layer12_v_proj",
          "layer": "layer_12",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_151",
          "type": "Linear",
          "name": "layer12_o_proj",
          "layer": "layer_12",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_152",
          "type": "Linear",
          "name": "layer12_gate_proj",
          "layer": "layer_12",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_153",
          "type": "Linear",
          "name": "layer12_up_proj",
          "layer": "layer_12",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_154",
          "type": "Linear",
          "name": "layer12_down_proj",
          "layer": "layer_12",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_155",
          "type": "RMSNorm",
          "name": "layer12_input_layernorm",
          "layer": "layer_12",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_156",
          "type": "RMSNorm",
          "name": "layer12_post_attention_layernorm",
          "layer": "layer_12",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_157",
          "type": "Softmax",
          "name": "layer12_attention_softmax",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_158",
          "type": "SiLU",
          "name": "layer12_activation",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_159",
          "type": "Dropout",
          "name": "layer12_attention_dropout",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_160",
          "type": "Linear",
          "name": "layer13_q_proj",
          "layer": "layer_13",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_161",
          "type": "Linear",
          "name": "layer13_k_proj",
          "layer": "layer_13",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_162",
          "type": "Linear",
          "name": "layer13_v_proj",
          "layer": "layer_13",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_163",
          "type": "Linear",
          "name": "layer13_o_proj",
          "layer": "layer_13",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_164",
          "type": "Linear",
          "name": "layer13_gate_proj",
          "layer": "layer_13",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_165",
          "type": "Linear",
          "name": "layer13_up_proj",
          "layer": "layer_13",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_166",
          "type": "Linear",
          "name": "layer13_down_proj",
          "layer": "layer_13",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_167",
          "type": "RMSNorm",
          "name": "layer13_input_layernorm",
          "layer": "layer_13",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_168",
          "type": "RMSNorm",
          "name": "layer13_post_attention_layernorm",
          "layer": "layer_13",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_169",
          "type": "Softmax",
          "name": "layer13_attention_softmax",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_170",
          "type": "SiLU",
          "name": "layer13_activation",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_171",
          "type": "Dropout",
          "name": "layer13_attention_dropout",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_172",
          "type": "Linear",
          "name": "layer14_q_proj",
          "layer": "layer_14",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_173",
          "type": "Linear",
          "name": "layer14_k_proj",
          "layer": "layer_14",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_174",
          "type": "Linear",
          "name": "layer14_v_proj",
          "layer": "layer_14",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_175",
          "type": "Linear",
          "name": "layer14_o_proj",
          "layer": "layer_14",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_176",
          "type": "Linear",
          "name": "layer14_gate_proj",
          "layer": "layer_14",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_177",
          "type": "Linear",
          "name": "layer14_up_proj",
          "layer": "layer_14",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_178",
          "type": "Linear",
          "name": "layer14_down_proj",
          "layer": "layer_14",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_179",
          "type": "RMSNorm",
          "name": "layer14_input_layernorm",
          "layer": "layer_14",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_180",
          "type": "RMSNorm",
          "name": "layer14_post_attention_layernorm",
          "layer": "layer_14",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_181",
          "type": "Softmax",
          "name": "layer14_attention_softmax",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_182",
          "type": "SiLU",
          "name": "layer14_activation",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_183",
          "type": "Dropout",
          "name": "layer14_attention_dropout",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_184",
          "type": "Linear",
          "name": "layer15_q_proj",
          "layer": "layer_15",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_185",
          "type": "Linear",
          "name": "layer15_k_proj",
          "layer": "layer_15",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_186",
          "type": "Linear",
          "name": "layer15_v_proj",
          "layer": "layer_15",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_187",
          "type": "Linear",
          "name": "layer15_o_proj",
          "layer": "layer_15",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_188",
          "type": "Linear",
          "name": "layer15_gate_proj",
          "layer": "layer_15",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_189",
          "type": "Linear",
          "name": "layer15_up_proj",
          "layer": "layer_15",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_190",
          "type": "Linear",
          "name": "layer15_down_proj",
          "layer": "layer_15",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_191",
          "type": "RMSNorm",
          "name": "layer15_input_layernorm",
          "layer": "layer_15",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_192",
          "type": "RMSNorm",
          "name": "layer15_post_attention_layernorm",
          "layer": "layer_15",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_193",
          "type": "Softmax",
          "name": "layer15_attention_softmax",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_194",
          "type": "SiLU",
          "name": "layer15_activation",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_195",
          "type": "Dropout",
          "name": "layer15_attention_dropout",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_196",
          "type": "Linear",
          "name": "layer16_q_proj",
          "layer": "layer_16",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_197",
          "type": "Linear",
          "name": "layer16_k_proj",
          "layer": "layer_16",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_198",
          "type": "Linear",
          "name": "layer16_v_proj",
          "layer": "layer_16",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_199",
          "type": "Linear",
          "name": "layer16_o_proj",
          "layer": "layer_16",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_200",
          "type": "Linear",
          "name": "layer16_gate_proj",
          "layer": "layer_16",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_201",
          "type": "Linear",
          "name": "layer16_up_proj",
          "layer": "layer_16",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_202",
          "type": "Linear",
          "name": "layer16_down_proj",
          "layer": "layer_16",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_203",
          "type": "RMSNorm",
          "name": "layer16_input_layernorm",
          "layer": "layer_16",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_204",
          "type": "RMSNorm",
          "name": "layer16_post_attention_layernorm",
          "layer": "layer_16",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_205",
          "type": "Softmax",
          "name": "layer16_attention_softmax",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_206",
          "type": "SiLU",
          "name": "layer16_activation",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_207",
          "type": "Dropout",
          "name": "layer16_attention_dropout",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_208",
          "type": "Linear",
          "name": "layer17_q_proj",
          "layer": "layer_17",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_209",
          "type": "Linear",
          "name": "layer17_k_proj",
          "layer": "layer_17",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_210",
          "type": "Linear",
          "name": "layer17_v_proj",
          "layer": "layer_17",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_211",
          "type": "Linear",
          "name": "layer17_o_proj",
          "layer": "layer_17",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_212",
          "type": "Linear",
          "name": "layer17_gate_proj",
          "layer": "layer_17",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_213",
          "type": "Linear",
          "name": "layer17_up_proj",
          "layer": "layer_17",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_214",
          "type": "Linear",
          "name": "layer17_down_proj",
          "layer": "layer_17",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_215",
          "type": "RMSNorm",
          "name": "layer17_input_layernorm",
          "layer": "layer_17",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_216",
          "type": "RMSNorm",
          "name": "layer17_post_attention_layernorm",
          "layer": "layer_17",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_217",
          "type": "Softmax",
          "name": "layer17_attention_softmax",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_218",
          "type": "SiLU",
          "name": "layer17_activation",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_219",
          "type": "Dropout",
          "name": "layer17_attention_dropout",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_220",
          "type": "Linear",
          "name": "layer18_q_proj",
          "layer": "layer_18",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_221",
          "type": "Linear",
          "name": "layer18_k_proj",
          "layer": "layer_18",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_222",
          "type": "Linear",
          "name": "layer18_v_proj",
          "layer": "layer_18",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_223",
          "type": "Linear",
          "name": "layer18_o_proj",
          "layer": "layer_18",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_224",
          "type": "Linear",
          "name": "layer18_gate_proj",
          "layer": "layer_18",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_225",
          "type": "Linear",
          "name": "layer18_up_proj",
          "layer": "layer_18",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_226",
          "type": "Linear",
          "name": "layer18_down_proj",
          "layer": "layer_18",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_227",
          "type": "RMSNorm",
          "name": "layer18_input_layernorm",
          "layer": "layer_18",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_228",
          "type": "RMSNorm",
          "name": "layer18_post_attention_layernorm",
          "layer": "layer_18",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_229",
          "type": "Softmax",
          "name": "layer18_attention_softmax",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_230",
          "type": "SiLU",
          "name": "layer18_activation",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_231",
          "type": "Dropout",
          "name": "layer18_attention_dropout",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_232",
          "type": "Linear",
          "name": "layer19_q_proj",
          "layer": "layer_19",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_233",
          "type": "Linear",
          "name": "layer19_k_proj",
          "layer": "layer_19",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_234",
          "type": "Linear",
          "name": "layer19_v_proj",
          "layer": "layer_19",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_235",
          "type": "Linear",
          "name": "layer19_o_proj",
          "layer": "layer_19",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_236",
          "type": "Linear",
          "name": "layer19_gate_proj",
          "layer": "layer_19",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_237",
          "type": "Linear",
          "name": "layer19_up_proj",
          "layer": "layer_19",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_238",
          "type": "Linear",
          "name": "layer19_down_proj",
          "layer": "layer_19",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_239",
          "type": "RMSNorm",
          "name": "layer19_input_layernorm",
          "layer": "layer_19",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_240",
          "type": "RMSNorm",
          "name": "layer19_post_attention_layernorm",
          "layer": "layer_19",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_241",
          "type": "Softmax",
          "name": "layer19_attention_softmax",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_242",
          "type": "SiLU",
          "name": "layer19_activation",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_243",
          "type": "Dropout",
          "name": "layer19_attention_dropout",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_244",
          "type": "Linear",
          "name": "layer20_q_proj",
          "layer": "layer_20",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_245",
          "type": "Linear",
          "name": "layer20_k_proj",
          "layer": "layer_20",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_246",
          "type": "Linear",
          "name": "layer20_v_proj",
          "layer": "layer_20",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_247",
          "type": "Linear",
          "name": "layer20_o_proj",
          "layer": "layer_20",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_248",
          "type": "Linear",
          "name": "layer20_gate_proj",
          "layer": "layer_20",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_249",
          "type": "Linear",
          "name": "layer20_up_proj",
          "layer": "layer_20",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_250",
          "type": "Linear",
          "name": "layer20_down_proj",
          "layer": "layer_20",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_251",
          "type": "RMSNorm",
          "name": "layer20_input_layernorm",
          "layer": "layer_20",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_252",
          "type": "RMSNorm",
          "name": "layer20_post_attention_layernorm",
          "layer": "layer_20",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_253",
          "type": "Softmax",
          "name": "layer20_attention_softmax",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_254",
          "type": "SiLU",
          "name": "layer20_activation",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_255",
          "type": "Dropout",
          "name": "layer20_attention_dropout",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_256",
          "type": "Linear",
          "name": "layer21_q_proj",
          "layer": "layer_21",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_257",
          "type": "Linear",
          "name": "layer21_k_proj",
          "layer": "layer_21",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_258",
          "type": "Linear",
          "name": "layer21_v_proj",
          "layer": "layer_21",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_259",
          "type": "Linear",
          "name": "layer21_o_proj",
          "layer": "layer_21",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_260",
          "type": "Linear",
          "name": "layer21_gate_proj",
          "layer": "layer_21",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_261",
          "type": "Linear",
          "name": "layer21_up_proj",
          "layer": "layer_21",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_262",
          "type": "Linear",
          "name": "layer21_down_proj",
          "layer": "layer_21",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_263",
          "type": "RMSNorm",
          "name": "layer21_input_layernorm",
          "layer": "layer_21",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_264",
          "type": "RMSNorm",
          "name": "layer21_post_attention_layernorm",
          "layer": "layer_21",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_265",
          "type": "Softmax",
          "name": "layer21_attention_softmax",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_266",
          "type": "SiLU",
          "name": "layer21_activation",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_267",
          "type": "Dropout",
          "name": "layer21_attention_dropout",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_268",
          "type": "Linear",
          "name": "layer22_q_proj",
          "layer": "layer_22",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_269",
          "type": "Linear",
          "name": "layer22_k_proj",
          "layer": "layer_22",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_270",
          "type": "Linear",
          "name": "layer22_v_proj",
          "layer": "layer_22",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_271",
          "type": "Linear",
          "name": "layer22_o_proj",
          "layer": "layer_22",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_272",
          "type": "Linear",
          "name": "layer22_gate_proj",
          "layer": "layer_22",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_273",
          "type": "Linear",
          "name": "layer22_up_proj",
          "layer": "layer_22",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_274",
          "type": "Linear",
          "name": "layer22_down_proj",
          "layer": "layer_22",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_275",
          "type": "RMSNorm",
          "name": "layer22_input_layernorm",
          "layer": "layer_22",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_276",
          "type": "RMSNorm",
          "name": "layer22_post_attention_layernorm",
          "layer": "layer_22",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_277",
          "type": "Softmax",
          "name": "layer22_attention_softmax",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_278",
          "type": "SiLU",
          "name": "layer22_activation",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_279",
          "type": "Dropout",
          "name": "layer22_attention_dropout",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_280",
          "type": "Linear",
          "name": "layer23_q_proj",
          "layer": "layer_23",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_281",
          "type": "Linear",
          "name": "layer23_k_proj",
          "layer": "layer_23",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_282",
          "type": "Linear",
          "name": "layer23_v_proj",
          "layer": "layer_23",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_283",
          "type": "Linear",
          "name": "layer23_o_proj",
          "layer": "layer_23",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_284",
          "type": "Linear",
          "name": "layer23_gate_proj",
          "layer": "layer_23",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_285",
          "type": "Linear",
          "name": "layer23_up_proj",
          "layer": "layer_23",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_286",
          "type": "Linear",
          "name": "layer23_down_proj",
          "layer": "layer_23",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_287",
          "type": "RMSNorm",
          "name": "layer23_input_layernorm",
          "layer": "layer_23",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_288",
          "type": "RMSNorm",
          "name": "layer23_post_attention_layernorm",
          "layer": "layer_23",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_289",
          "type": "Softmax",
          "name": "layer23_attention_softmax",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_290",
          "type": "SiLU",
          "name": "layer23_activation",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_291",
          "type": "Dropout",
          "name": "layer23_attention_dropout",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_292",
          "type": "Linear",
          "name": "layer24_q_proj",
          "layer": "layer_24",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_293",
          "type": "Linear",
          "name": "layer24_k_proj",
          "layer": "layer_24",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_294",
          "type": "Linear",
          "name": "layer24_v_proj",
          "layer": "layer_24",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_295",
          "type": "Linear",
          "name": "layer24_o_proj",
          "layer": "layer_24",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_296",
          "type": "Linear",
          "name": "layer24_gate_proj",
          "layer": "layer_24",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_297",
          "type": "Linear",
          "name": "layer24_up_proj",
          "layer": "layer_24",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_298",
          "type": "Linear",
          "name": "layer24_down_proj",
          "layer": "layer_24",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_299",
          "type": "RMSNorm",
          "name": "layer24_input_layernorm",
          "layer": "layer_24",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_300",
          "type": "RMSNorm",
          "name": "layer24_post_attention_layernorm",
          "layer": "layer_24",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_301",
          "type": "Softmax",
          "name": "layer24_attention_softmax",
          "layer": "layer_24",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_302",
          "type": "SiLU",
          "name": "layer24_activation",
          "layer": "layer_24",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_303",
          "type": "Dropout",
          "name": "layer24_attention_dropout",
          "layer": "layer_24",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_304",
          "type": "Linear",
          "name": "layer25_q_proj",
          "layer": "layer_25",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_305",
          "type": "Linear",
          "name": "layer25_k_proj",
          "layer": "layer_25",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_306",
          "type": "Linear",
          "name": "layer25_v_proj",
          "layer": "layer_25",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_307",
          "type": "Linear",
          "name": "layer25_o_proj",
          "layer": "layer_25",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_308",
          "type": "Linear",
          "name": "layer25_gate_proj",
          "layer": "layer_25",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_309",
          "type": "Linear",
          "name": "layer25_up_proj",
          "layer": "layer_25",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_310",
          "type": "Linear",
          "name": "layer25_down_proj",
          "layer": "layer_25",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_311",
          "type": "RMSNorm",
          "name": "layer25_input_layernorm",
          "layer": "layer_25",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_312",
          "type": "RMSNorm",
          "name": "layer25_post_attention_layernorm",
          "layer": "layer_25",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_313",
          "type": "Softmax",
          "name": "layer25_attention_softmax",
          "layer": "layer_25",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_314",
          "type": "SiLU",
          "name": "layer25_activation",
          "layer": "layer_25",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_315",
          "type": "Dropout",
          "name": "layer25_attention_dropout",
          "layer": "layer_25",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_316",
          "type": "Linear",
          "name": "layer26_q_proj",
          "layer": "layer_26",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_317",
          "type": "Linear",
          "name": "layer26_k_proj",
          "layer": "layer_26",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_318",
          "type": "Linear",
          "name": "layer26_v_proj",
          "layer": "layer_26",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_319",
          "type": "Linear",
          "name": "layer26_o_proj",
          "layer": "layer_26",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_320",
          "type": "Linear",
          "name": "layer26_gate_proj",
          "layer": "layer_26",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_321",
          "type": "Linear",
          "name": "layer26_up_proj",
          "layer": "layer_26",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_322",
          "type": "Linear",
          "name": "layer26_down_proj",
          "layer": "layer_26",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_323",
          "type": "RMSNorm",
          "name": "layer26_input_layernorm",
          "layer": "layer_26",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_324",
          "type": "RMSNorm",
          "name": "layer26_post_attention_layernorm",
          "layer": "layer_26",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_325",
          "type": "Softmax",
          "name": "layer26_attention_softmax",
          "layer": "layer_26",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_326",
          "type": "SiLU",
          "name": "layer26_activation",
          "layer": "layer_26",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_327",
          "type": "Dropout",
          "name": "layer26_attention_dropout",
          "layer": "layer_26",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_328",
          "type": "Linear",
          "name": "layer27_q_proj",
          "layer": "layer_27",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_329",
          "type": "Linear",
          "name": "layer27_k_proj",
          "layer": "layer_27",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_330",
          "type": "Linear",
          "name": "layer27_v_proj",
          "layer": "layer_27",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_331",
          "type": "Linear",
          "name": "layer27_o_proj",
          "layer": "layer_27",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_332",
          "type": "Linear",
          "name": "layer27_gate_proj",
          "layer": "layer_27",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_333",
          "type": "Linear",
          "name": "layer27_up_proj",
          "layer": "layer_27",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_334",
          "type": "Linear",
          "name": "layer27_down_proj",
          "layer": "layer_27",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_335",
          "type": "RMSNorm",
          "name": "layer27_input_layernorm",
          "layer": "layer_27",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_336",
          "type": "RMSNorm",
          "name": "layer27_post_attention_layernorm",
          "layer": "layer_27",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_337",
          "type": "Softmax",
          "name": "layer27_attention_softmax",
          "layer": "layer_27",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_338",
          "type": "SiLU",
          "name": "layer27_activation",
          "layer": "layer_27",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_339",
          "type": "Dropout",
          "name": "layer27_attention_dropout",
          "layer": "layer_27",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_340",
          "type": "Linear",
          "name": "layer28_q_proj",
          "layer": "layer_28",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_341",
          "type": "Linear",
          "name": "layer28_k_proj",
          "layer": "layer_28",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_342",
          "type": "Linear",
          "name": "layer28_v_proj",
          "layer": "layer_28",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_343",
          "type": "Linear",
          "name": "layer28_o_proj",
          "layer": "layer_28",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_344",
          "type": "Linear",
          "name": "layer28_gate_proj",
          "layer": "layer_28",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_345",
          "type": "Linear",
          "name": "layer28_up_proj",
          "layer": "layer_28",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_346",
          "type": "Linear",
          "name": "layer28_down_proj",
          "layer": "layer_28",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_347",
          "type": "RMSNorm",
          "name": "layer28_input_layernorm",
          "layer": "layer_28",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_348",
          "type": "RMSNorm",
          "name": "layer28_post_attention_layernorm",
          "layer": "layer_28",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_349",
          "type": "Softmax",
          "name": "layer28_attention_softmax",
          "layer": "layer_28",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_350",
          "type": "SiLU",
          "name": "layer28_activation",
          "layer": "layer_28",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_351",
          "type": "Dropout",
          "name": "layer28_attention_dropout",
          "layer": "layer_28",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_352",
          "type": "Linear",
          "name": "layer29_q_proj",
          "layer": "layer_29",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_353",
          "type": "Linear",
          "name": "layer29_k_proj",
          "layer": "layer_29",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_354",
          "type": "Linear",
          "name": "layer29_v_proj",
          "layer": "layer_29",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_355",
          "type": "Linear",
          "name": "layer29_o_proj",
          "layer": "layer_29",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_356",
          "type": "Linear",
          "name": "layer29_gate_proj",
          "layer": "layer_29",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_357",
          "type": "Linear",
          "name": "layer29_up_proj",
          "layer": "layer_29",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_358",
          "type": "Linear",
          "name": "layer29_down_proj",
          "layer": "layer_29",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_359",
          "type": "RMSNorm",
          "name": "layer29_input_layernorm",
          "layer": "layer_29",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_360",
          "type": "RMSNorm",
          "name": "layer29_post_attention_layernorm",
          "layer": "layer_29",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_361",
          "type": "Softmax",
          "name": "layer29_attention_softmax",
          "layer": "layer_29",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_362",
          "type": "SiLU",
          "name": "layer29_activation",
          "layer": "layer_29",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_363",
          "type": "Dropout",
          "name": "layer29_attention_dropout",
          "layer": "layer_29",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_364",
          "type": "Linear",
          "name": "layer30_q_proj",
          "layer": "layer_30",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_365",
          "type": "Linear",
          "name": "layer30_k_proj",
          "layer": "layer_30",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_366",
          "type": "Linear",
          "name": "layer30_v_proj",
          "layer": "layer_30",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_367",
          "type": "Linear",
          "name": "layer30_o_proj",
          "layer": "layer_30",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_368",
          "type": "Linear",
          "name": "layer30_gate_proj",
          "layer": "layer_30",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_369",
          "type": "Linear",
          "name": "layer30_up_proj",
          "layer": "layer_30",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_370",
          "type": "Linear",
          "name": "layer30_down_proj",
          "layer": "layer_30",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_371",
          "type": "RMSNorm",
          "name": "layer30_input_layernorm",
          "layer": "layer_30",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_372",
          "type": "RMSNorm",
          "name": "layer30_post_attention_layernorm",
          "layer": "layer_30",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_373",
          "type": "Softmax",
          "name": "layer30_attention_softmax",
          "layer": "layer_30",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_374",
          "type": "SiLU",
          "name": "layer30_activation",
          "layer": "layer_30",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_375",
          "type": "Dropout",
          "name": "layer30_attention_dropout",
          "layer": "layer_30",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_376",
          "type": "Linear",
          "name": "layer31_q_proj",
          "layer": "layer_31",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_377",
          "type": "Linear",
          "name": "layer31_k_proj",
          "layer": "layer_31",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_378",
          "type": "Linear",
          "name": "layer31_v_proj",
          "layer": "layer_31",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_379",
          "type": "Linear",
          "name": "layer31_o_proj",
          "layer": "layer_31",
          "parameters": 20647936,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_380",
          "type": "Linear",
          "name": "layer31_gate_proj",
          "layer": "layer_31",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_381",
          "type": "Linear",
          "name": "layer31_up_proj",
          "layer": "layer_31",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_382",
          "type": "Linear",
          "name": "layer31_down_proj",
          "layer": "layer_31",
          "parameters": 82591744,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Falcon_7B_op_383",
          "type": "RMSNorm",
          "name": "layer31_input_layernorm",
          "layer": "layer_31",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_384",
          "type": "RMSNorm",
          "name": "layer31_post_attention_layernorm",
          "layer": "layer_31",
          "parameters": 4544,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        },
        {
          "op_id": "Falcon_7B_op_385",
          "type": "Softmax",
          "name": "layer31_attention_softmax",
          "layer": "layer_31",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Falcon_7B_op_386",
          "type": "SiLU",
          "name": "layer31_activation",
          "layer": "layer_31",
          "parameters": 0,
          "input_shape": "[batch, seq, 18176]",
          "output_shape": "[batch, seq, 18176]"
        },
        {
          "op_id": "Falcon_7B_op_387",
          "type": "Dropout",
          "name": "layer31_attention_dropout",
          "layer": "layer_31",
          "parameters": 0,
          "input_shape": "[batch, seq, 4544]",
          "output_shape": "[batch, seq, 4544]"
        }
      ],
      "parameters": 7000000000
    },
    {
      "model_id": "StableLM_3B",
      "name": "StableLM-3B",
      "source": "huggingface/stablelm-3b",
      "category": "LLM",
      "architecture": "llm_decoder",
      "num_params": 3000000000,
      "num_layers": 32,
      "hidden_size": 2560,
      "vocab_size": 50304,
      "intermediate_size": 6912,
      "operator_count": 387,
      "operators": [
        {
          "op_id": "StableLM_3B_op_1",
          "type": "Embedding",
          "name": "embed_tokens",
          "layer": "global",
          "parameters": 128778240,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_2",
          "type": "RMSNorm",
          "name": "norm",
          "layer": "global",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_3",
          "type": "Linear",
          "name": "lm_head",
          "layer": "global",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_4",
          "type": "Linear",
          "name": "layer0_q_proj",
          "layer": "layer_0",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_5",
          "type": "Linear",
          "name": "layer0_k_proj",
          "layer": "layer_0",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_6",
          "type": "Linear",
          "name": "layer0_v_proj",
          "layer": "layer_0",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_7",
          "type": "Linear",
          "name": "layer0_o_proj",
          "layer": "layer_0",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_8",
          "type": "Linear",
          "name": "layer0_gate_proj",
          "layer": "layer_0",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_9",
          "type": "Linear",
          "name": "layer0_up_proj",
          "layer": "layer_0",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_10",
          "type": "Linear",
          "name": "layer0_down_proj",
          "layer": "layer_0",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_11",
          "type": "RMSNorm",
          "name": "layer0_input_layernorm",
          "layer": "layer_0",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_12",
          "type": "RMSNorm",
          "name": "layer0_post_attention_layernorm",
          "layer": "layer_0",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_13",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_14",
          "type": "SiLU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_15",
          "type": "Dropout",
          "name": "layer0_attention_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_16",
          "type": "Linear",
          "name": "layer1_q_proj",
          "layer": "layer_1",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_17",
          "type": "Linear",
          "name": "layer1_k_proj",
          "layer": "layer_1",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_18",
          "type": "Linear",
          "name": "layer1_v_proj",
          "layer": "layer_1",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_19",
          "type": "Linear",
          "name": "layer1_o_proj",
          "layer": "layer_1",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_20",
          "type": "Linear",
          "name": "layer1_gate_proj",
          "layer": "layer_1",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_21",
          "type": "Linear",
          "name": "layer1_up_proj",
          "layer": "layer_1",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_22",
          "type": "Linear",
          "name": "layer1_down_proj",
          "layer": "layer_1",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_23",
          "type": "RMSNorm",
          "name": "layer1_input_layernorm",
          "layer": "layer_1",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_24",
          "type": "RMSNorm",
          "name": "layer1_post_attention_layernorm",
          "layer": "layer_1",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_25",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_26",
          "type": "SiLU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_27",
          "type": "Dropout",
          "name": "layer1_attention_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_28",
          "type": "Linear",
          "name": "layer2_q_proj",
          "layer": "layer_2",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_29",
          "type": "Linear",
          "name": "layer2_k_proj",
          "layer": "layer_2",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_30",
          "type": "Linear",
          "name": "layer2_v_proj",
          "layer": "layer_2",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_31",
          "type": "Linear",
          "name": "layer2_o_proj",
          "layer": "layer_2",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_32",
          "type": "Linear",
          "name": "layer2_gate_proj",
          "layer": "layer_2",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_33",
          "type": "Linear",
          "name": "layer2_up_proj",
          "layer": "layer_2",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_34",
          "type": "Linear",
          "name": "layer2_down_proj",
          "layer": "layer_2",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_35",
          "type": "RMSNorm",
          "name": "layer2_input_layernorm",
          "layer": "layer_2",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_36",
          "type": "RMSNorm",
          "name": "layer2_post_attention_layernorm",
          "layer": "layer_2",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_37",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_38",
          "type": "SiLU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_39",
          "type": "Dropout",
          "name": "layer2_attention_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_40",
          "type": "Linear",
          "name": "layer3_q_proj",
          "layer": "layer_3",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_41",
          "type": "Linear",
          "name": "layer3_k_proj",
          "layer": "layer_3",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_42",
          "type": "Linear",
          "name": "layer3_v_proj",
          "layer": "layer_3",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_43",
          "type": "Linear",
          "name": "layer3_o_proj",
          "layer": "layer_3",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_44",
          "type": "Linear",
          "name": "layer3_gate_proj",
          "layer": "layer_3",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_45",
          "type": "Linear",
          "name": "layer3_up_proj",
          "layer": "layer_3",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_46",
          "type": "Linear",
          "name": "layer3_down_proj",
          "layer": "layer_3",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_47",
          "type": "RMSNorm",
          "name": "layer3_input_layernorm",
          "layer": "layer_3",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_48",
          "type": "RMSNorm",
          "name": "layer3_post_attention_layernorm",
          "layer": "layer_3",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_49",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_50",
          "type": "SiLU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_51",
          "type": "Dropout",
          "name": "layer3_attention_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_52",
          "type": "Linear",
          "name": "layer4_q_proj",
          "layer": "layer_4",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_53",
          "type": "Linear",
          "name": "layer4_k_proj",
          "layer": "layer_4",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_54",
          "type": "Linear",
          "name": "layer4_v_proj",
          "layer": "layer_4",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_55",
          "type": "Linear",
          "name": "layer4_o_proj",
          "layer": "layer_4",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_56",
          "type": "Linear",
          "name": "layer4_gate_proj",
          "layer": "layer_4",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_57",
          "type": "Linear",
          "name": "layer4_up_proj",
          "layer": "layer_4",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_58",
          "type": "Linear",
          "name": "layer4_down_proj",
          "layer": "layer_4",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_59",
          "type": "RMSNorm",
          "name": "layer4_input_layernorm",
          "layer": "layer_4",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_60",
          "type": "RMSNorm",
          "name": "layer4_post_attention_layernorm",
          "layer": "layer_4",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_61",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_62",
          "type": "SiLU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_63",
          "type": "Dropout",
          "name": "layer4_attention_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_64",
          "type": "Linear",
          "name": "layer5_q_proj",
          "layer": "layer_5",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_65",
          "type": "Linear",
          "name": "layer5_k_proj",
          "layer": "layer_5",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_66",
          "type": "Linear",
          "name": "layer5_v_proj",
          "layer": "layer_5",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_67",
          "type": "Linear",
          "name": "layer5_o_proj",
          "layer": "layer_5",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_68",
          "type": "Linear",
          "name": "layer5_gate_proj",
          "layer": "layer_5",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_69",
          "type": "Linear",
          "name": "layer5_up_proj",
          "layer": "layer_5",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_70",
          "type": "Linear",
          "name": "layer5_down_proj",
          "layer": "layer_5",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_71",
          "type": "RMSNorm",
          "name": "layer5_input_layernorm",
          "layer": "layer_5",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_72",
          "type": "RMSNorm",
          "name": "layer5_post_attention_layernorm",
          "layer": "layer_5",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_73",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_74",
          "type": "SiLU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_75",
          "type": "Dropout",
          "name": "layer5_attention_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_76",
          "type": "Linear",
          "name": "layer6_q_proj",
          "layer": "layer_6",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_77",
          "type": "Linear",
          "name": "layer6_k_proj",
          "layer": "layer_6",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_78",
          "type": "Linear",
          "name": "layer6_v_proj",
          "layer": "layer_6",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_79",
          "type": "Linear",
          "name": "layer6_o_proj",
          "layer": "layer_6",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_80",
          "type": "Linear",
          "name": "layer6_gate_proj",
          "layer": "layer_6",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_81",
          "type": "Linear",
          "name": "layer6_up_proj",
          "layer": "layer_6",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_82",
          "type": "Linear",
          "name": "layer6_down_proj",
          "layer": "layer_6",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_83",
          "type": "RMSNorm",
          "name": "layer6_input_layernorm",
          "layer": "layer_6",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_84",
          "type": "RMSNorm",
          "name": "layer6_post_attention_layernorm",
          "layer": "layer_6",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_85",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_86",
          "type": "SiLU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_87",
          "type": "Dropout",
          "name": "layer6_attention_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_88",
          "type": "Linear",
          "name": "layer7_q_proj",
          "layer": "layer_7",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_89",
          "type": "Linear",
          "name": "layer7_k_proj",
          "layer": "layer_7",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_90",
          "type": "Linear",
          "name": "layer7_v_proj",
          "layer": "layer_7",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_91",
          "type": "Linear",
          "name": "layer7_o_proj",
          "layer": "layer_7",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_92",
          "type": "Linear",
          "name": "layer7_gate_proj",
          "layer": "layer_7",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_93",
          "type": "Linear",
          "name": "layer7_up_proj",
          "layer": "layer_7",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_94",
          "type": "Linear",
          "name": "layer7_down_proj",
          "layer": "layer_7",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_95",
          "type": "RMSNorm",
          "name": "layer7_input_layernorm",
          "layer": "layer_7",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_96",
          "type": "RMSNorm",
          "name": "layer7_post_attention_layernorm",
          "layer": "layer_7",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_97",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_98",
          "type": "SiLU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_99",
          "type": "Dropout",
          "name": "layer7_attention_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_100",
          "type": "Linear",
          "name": "layer8_q_proj",
          "layer": "layer_8",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_101",
          "type": "Linear",
          "name": "layer8_k_proj",
          "layer": "layer_8",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_102",
          "type": "Linear",
          "name": "layer8_v_proj",
          "layer": "layer_8",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_103",
          "type": "Linear",
          "name": "layer8_o_proj",
          "layer": "layer_8",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_104",
          "type": "Linear",
          "name": "layer8_gate_proj",
          "layer": "layer_8",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_105",
          "type": "Linear",
          "name": "layer8_up_proj",
          "layer": "layer_8",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_106",
          "type": "Linear",
          "name": "layer8_down_proj",
          "layer": "layer_8",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_107",
          "type": "RMSNorm",
          "name": "layer8_input_layernorm",
          "layer": "layer_8",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_108",
          "type": "RMSNorm",
          "name": "layer8_post_attention_layernorm",
          "layer": "layer_8",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_109",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_110",
          "type": "SiLU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_111",
          "type": "Dropout",
          "name": "layer8_attention_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_112",
          "type": "Linear",
          "name": "layer9_q_proj",
          "layer": "layer_9",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_113",
          "type": "Linear",
          "name": "layer9_k_proj",
          "layer": "layer_9",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_114",
          "type": "Linear",
          "name": "layer9_v_proj",
          "layer": "layer_9",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_115",
          "type": "Linear",
          "name": "layer9_o_proj",
          "layer": "layer_9",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_116",
          "type": "Linear",
          "name": "layer9_gate_proj",
          "layer": "layer_9",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_117",
          "type": "Linear",
          "name": "layer9_up_proj",
          "layer": "layer_9",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_118",
          "type": "Linear",
          "name": "layer9_down_proj",
          "layer": "layer_9",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_119",
          "type": "RMSNorm",
          "name": "layer9_input_layernorm",
          "layer": "layer_9",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_120",
          "type": "RMSNorm",
          "name": "layer9_post_attention_layernorm",
          "layer": "layer_9",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_121",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_122",
          "type": "SiLU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_123",
          "type": "Dropout",
          "name": "layer9_attention_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_124",
          "type": "Linear",
          "name": "layer10_q_proj",
          "layer": "layer_10",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_125",
          "type": "Linear",
          "name": "layer10_k_proj",
          "layer": "layer_10",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_126",
          "type": "Linear",
          "name": "layer10_v_proj",
          "layer": "layer_10",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_127",
          "type": "Linear",
          "name": "layer10_o_proj",
          "layer": "layer_10",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_128",
          "type": "Linear",
          "name": "layer10_gate_proj",
          "layer": "layer_10",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_129",
          "type": "Linear",
          "name": "layer10_up_proj",
          "layer": "layer_10",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_130",
          "type": "Linear",
          "name": "layer10_down_proj",
          "layer": "layer_10",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_131",
          "type": "RMSNorm",
          "name": "layer10_input_layernorm",
          "layer": "layer_10",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_132",
          "type": "RMSNorm",
          "name": "layer10_post_attention_layernorm",
          "layer": "layer_10",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_133",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_134",
          "type": "SiLU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_135",
          "type": "Dropout",
          "name": "layer10_attention_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_136",
          "type": "Linear",
          "name": "layer11_q_proj",
          "layer": "layer_11",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_137",
          "type": "Linear",
          "name": "layer11_k_proj",
          "layer": "layer_11",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_138",
          "type": "Linear",
          "name": "layer11_v_proj",
          "layer": "layer_11",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_139",
          "type": "Linear",
          "name": "layer11_o_proj",
          "layer": "layer_11",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_140",
          "type": "Linear",
          "name": "layer11_gate_proj",
          "layer": "layer_11",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_141",
          "type": "Linear",
          "name": "layer11_up_proj",
          "layer": "layer_11",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_142",
          "type": "Linear",
          "name": "layer11_down_proj",
          "layer": "layer_11",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_143",
          "type": "RMSNorm",
          "name": "layer11_input_layernorm",
          "layer": "layer_11",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_144",
          "type": "RMSNorm",
          "name": "layer11_post_attention_layernorm",
          "layer": "layer_11",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_145",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_146",
          "type": "SiLU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_147",
          "type": "Dropout",
          "name": "layer11_attention_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_148",
          "type": "Linear",
          "name": "layer12_q_proj",
          "layer": "layer_12",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_149",
          "type": "Linear",
          "name": "layer12_k_proj",
          "layer": "layer_12",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_150",
          "type": "Linear",
          "name": "layer12_v_proj",
          "layer": "layer_12",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_151",
          "type": "Linear",
          "name": "layer12_o_proj",
          "layer": "layer_12",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_152",
          "type": "Linear",
          "name": "layer12_gate_proj",
          "layer": "layer_12",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_153",
          "type": "Linear",
          "name": "layer12_up_proj",
          "layer": "layer_12",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_154",
          "type": "Linear",
          "name": "layer12_down_proj",
          "layer": "layer_12",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_155",
          "type": "RMSNorm",
          "name": "layer12_input_layernorm",
          "layer": "layer_12",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_156",
          "type": "RMSNorm",
          "name": "layer12_post_attention_layernorm",
          "layer": "layer_12",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_157",
          "type": "Softmax",
          "name": "layer12_attention_softmax",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_158",
          "type": "SiLU",
          "name": "layer12_activation",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_159",
          "type": "Dropout",
          "name": "layer12_attention_dropout",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_160",
          "type": "Linear",
          "name": "layer13_q_proj",
          "layer": "layer_13",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_161",
          "type": "Linear",
          "name": "layer13_k_proj",
          "layer": "layer_13",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_162",
          "type": "Linear",
          "name": "layer13_v_proj",
          "layer": "layer_13",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_163",
          "type": "Linear",
          "name": "layer13_o_proj",
          "layer": "layer_13",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_164",
          "type": "Linear",
          "name": "layer13_gate_proj",
          "layer": "layer_13",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_165",
          "type": "Linear",
          "name": "layer13_up_proj",
          "layer": "layer_13",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_166",
          "type": "Linear",
          "name": "layer13_down_proj",
          "layer": "layer_13",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_167",
          "type": "RMSNorm",
          "name": "layer13_input_layernorm",
          "layer": "layer_13",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_168",
          "type": "RMSNorm",
          "name": "layer13_post_attention_layernorm",
          "layer": "layer_13",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_169",
          "type": "Softmax",
          "name": "layer13_attention_softmax",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_170",
          "type": "SiLU",
          "name": "layer13_activation",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_171",
          "type": "Dropout",
          "name": "layer13_attention_dropout",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_172",
          "type": "Linear",
          "name": "layer14_q_proj",
          "layer": "layer_14",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_173",
          "type": "Linear",
          "name": "layer14_k_proj",
          "layer": "layer_14",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_174",
          "type": "Linear",
          "name": "layer14_v_proj",
          "layer": "layer_14",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_175",
          "type": "Linear",
          "name": "layer14_o_proj",
          "layer": "layer_14",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_176",
          "type": "Linear",
          "name": "layer14_gate_proj",
          "layer": "layer_14",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_177",
          "type": "Linear",
          "name": "layer14_up_proj",
          "layer": "layer_14",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_178",
          "type": "Linear",
          "name": "layer14_down_proj",
          "layer": "layer_14",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_179",
          "type": "RMSNorm",
          "name": "layer14_input_layernorm",
          "layer": "layer_14",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_180",
          "type": "RMSNorm",
          "name": "layer14_post_attention_layernorm",
          "layer": "layer_14",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_181",
          "type": "Softmax",
          "name": "layer14_attention_softmax",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_182",
          "type": "SiLU",
          "name": "layer14_activation",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_183",
          "type": "Dropout",
          "name": "layer14_attention_dropout",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_184",
          "type": "Linear",
          "name": "layer15_q_proj",
          "layer": "layer_15",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_185",
          "type": "Linear",
          "name": "layer15_k_proj",
          "layer": "layer_15",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_186",
          "type": "Linear",
          "name": "layer15_v_proj",
          "layer": "layer_15",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_187",
          "type": "Linear",
          "name": "layer15_o_proj",
          "layer": "layer_15",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_188",
          "type": "Linear",
          "name": "layer15_gate_proj",
          "layer": "layer_15",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_189",
          "type": "Linear",
          "name": "layer15_up_proj",
          "layer": "layer_15",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_190",
          "type": "Linear",
          "name": "layer15_down_proj",
          "layer": "layer_15",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_191",
          "type": "RMSNorm",
          "name": "layer15_input_layernorm",
          "layer": "layer_15",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_192",
          "type": "RMSNorm",
          "name": "layer15_post_attention_layernorm",
          "layer": "layer_15",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_193",
          "type": "Softmax",
          "name": "layer15_attention_softmax",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_194",
          "type": "SiLU",
          "name": "layer15_activation",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_195",
          "type": "Dropout",
          "name": "layer15_attention_dropout",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_196",
          "type": "Linear",
          "name": "layer16_q_proj",
          "layer": "layer_16",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_197",
          "type": "Linear",
          "name": "layer16_k_proj",
          "layer": "layer_16",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_198",
          "type": "Linear",
          "name": "layer16_v_proj",
          "layer": "layer_16",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_199",
          "type": "Linear",
          "name": "layer16_o_proj",
          "layer": "layer_16",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_200",
          "type": "Linear",
          "name": "layer16_gate_proj",
          "layer": "layer_16",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_201",
          "type": "Linear",
          "name": "layer16_up_proj",
          "layer": "layer_16",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_202",
          "type": "Linear",
          "name": "layer16_down_proj",
          "layer": "layer_16",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_203",
          "type": "RMSNorm",
          "name": "layer16_input_layernorm",
          "layer": "layer_16",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_204",
          "type": "RMSNorm",
          "name": "layer16_post_attention_layernorm",
          "layer": "layer_16",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_205",
          "type": "Softmax",
          "name": "layer16_attention_softmax",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_206",
          "type": "SiLU",
          "name": "layer16_activation",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_207",
          "type": "Dropout",
          "name": "layer16_attention_dropout",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_208",
          "type": "Linear",
          "name": "layer17_q_proj",
          "layer": "layer_17",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_209",
          "type": "Linear",
          "name": "layer17_k_proj",
          "layer": "layer_17",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_210",
          "type": "Linear",
          "name": "layer17_v_proj",
          "layer": "layer_17",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_211",
          "type": "Linear",
          "name": "layer17_o_proj",
          "layer": "layer_17",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_212",
          "type": "Linear",
          "name": "layer17_gate_proj",
          "layer": "layer_17",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_213",
          "type": "Linear",
          "name": "layer17_up_proj",
          "layer": "layer_17",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_214",
          "type": "Linear",
          "name": "layer17_down_proj",
          "layer": "layer_17",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_215",
          "type": "RMSNorm",
          "name": "layer17_input_layernorm",
          "layer": "layer_17",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_216",
          "type": "RMSNorm",
          "name": "layer17_post_attention_layernorm",
          "layer": "layer_17",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_217",
          "type": "Softmax",
          "name": "layer17_attention_softmax",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_218",
          "type": "SiLU",
          "name": "layer17_activation",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_219",
          "type": "Dropout",
          "name": "layer17_attention_dropout",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_220",
          "type": "Linear",
          "name": "layer18_q_proj",
          "layer": "layer_18",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_221",
          "type": "Linear",
          "name": "layer18_k_proj",
          "layer": "layer_18",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_222",
          "type": "Linear",
          "name": "layer18_v_proj",
          "layer": "layer_18",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_223",
          "type": "Linear",
          "name": "layer18_o_proj",
          "layer": "layer_18",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_224",
          "type": "Linear",
          "name": "layer18_gate_proj",
          "layer": "layer_18",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_225",
          "type": "Linear",
          "name": "layer18_up_proj",
          "layer": "layer_18",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_226",
          "type": "Linear",
          "name": "layer18_down_proj",
          "layer": "layer_18",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_227",
          "type": "RMSNorm",
          "name": "layer18_input_layernorm",
          "layer": "layer_18",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_228",
          "type": "RMSNorm",
          "name": "layer18_post_attention_layernorm",
          "layer": "layer_18",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_229",
          "type": "Softmax",
          "name": "layer18_attention_softmax",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_230",
          "type": "SiLU",
          "name": "layer18_activation",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_231",
          "type": "Dropout",
          "name": "layer18_attention_dropout",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_232",
          "type": "Linear",
          "name": "layer19_q_proj",
          "layer": "layer_19",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_233",
          "type": "Linear",
          "name": "layer19_k_proj",
          "layer": "layer_19",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_234",
          "type": "Linear",
          "name": "layer19_v_proj",
          "layer": "layer_19",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_235",
          "type": "Linear",
          "name": "layer19_o_proj",
          "layer": "layer_19",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_236",
          "type": "Linear",
          "name": "layer19_gate_proj",
          "layer": "layer_19",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_237",
          "type": "Linear",
          "name": "layer19_up_proj",
          "layer": "layer_19",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_238",
          "type": "Linear",
          "name": "layer19_down_proj",
          "layer": "layer_19",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_239",
          "type": "RMSNorm",
          "name": "layer19_input_layernorm",
          "layer": "layer_19",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_240",
          "type": "RMSNorm",
          "name": "layer19_post_attention_layernorm",
          "layer": "layer_19",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_241",
          "type": "Softmax",
          "name": "layer19_attention_softmax",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_242",
          "type": "SiLU",
          "name": "layer19_activation",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_243",
          "type": "Dropout",
          "name": "layer19_attention_dropout",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_244",
          "type": "Linear",
          "name": "layer20_q_proj",
          "layer": "layer_20",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_245",
          "type": "Linear",
          "name": "layer20_k_proj",
          "layer": "layer_20",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_246",
          "type": "Linear",
          "name": "layer20_v_proj",
          "layer": "layer_20",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_247",
          "type": "Linear",
          "name": "layer20_o_proj",
          "layer": "layer_20",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_248",
          "type": "Linear",
          "name": "layer20_gate_proj",
          "layer": "layer_20",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_249",
          "type": "Linear",
          "name": "layer20_up_proj",
          "layer": "layer_20",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_250",
          "type": "Linear",
          "name": "layer20_down_proj",
          "layer": "layer_20",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_251",
          "type": "RMSNorm",
          "name": "layer20_input_layernorm",
          "layer": "layer_20",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_252",
          "type": "RMSNorm",
          "name": "layer20_post_attention_layernorm",
          "layer": "layer_20",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_253",
          "type": "Softmax",
          "name": "layer20_attention_softmax",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_254",
          "type": "SiLU",
          "name": "layer20_activation",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_255",
          "type": "Dropout",
          "name": "layer20_attention_dropout",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_256",
          "type": "Linear",
          "name": "layer21_q_proj",
          "layer": "layer_21",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_257",
          "type": "Linear",
          "name": "layer21_k_proj",
          "layer": "layer_21",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_258",
          "type": "Linear",
          "name": "layer21_v_proj",
          "layer": "layer_21",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_259",
          "type": "Linear",
          "name": "layer21_o_proj",
          "layer": "layer_21",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_260",
          "type": "Linear",
          "name": "layer21_gate_proj",
          "layer": "layer_21",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_261",
          "type": "Linear",
          "name": "layer21_up_proj",
          "layer": "layer_21",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_262",
          "type": "Linear",
          "name": "layer21_down_proj",
          "layer": "layer_21",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_263",
          "type": "RMSNorm",
          "name": "layer21_input_layernorm",
          "layer": "layer_21",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_264",
          "type": "RMSNorm",
          "name": "layer21_post_attention_layernorm",
          "layer": "layer_21",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_265",
          "type": "Softmax",
          "name": "layer21_attention_softmax",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_266",
          "type": "SiLU",
          "name": "layer21_activation",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_267",
          "type": "Dropout",
          "name": "layer21_attention_dropout",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_268",
          "type": "Linear",
          "name": "layer22_q_proj",
          "layer": "layer_22",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_269",
          "type": "Linear",
          "name": "layer22_k_proj",
          "layer": "layer_22",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_270",
          "type": "Linear",
          "name": "layer22_v_proj",
          "layer": "layer_22",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_271",
          "type": "Linear",
          "name": "layer22_o_proj",
          "layer": "layer_22",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_272",
          "type": "Linear",
          "name": "layer22_gate_proj",
          "layer": "layer_22",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_273",
          "type": "Linear",
          "name": "layer22_up_proj",
          "layer": "layer_22",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_274",
          "type": "Linear",
          "name": "layer22_down_proj",
          "layer": "layer_22",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_275",
          "type": "RMSNorm",
          "name": "layer22_input_layernorm",
          "layer": "layer_22",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_276",
          "type": "RMSNorm",
          "name": "layer22_post_attention_layernorm",
          "layer": "layer_22",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_277",
          "type": "Softmax",
          "name": "layer22_attention_softmax",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_278",
          "type": "SiLU",
          "name": "layer22_activation",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_279",
          "type": "Dropout",
          "name": "layer22_attention_dropout",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_280",
          "type": "Linear",
          "name": "layer23_q_proj",
          "layer": "layer_23",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_281",
          "type": "Linear",
          "name": "layer23_k_proj",
          "layer": "layer_23",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_282",
          "type": "Linear",
          "name": "layer23_v_proj",
          "layer": "layer_23",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_283",
          "type": "Linear",
          "name": "layer23_o_proj",
          "layer": "layer_23",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_284",
          "type": "Linear",
          "name": "layer23_gate_proj",
          "layer": "layer_23",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_285",
          "type": "Linear",
          "name": "layer23_up_proj",
          "layer": "layer_23",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_286",
          "type": "Linear",
          "name": "layer23_down_proj",
          "layer": "layer_23",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_287",
          "type": "RMSNorm",
          "name": "layer23_input_layernorm",
          "layer": "layer_23",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_288",
          "type": "RMSNorm",
          "name": "layer23_post_attention_layernorm",
          "layer": "layer_23",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_289",
          "type": "Softmax",
          "name": "layer23_attention_softmax",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_290",
          "type": "SiLU",
          "name": "layer23_activation",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_291",
          "type": "Dropout",
          "name": "layer23_attention_dropout",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_292",
          "type": "Linear",
          "name": "layer24_q_proj",
          "layer": "layer_24",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_293",
          "type": "Linear",
          "name": "layer24_k_proj",
          "layer": "layer_24",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_294",
          "type": "Linear",
          "name": "layer24_v_proj",
          "layer": "layer_24",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_295",
          "type": "Linear",
          "name": "layer24_o_proj",
          "layer": "layer_24",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_296",
          "type": "Linear",
          "name": "layer24_gate_proj",
          "layer": "layer_24",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_297",
          "type": "Linear",
          "name": "layer24_up_proj",
          "layer": "layer_24",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_298",
          "type": "Linear",
          "name": "layer24_down_proj",
          "layer": "layer_24",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_299",
          "type": "RMSNorm",
          "name": "layer24_input_layernorm",
          "layer": "layer_24",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_300",
          "type": "RMSNorm",
          "name": "layer24_post_attention_layernorm",
          "layer": "layer_24",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_301",
          "type": "Softmax",
          "name": "layer24_attention_softmax",
          "layer": "layer_24",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_302",
          "type": "SiLU",
          "name": "layer24_activation",
          "layer": "layer_24",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_303",
          "type": "Dropout",
          "name": "layer24_attention_dropout",
          "layer": "layer_24",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_304",
          "type": "Linear",
          "name": "layer25_q_proj",
          "layer": "layer_25",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_305",
          "type": "Linear",
          "name": "layer25_k_proj",
          "layer": "layer_25",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_306",
          "type": "Linear",
          "name": "layer25_v_proj",
          "layer": "layer_25",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_307",
          "type": "Linear",
          "name": "layer25_o_proj",
          "layer": "layer_25",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_308",
          "type": "Linear",
          "name": "layer25_gate_proj",
          "layer": "layer_25",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_309",
          "type": "Linear",
          "name": "layer25_up_proj",
          "layer": "layer_25",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_310",
          "type": "Linear",
          "name": "layer25_down_proj",
          "layer": "layer_25",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_311",
          "type": "RMSNorm",
          "name": "layer25_input_layernorm",
          "layer": "layer_25",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_312",
          "type": "RMSNorm",
          "name": "layer25_post_attention_layernorm",
          "layer": "layer_25",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_313",
          "type": "Softmax",
          "name": "layer25_attention_softmax",
          "layer": "layer_25",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_314",
          "type": "SiLU",
          "name": "layer25_activation",
          "layer": "layer_25",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_315",
          "type": "Dropout",
          "name": "layer25_attention_dropout",
          "layer": "layer_25",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_316",
          "type": "Linear",
          "name": "layer26_q_proj",
          "layer": "layer_26",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_317",
          "type": "Linear",
          "name": "layer26_k_proj",
          "layer": "layer_26",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_318",
          "type": "Linear",
          "name": "layer26_v_proj",
          "layer": "layer_26",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_319",
          "type": "Linear",
          "name": "layer26_o_proj",
          "layer": "layer_26",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_320",
          "type": "Linear",
          "name": "layer26_gate_proj",
          "layer": "layer_26",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_321",
          "type": "Linear",
          "name": "layer26_up_proj",
          "layer": "layer_26",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_322",
          "type": "Linear",
          "name": "layer26_down_proj",
          "layer": "layer_26",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_323",
          "type": "RMSNorm",
          "name": "layer26_input_layernorm",
          "layer": "layer_26",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_324",
          "type": "RMSNorm",
          "name": "layer26_post_attention_layernorm",
          "layer": "layer_26",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_325",
          "type": "Softmax",
          "name": "layer26_attention_softmax",
          "layer": "layer_26",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_326",
          "type": "SiLU",
          "name": "layer26_activation",
          "layer": "layer_26",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_327",
          "type": "Dropout",
          "name": "layer26_attention_dropout",
          "layer": "layer_26",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_328",
          "type": "Linear",
          "name": "layer27_q_proj",
          "layer": "layer_27",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_329",
          "type": "Linear",
          "name": "layer27_k_proj",
          "layer": "layer_27",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_330",
          "type": "Linear",
          "name": "layer27_v_proj",
          "layer": "layer_27",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_331",
          "type": "Linear",
          "name": "layer27_o_proj",
          "layer": "layer_27",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_332",
          "type": "Linear",
          "name": "layer27_gate_proj",
          "layer": "layer_27",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_333",
          "type": "Linear",
          "name": "layer27_up_proj",
          "layer": "layer_27",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_334",
          "type": "Linear",
          "name": "layer27_down_proj",
          "layer": "layer_27",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_335",
          "type": "RMSNorm",
          "name": "layer27_input_layernorm",
          "layer": "layer_27",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_336",
          "type": "RMSNorm",
          "name": "layer27_post_attention_layernorm",
          "layer": "layer_27",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_337",
          "type": "Softmax",
          "name": "layer27_attention_softmax",
          "layer": "layer_27",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_338",
          "type": "SiLU",
          "name": "layer27_activation",
          "layer": "layer_27",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_339",
          "type": "Dropout",
          "name": "layer27_attention_dropout",
          "layer": "layer_27",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_340",
          "type": "Linear",
          "name": "layer28_q_proj",
          "layer": "layer_28",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_341",
          "type": "Linear",
          "name": "layer28_k_proj",
          "layer": "layer_28",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_342",
          "type": "Linear",
          "name": "layer28_v_proj",
          "layer": "layer_28",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_343",
          "type": "Linear",
          "name": "layer28_o_proj",
          "layer": "layer_28",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_344",
          "type": "Linear",
          "name": "layer28_gate_proj",
          "layer": "layer_28",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_345",
          "type": "Linear",
          "name": "layer28_up_proj",
          "layer": "layer_28",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_346",
          "type": "Linear",
          "name": "layer28_down_proj",
          "layer": "layer_28",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_347",
          "type": "RMSNorm",
          "name": "layer28_input_layernorm",
          "layer": "layer_28",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_348",
          "type": "RMSNorm",
          "name": "layer28_post_attention_layernorm",
          "layer": "layer_28",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_349",
          "type": "Softmax",
          "name": "layer28_attention_softmax",
          "layer": "layer_28",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_350",
          "type": "SiLU",
          "name": "layer28_activation",
          "layer": "layer_28",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_351",
          "type": "Dropout",
          "name": "layer28_attention_dropout",
          "layer": "layer_28",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_352",
          "type": "Linear",
          "name": "layer29_q_proj",
          "layer": "layer_29",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_353",
          "type": "Linear",
          "name": "layer29_k_proj",
          "layer": "layer_29",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_354",
          "type": "Linear",
          "name": "layer29_v_proj",
          "layer": "layer_29",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_355",
          "type": "Linear",
          "name": "layer29_o_proj",
          "layer": "layer_29",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_356",
          "type": "Linear",
          "name": "layer29_gate_proj",
          "layer": "layer_29",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_357",
          "type": "Linear",
          "name": "layer29_up_proj",
          "layer": "layer_29",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_358",
          "type": "Linear",
          "name": "layer29_down_proj",
          "layer": "layer_29",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_359",
          "type": "RMSNorm",
          "name": "layer29_input_layernorm",
          "layer": "layer_29",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_360",
          "type": "RMSNorm",
          "name": "layer29_post_attention_layernorm",
          "layer": "layer_29",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_361",
          "type": "Softmax",
          "name": "layer29_attention_softmax",
          "layer": "layer_29",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_362",
          "type": "SiLU",
          "name": "layer29_activation",
          "layer": "layer_29",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_363",
          "type": "Dropout",
          "name": "layer29_attention_dropout",
          "layer": "layer_29",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_364",
          "type": "Linear",
          "name": "layer30_q_proj",
          "layer": "layer_30",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_365",
          "type": "Linear",
          "name": "layer30_k_proj",
          "layer": "layer_30",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_366",
          "type": "Linear",
          "name": "layer30_v_proj",
          "layer": "layer_30",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_367",
          "type": "Linear",
          "name": "layer30_o_proj",
          "layer": "layer_30",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_368",
          "type": "Linear",
          "name": "layer30_gate_proj",
          "layer": "layer_30",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_369",
          "type": "Linear",
          "name": "layer30_up_proj",
          "layer": "layer_30",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_370",
          "type": "Linear",
          "name": "layer30_down_proj",
          "layer": "layer_30",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_371",
          "type": "RMSNorm",
          "name": "layer30_input_layernorm",
          "layer": "layer_30",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_372",
          "type": "RMSNorm",
          "name": "layer30_post_attention_layernorm",
          "layer": "layer_30",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_373",
          "type": "Softmax",
          "name": "layer30_attention_softmax",
          "layer": "layer_30",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_374",
          "type": "SiLU",
          "name": "layer30_activation",
          "layer": "layer_30",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_375",
          "type": "Dropout",
          "name": "layer30_attention_dropout",
          "layer": "layer_30",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_376",
          "type": "Linear",
          "name": "layer31_q_proj",
          "layer": "layer_31",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_377",
          "type": "Linear",
          "name": "layer31_k_proj",
          "layer": "layer_31",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_378",
          "type": "Linear",
          "name": "layer31_v_proj",
          "layer": "layer_31",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_379",
          "type": "Linear",
          "name": "layer31_o_proj",
          "layer": "layer_31",
          "parameters": 6553600,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_380",
          "type": "Linear",
          "name": "layer31_gate_proj",
          "layer": "layer_31",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_381",
          "type": "Linear",
          "name": "layer31_up_proj",
          "layer": "layer_31",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_382",
          "type": "Linear",
          "name": "layer31_down_proj",
          "layer": "layer_31",
          "parameters": 17694720,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "StableLM_3B_op_383",
          "type": "RMSNorm",
          "name": "layer31_input_layernorm",
          "layer": "layer_31",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_384",
          "type": "RMSNorm",
          "name": "layer31_post_attention_layernorm",
          "layer": "layer_31",
          "parameters": 2560,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        },
        {
          "op_id": "StableLM_3B_op_385",
          "type": "Softmax",
          "name": "layer31_attention_softmax",
          "layer": "layer_31",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "StableLM_3B_op_386",
          "type": "SiLU",
          "name": "layer31_activation",
          "layer": "layer_31",
          "parameters": 0,
          "input_shape": "[batch, seq, 6912]",
          "output_shape": "[batch, seq, 6912]"
        },
        {
          "op_id": "StableLM_3B_op_387",
          "type": "Dropout",
          "name": "layer31_attention_dropout",
          "layer": "layer_31",
          "parameters": 0,
          "input_shape": "[batch, seq, 2560]",
          "output_shape": "[batch, seq, 2560]"
        }
      ],
      "parameters": 3000000000
    },
    {
      "model_id": "TinyLlama_1.1B",
      "name": "TinyLlama-1.1B",
      "source": "huggingface/tinyllama-1.1b",
      "category": "LLM",
      "architecture": "llm_decoder",
      "num_params": 1100000000,
      "num_layers": 22,
      "hidden_size": 2048,
      "vocab_size": 32000,
      "intermediate_size": 5632,
      "operator_count": 267,
      "operators": [
        {
          "op_id": "TinyLlama_1.1B_op_1",
          "type": "Embedding",
          "name": "embed_tokens",
          "layer": "global",
          "parameters": 65536000,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_2",
          "type": "RMSNorm",
          "name": "norm",
          "layer": "global",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_3",
          "type": "Linear",
          "name": "lm_head",
          "layer": "global",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_4",
          "type": "Linear",
          "name": "layer0_q_proj",
          "layer": "layer_0",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_5",
          "type": "Linear",
          "name": "layer0_k_proj",
          "layer": "layer_0",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_6",
          "type": "Linear",
          "name": "layer0_v_proj",
          "layer": "layer_0",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_7",
          "type": "Linear",
          "name": "layer0_o_proj",
          "layer": "layer_0",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_8",
          "type": "Linear",
          "name": "layer0_gate_proj",
          "layer": "layer_0",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_9",
          "type": "Linear",
          "name": "layer0_up_proj",
          "layer": "layer_0",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_10",
          "type": "Linear",
          "name": "layer0_down_proj",
          "layer": "layer_0",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_11",
          "type": "RMSNorm",
          "name": "layer0_input_layernorm",
          "layer": "layer_0",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_12",
          "type": "RMSNorm",
          "name": "layer0_post_attention_layernorm",
          "layer": "layer_0",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_13",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_14",
          "type": "SiLU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_15",
          "type": "Dropout",
          "name": "layer0_attention_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_16",
          "type": "Linear",
          "name": "layer1_q_proj",
          "layer": "layer_1",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_17",
          "type": "Linear",
          "name": "layer1_k_proj",
          "layer": "layer_1",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_18",
          "type": "Linear",
          "name": "layer1_v_proj",
          "layer": "layer_1",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_19",
          "type": "Linear",
          "name": "layer1_o_proj",
          "layer": "layer_1",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_20",
          "type": "Linear",
          "name": "layer1_gate_proj",
          "layer": "layer_1",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_21",
          "type": "Linear",
          "name": "layer1_up_proj",
          "layer": "layer_1",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_22",
          "type": "Linear",
          "name": "layer1_down_proj",
          "layer": "layer_1",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_23",
          "type": "RMSNorm",
          "name": "layer1_input_layernorm",
          "layer": "layer_1",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_24",
          "type": "RMSNorm",
          "name": "layer1_post_attention_layernorm",
          "layer": "layer_1",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_25",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_26",
          "type": "SiLU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_27",
          "type": "Dropout",
          "name": "layer1_attention_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_28",
          "type": "Linear",
          "name": "layer2_q_proj",
          "layer": "layer_2",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_29",
          "type": "Linear",
          "name": "layer2_k_proj",
          "layer": "layer_2",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_30",
          "type": "Linear",
          "name": "layer2_v_proj",
          "layer": "layer_2",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_31",
          "type": "Linear",
          "name": "layer2_o_proj",
          "layer": "layer_2",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_32",
          "type": "Linear",
          "name": "layer2_gate_proj",
          "layer": "layer_2",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_33",
          "type": "Linear",
          "name": "layer2_up_proj",
          "layer": "layer_2",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_34",
          "type": "Linear",
          "name": "layer2_down_proj",
          "layer": "layer_2",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_35",
          "type": "RMSNorm",
          "name": "layer2_input_layernorm",
          "layer": "layer_2",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_36",
          "type": "RMSNorm",
          "name": "layer2_post_attention_layernorm",
          "layer": "layer_2",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_37",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_38",
          "type": "SiLU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_39",
          "type": "Dropout",
          "name": "layer2_attention_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_40",
          "type": "Linear",
          "name": "layer3_q_proj",
          "layer": "layer_3",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_41",
          "type": "Linear",
          "name": "layer3_k_proj",
          "layer": "layer_3",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_42",
          "type": "Linear",
          "name": "layer3_v_proj",
          "layer": "layer_3",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_43",
          "type": "Linear",
          "name": "layer3_o_proj",
          "layer": "layer_3",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_44",
          "type": "Linear",
          "name": "layer3_gate_proj",
          "layer": "layer_3",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_45",
          "type": "Linear",
          "name": "layer3_up_proj",
          "layer": "layer_3",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_46",
          "type": "Linear",
          "name": "layer3_down_proj",
          "layer": "layer_3",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_47",
          "type": "RMSNorm",
          "name": "layer3_input_layernorm",
          "layer": "layer_3",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_48",
          "type": "RMSNorm",
          "name": "layer3_post_attention_layernorm",
          "layer": "layer_3",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_49",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_50",
          "type": "SiLU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_51",
          "type": "Dropout",
          "name": "layer3_attention_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_52",
          "type": "Linear",
          "name": "layer4_q_proj",
          "layer": "layer_4",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_53",
          "type": "Linear",
          "name": "layer4_k_proj",
          "layer": "layer_4",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_54",
          "type": "Linear",
          "name": "layer4_v_proj",
          "layer": "layer_4",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_55",
          "type": "Linear",
          "name": "layer4_o_proj",
          "layer": "layer_4",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_56",
          "type": "Linear",
          "name": "layer4_gate_proj",
          "layer": "layer_4",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_57",
          "type": "Linear",
          "name": "layer4_up_proj",
          "layer": "layer_4",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_58",
          "type": "Linear",
          "name": "layer4_down_proj",
          "layer": "layer_4",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_59",
          "type": "RMSNorm",
          "name": "layer4_input_layernorm",
          "layer": "layer_4",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_60",
          "type": "RMSNorm",
          "name": "layer4_post_attention_layernorm",
          "layer": "layer_4",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_61",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_62",
          "type": "SiLU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_63",
          "type": "Dropout",
          "name": "layer4_attention_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_64",
          "type": "Linear",
          "name": "layer5_q_proj",
          "layer": "layer_5",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_65",
          "type": "Linear",
          "name": "layer5_k_proj",
          "layer": "layer_5",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_66",
          "type": "Linear",
          "name": "layer5_v_proj",
          "layer": "layer_5",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_67",
          "type": "Linear",
          "name": "layer5_o_proj",
          "layer": "layer_5",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_68",
          "type": "Linear",
          "name": "layer5_gate_proj",
          "layer": "layer_5",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_69",
          "type": "Linear",
          "name": "layer5_up_proj",
          "layer": "layer_5",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_70",
          "type": "Linear",
          "name": "layer5_down_proj",
          "layer": "layer_5",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_71",
          "type": "RMSNorm",
          "name": "layer5_input_layernorm",
          "layer": "layer_5",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_72",
          "type": "RMSNorm",
          "name": "layer5_post_attention_layernorm",
          "layer": "layer_5",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_73",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_74",
          "type": "SiLU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_75",
          "type": "Dropout",
          "name": "layer5_attention_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_76",
          "type": "Linear",
          "name": "layer6_q_proj",
          "layer": "layer_6",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_77",
          "type": "Linear",
          "name": "layer6_k_proj",
          "layer": "layer_6",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_78",
          "type": "Linear",
          "name": "layer6_v_proj",
          "layer": "layer_6",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_79",
          "type": "Linear",
          "name": "layer6_o_proj",
          "layer": "layer_6",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_80",
          "type": "Linear",
          "name": "layer6_gate_proj",
          "layer": "layer_6",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_81",
          "type": "Linear",
          "name": "layer6_up_proj",
          "layer": "layer_6",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_82",
          "type": "Linear",
          "name": "layer6_down_proj",
          "layer": "layer_6",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_83",
          "type": "RMSNorm",
          "name": "layer6_input_layernorm",
          "layer": "layer_6",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_84",
          "type": "RMSNorm",
          "name": "layer6_post_attention_layernorm",
          "layer": "layer_6",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_85",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_86",
          "type": "SiLU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_87",
          "type": "Dropout",
          "name": "layer6_attention_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_88",
          "type": "Linear",
          "name": "layer7_q_proj",
          "layer": "layer_7",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_89",
          "type": "Linear",
          "name": "layer7_k_proj",
          "layer": "layer_7",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_90",
          "type": "Linear",
          "name": "layer7_v_proj",
          "layer": "layer_7",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_91",
          "type": "Linear",
          "name": "layer7_o_proj",
          "layer": "layer_7",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_92",
          "type": "Linear",
          "name": "layer7_gate_proj",
          "layer": "layer_7",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_93",
          "type": "Linear",
          "name": "layer7_up_proj",
          "layer": "layer_7",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_94",
          "type": "Linear",
          "name": "layer7_down_proj",
          "layer": "layer_7",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_95",
          "type": "RMSNorm",
          "name": "layer7_input_layernorm",
          "layer": "layer_7",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_96",
          "type": "RMSNorm",
          "name": "layer7_post_attention_layernorm",
          "layer": "layer_7",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_97",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_98",
          "type": "SiLU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_99",
          "type": "Dropout",
          "name": "layer7_attention_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_100",
          "type": "Linear",
          "name": "layer8_q_proj",
          "layer": "layer_8",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_101",
          "type": "Linear",
          "name": "layer8_k_proj",
          "layer": "layer_8",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_102",
          "type": "Linear",
          "name": "layer8_v_proj",
          "layer": "layer_8",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_103",
          "type": "Linear",
          "name": "layer8_o_proj",
          "layer": "layer_8",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_104",
          "type": "Linear",
          "name": "layer8_gate_proj",
          "layer": "layer_8",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_105",
          "type": "Linear",
          "name": "layer8_up_proj",
          "layer": "layer_8",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_106",
          "type": "Linear",
          "name": "layer8_down_proj",
          "layer": "layer_8",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_107",
          "type": "RMSNorm",
          "name": "layer8_input_layernorm",
          "layer": "layer_8",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_108",
          "type": "RMSNorm",
          "name": "layer8_post_attention_layernorm",
          "layer": "layer_8",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_109",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_110",
          "type": "SiLU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_111",
          "type": "Dropout",
          "name": "layer8_attention_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_112",
          "type": "Linear",
          "name": "layer9_q_proj",
          "layer": "layer_9",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_113",
          "type": "Linear",
          "name": "layer9_k_proj",
          "layer": "layer_9",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_114",
          "type": "Linear",
          "name": "layer9_v_proj",
          "layer": "layer_9",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_115",
          "type": "Linear",
          "name": "layer9_o_proj",
          "layer": "layer_9",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_116",
          "type": "Linear",
          "name": "layer9_gate_proj",
          "layer": "layer_9",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_117",
          "type": "Linear",
          "name": "layer9_up_proj",
          "layer": "layer_9",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_118",
          "type": "Linear",
          "name": "layer9_down_proj",
          "layer": "layer_9",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_119",
          "type": "RMSNorm",
          "name": "layer9_input_layernorm",
          "layer": "layer_9",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_120",
          "type": "RMSNorm",
          "name": "layer9_post_attention_layernorm",
          "layer": "layer_9",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_121",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_122",
          "type": "SiLU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_123",
          "type": "Dropout",
          "name": "layer9_attention_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_124",
          "type": "Linear",
          "name": "layer10_q_proj",
          "layer": "layer_10",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_125",
          "type": "Linear",
          "name": "layer10_k_proj",
          "layer": "layer_10",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_126",
          "type": "Linear",
          "name": "layer10_v_proj",
          "layer": "layer_10",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_127",
          "type": "Linear",
          "name": "layer10_o_proj",
          "layer": "layer_10",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_128",
          "type": "Linear",
          "name": "layer10_gate_proj",
          "layer": "layer_10",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_129",
          "type": "Linear",
          "name": "layer10_up_proj",
          "layer": "layer_10",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_130",
          "type": "Linear",
          "name": "layer10_down_proj",
          "layer": "layer_10",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_131",
          "type": "RMSNorm",
          "name": "layer10_input_layernorm",
          "layer": "layer_10",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_132",
          "type": "RMSNorm",
          "name": "layer10_post_attention_layernorm",
          "layer": "layer_10",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_133",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_134",
          "type": "SiLU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_135",
          "type": "Dropout",
          "name": "layer10_attention_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_136",
          "type": "Linear",
          "name": "layer11_q_proj",
          "layer": "layer_11",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_137",
          "type": "Linear",
          "name": "layer11_k_proj",
          "layer": "layer_11",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_138",
          "type": "Linear",
          "name": "layer11_v_proj",
          "layer": "layer_11",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_139",
          "type": "Linear",
          "name": "layer11_o_proj",
          "layer": "layer_11",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_140",
          "type": "Linear",
          "name": "layer11_gate_proj",
          "layer": "layer_11",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_141",
          "type": "Linear",
          "name": "layer11_up_proj",
          "layer": "layer_11",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_142",
          "type": "Linear",
          "name": "layer11_down_proj",
          "layer": "layer_11",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_143",
          "type": "RMSNorm",
          "name": "layer11_input_layernorm",
          "layer": "layer_11",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_144",
          "type": "RMSNorm",
          "name": "layer11_post_attention_layernorm",
          "layer": "layer_11",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_145",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_146",
          "type": "SiLU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_147",
          "type": "Dropout",
          "name": "layer11_attention_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_148",
          "type": "Linear",
          "name": "layer12_q_proj",
          "layer": "layer_12",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_149",
          "type": "Linear",
          "name": "layer12_k_proj",
          "layer": "layer_12",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_150",
          "type": "Linear",
          "name": "layer12_v_proj",
          "layer": "layer_12",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_151",
          "type": "Linear",
          "name": "layer12_o_proj",
          "layer": "layer_12",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_152",
          "type": "Linear",
          "name": "layer12_gate_proj",
          "layer": "layer_12",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_153",
          "type": "Linear",
          "name": "layer12_up_proj",
          "layer": "layer_12",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_154",
          "type": "Linear",
          "name": "layer12_down_proj",
          "layer": "layer_12",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_155",
          "type": "RMSNorm",
          "name": "layer12_input_layernorm",
          "layer": "layer_12",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_156",
          "type": "RMSNorm",
          "name": "layer12_post_attention_layernorm",
          "layer": "layer_12",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_157",
          "type": "Softmax",
          "name": "layer12_attention_softmax",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_158",
          "type": "SiLU",
          "name": "layer12_activation",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_159",
          "type": "Dropout",
          "name": "layer12_attention_dropout",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_160",
          "type": "Linear",
          "name": "layer13_q_proj",
          "layer": "layer_13",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_161",
          "type": "Linear",
          "name": "layer13_k_proj",
          "layer": "layer_13",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_162",
          "type": "Linear",
          "name": "layer13_v_proj",
          "layer": "layer_13",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_163",
          "type": "Linear",
          "name": "layer13_o_proj",
          "layer": "layer_13",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_164",
          "type": "Linear",
          "name": "layer13_gate_proj",
          "layer": "layer_13",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_165",
          "type": "Linear",
          "name": "layer13_up_proj",
          "layer": "layer_13",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_166",
          "type": "Linear",
          "name": "layer13_down_proj",
          "layer": "layer_13",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_167",
          "type": "RMSNorm",
          "name": "layer13_input_layernorm",
          "layer": "layer_13",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_168",
          "type": "RMSNorm",
          "name": "layer13_post_attention_layernorm",
          "layer": "layer_13",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_169",
          "type": "Softmax",
          "name": "layer13_attention_softmax",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_170",
          "type": "SiLU",
          "name": "layer13_activation",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_171",
          "type": "Dropout",
          "name": "layer13_attention_dropout",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_172",
          "type": "Linear",
          "name": "layer14_q_proj",
          "layer": "layer_14",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_173",
          "type": "Linear",
          "name": "layer14_k_proj",
          "layer": "layer_14",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_174",
          "type": "Linear",
          "name": "layer14_v_proj",
          "layer": "layer_14",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_175",
          "type": "Linear",
          "name": "layer14_o_proj",
          "layer": "layer_14",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_176",
          "type": "Linear",
          "name": "layer14_gate_proj",
          "layer": "layer_14",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_177",
          "type": "Linear",
          "name": "layer14_up_proj",
          "layer": "layer_14",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_178",
          "type": "Linear",
          "name": "layer14_down_proj",
          "layer": "layer_14",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_179",
          "type": "RMSNorm",
          "name": "layer14_input_layernorm",
          "layer": "layer_14",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_180",
          "type": "RMSNorm",
          "name": "layer14_post_attention_layernorm",
          "layer": "layer_14",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_181",
          "type": "Softmax",
          "name": "layer14_attention_softmax",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_182",
          "type": "SiLU",
          "name": "layer14_activation",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_183",
          "type": "Dropout",
          "name": "layer14_attention_dropout",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_184",
          "type": "Linear",
          "name": "layer15_q_proj",
          "layer": "layer_15",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_185",
          "type": "Linear",
          "name": "layer15_k_proj",
          "layer": "layer_15",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_186",
          "type": "Linear",
          "name": "layer15_v_proj",
          "layer": "layer_15",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_187",
          "type": "Linear",
          "name": "layer15_o_proj",
          "layer": "layer_15",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_188",
          "type": "Linear",
          "name": "layer15_gate_proj",
          "layer": "layer_15",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_189",
          "type": "Linear",
          "name": "layer15_up_proj",
          "layer": "layer_15",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_190",
          "type": "Linear",
          "name": "layer15_down_proj",
          "layer": "layer_15",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_191",
          "type": "RMSNorm",
          "name": "layer15_input_layernorm",
          "layer": "layer_15",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_192",
          "type": "RMSNorm",
          "name": "layer15_post_attention_layernorm",
          "layer": "layer_15",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_193",
          "type": "Softmax",
          "name": "layer15_attention_softmax",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_194",
          "type": "SiLU",
          "name": "layer15_activation",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_195",
          "type": "Dropout",
          "name": "layer15_attention_dropout",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_196",
          "type": "Linear",
          "name": "layer16_q_proj",
          "layer": "layer_16",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_197",
          "type": "Linear",
          "name": "layer16_k_proj",
          "layer": "layer_16",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_198",
          "type": "Linear",
          "name": "layer16_v_proj",
          "layer": "layer_16",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_199",
          "type": "Linear",
          "name": "layer16_o_proj",
          "layer": "layer_16",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_200",
          "type": "Linear",
          "name": "layer16_gate_proj",
          "layer": "layer_16",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_201",
          "type": "Linear",
          "name": "layer16_up_proj",
          "layer": "layer_16",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_202",
          "type": "Linear",
          "name": "layer16_down_proj",
          "layer": "layer_16",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_203",
          "type": "RMSNorm",
          "name": "layer16_input_layernorm",
          "layer": "layer_16",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_204",
          "type": "RMSNorm",
          "name": "layer16_post_attention_layernorm",
          "layer": "layer_16",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_205",
          "type": "Softmax",
          "name": "layer16_attention_softmax",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_206",
          "type": "SiLU",
          "name": "layer16_activation",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_207",
          "type": "Dropout",
          "name": "layer16_attention_dropout",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_208",
          "type": "Linear",
          "name": "layer17_q_proj",
          "layer": "layer_17",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_209",
          "type": "Linear",
          "name": "layer17_k_proj",
          "layer": "layer_17",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_210",
          "type": "Linear",
          "name": "layer17_v_proj",
          "layer": "layer_17",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_211",
          "type": "Linear",
          "name": "layer17_o_proj",
          "layer": "layer_17",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_212",
          "type": "Linear",
          "name": "layer17_gate_proj",
          "layer": "layer_17",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_213",
          "type": "Linear",
          "name": "layer17_up_proj",
          "layer": "layer_17",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_214",
          "type": "Linear",
          "name": "layer17_down_proj",
          "layer": "layer_17",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_215",
          "type": "RMSNorm",
          "name": "layer17_input_layernorm",
          "layer": "layer_17",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_216",
          "type": "RMSNorm",
          "name": "layer17_post_attention_layernorm",
          "layer": "layer_17",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_217",
          "type": "Softmax",
          "name": "layer17_attention_softmax",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_218",
          "type": "SiLU",
          "name": "layer17_activation",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_219",
          "type": "Dropout",
          "name": "layer17_attention_dropout",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_220",
          "type": "Linear",
          "name": "layer18_q_proj",
          "layer": "layer_18",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_221",
          "type": "Linear",
          "name": "layer18_k_proj",
          "layer": "layer_18",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_222",
          "type": "Linear",
          "name": "layer18_v_proj",
          "layer": "layer_18",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_223",
          "type": "Linear",
          "name": "layer18_o_proj",
          "layer": "layer_18",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_224",
          "type": "Linear",
          "name": "layer18_gate_proj",
          "layer": "layer_18",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_225",
          "type": "Linear",
          "name": "layer18_up_proj",
          "layer": "layer_18",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_226",
          "type": "Linear",
          "name": "layer18_down_proj",
          "layer": "layer_18",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_227",
          "type": "RMSNorm",
          "name": "layer18_input_layernorm",
          "layer": "layer_18",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_228",
          "type": "RMSNorm",
          "name": "layer18_post_attention_layernorm",
          "layer": "layer_18",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_229",
          "type": "Softmax",
          "name": "layer18_attention_softmax",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_230",
          "type": "SiLU",
          "name": "layer18_activation",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_231",
          "type": "Dropout",
          "name": "layer18_attention_dropout",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_232",
          "type": "Linear",
          "name": "layer19_q_proj",
          "layer": "layer_19",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_233",
          "type": "Linear",
          "name": "layer19_k_proj",
          "layer": "layer_19",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_234",
          "type": "Linear",
          "name": "layer19_v_proj",
          "layer": "layer_19",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_235",
          "type": "Linear",
          "name": "layer19_o_proj",
          "layer": "layer_19",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_236",
          "type": "Linear",
          "name": "layer19_gate_proj",
          "layer": "layer_19",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_237",
          "type": "Linear",
          "name": "layer19_up_proj",
          "layer": "layer_19",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_238",
          "type": "Linear",
          "name": "layer19_down_proj",
          "layer": "layer_19",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_239",
          "type": "RMSNorm",
          "name": "layer19_input_layernorm",
          "layer": "layer_19",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_240",
          "type": "RMSNorm",
          "name": "layer19_post_attention_layernorm",
          "layer": "layer_19",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_241",
          "type": "Softmax",
          "name": "layer19_attention_softmax",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_242",
          "type": "SiLU",
          "name": "layer19_activation",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_243",
          "type": "Dropout",
          "name": "layer19_attention_dropout",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_244",
          "type": "Linear",
          "name": "layer20_q_proj",
          "layer": "layer_20",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_245",
          "type": "Linear",
          "name": "layer20_k_proj",
          "layer": "layer_20",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_246",
          "type": "Linear",
          "name": "layer20_v_proj",
          "layer": "layer_20",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_247",
          "type": "Linear",
          "name": "layer20_o_proj",
          "layer": "layer_20",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_248",
          "type": "Linear",
          "name": "layer20_gate_proj",
          "layer": "layer_20",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_249",
          "type": "Linear",
          "name": "layer20_up_proj",
          "layer": "layer_20",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_250",
          "type": "Linear",
          "name": "layer20_down_proj",
          "layer": "layer_20",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_251",
          "type": "RMSNorm",
          "name": "layer20_input_layernorm",
          "layer": "layer_20",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_252",
          "type": "RMSNorm",
          "name": "layer20_post_attention_layernorm",
          "layer": "layer_20",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_253",
          "type": "Softmax",
          "name": "layer20_attention_softmax",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_254",
          "type": "SiLU",
          "name": "layer20_activation",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_255",
          "type": "Dropout",
          "name": "layer20_attention_dropout",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_256",
          "type": "Linear",
          "name": "layer21_q_proj",
          "layer": "layer_21",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_257",
          "type": "Linear",
          "name": "layer21_k_proj",
          "layer": "layer_21",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_258",
          "type": "Linear",
          "name": "layer21_v_proj",
          "layer": "layer_21",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_259",
          "type": "Linear",
          "name": "layer21_o_proj",
          "layer": "layer_21",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_260",
          "type": "Linear",
          "name": "layer21_gate_proj",
          "layer": "layer_21",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_261",
          "type": "Linear",
          "name": "layer21_up_proj",
          "layer": "layer_21",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_262",
          "type": "Linear",
          "name": "layer21_down_proj",
          "layer": "layer_21",
          "parameters": 11534336,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_263",
          "type": "RMSNorm",
          "name": "layer21_input_layernorm",
          "layer": "layer_21",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_264",
          "type": "RMSNorm",
          "name": "layer21_post_attention_layernorm",
          "layer": "layer_21",
          "parameters": 2048,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_265",
          "type": "Softmax",
          "name": "layer21_attention_softmax",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_266",
          "type": "SiLU",
          "name": "layer21_activation",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 5632]",
          "output_shape": "[batch, seq, 5632]"
        },
        {
          "op_id": "TinyLlama_1.1B_op_267",
          "type": "Dropout",
          "name": "layer21_attention_dropout",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        }
      ],
      "parameters": 1100000000
    },
    {
      "model_id": "Pythia_1.4B",
      "name": "Pythia-1.4B",
      "source": "huggingface/pythia-1.4b",
      "category": "LLM",
      "architecture": "gpt2_style",
      "num_params": 1400000000,
      "num_layers": 24,
      "hidden_size": 2048,
      "vocab_size": 50304,
      "intermediate_size": 8192,
      "operator_count": 243,
      "operators": [
        {
          "op_id": "Pythia_1.4B_op_1",
          "type": "Embedding",
          "name": "wte",
          "layer": "global",
          "parameters": 4194304,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_2",
          "type": "Embedding",
          "name": "wpe",
          "layer": "global",
          "parameters": 4194304,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_3",
          "type": "LayerNorm",
          "name": "ln_f",
          "layer": "global",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_4",
          "type": "Linear",
          "name": "layer0_c_attn",
          "layer": "layer_0",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_5",
          "type": "Linear",
          "name": "layer0_c_proj",
          "layer": "layer_0",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_6",
          "type": "Linear",
          "name": "layer0_c_fc",
          "layer": "layer_0",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_7",
          "type": "Linear",
          "name": "layer0_c_proj_mlp",
          "layer": "layer_0",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_8",
          "type": "LayerNorm",
          "name": "layer0_ln_1",
          "layer": "layer_0",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_9",
          "type": "LayerNorm",
          "name": "layer0_ln_2",
          "layer": "layer_0",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_10",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_11",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_12",
          "type": "Dropout",
          "name": "layer0_attn_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_13",
          "type": "Dropout",
          "name": "layer0_resid_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_14",
          "type": "Linear",
          "name": "layer1_c_attn",
          "layer": "layer_1",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_15",
          "type": "Linear",
          "name": "layer1_c_proj",
          "layer": "layer_1",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_16",
          "type": "Linear",
          "name": "layer1_c_fc",
          "layer": "layer_1",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_17",
          "type": "Linear",
          "name": "layer1_c_proj_mlp",
          "layer": "layer_1",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_18",
          "type": "LayerNorm",
          "name": "layer1_ln_1",
          "layer": "layer_1",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_19",
          "type": "LayerNorm",
          "name": "layer1_ln_2",
          "layer": "layer_1",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_20",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_21",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_22",
          "type": "Dropout",
          "name": "layer1_attn_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_23",
          "type": "Dropout",
          "name": "layer1_resid_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_24",
          "type": "Linear",
          "name": "layer2_c_attn",
          "layer": "layer_2",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_25",
          "type": "Linear",
          "name": "layer2_c_proj",
          "layer": "layer_2",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_26",
          "type": "Linear",
          "name": "layer2_c_fc",
          "layer": "layer_2",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_27",
          "type": "Linear",
          "name": "layer2_c_proj_mlp",
          "layer": "layer_2",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_28",
          "type": "LayerNorm",
          "name": "layer2_ln_1",
          "layer": "layer_2",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_29",
          "type": "LayerNorm",
          "name": "layer2_ln_2",
          "layer": "layer_2",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_30",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_31",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_32",
          "type": "Dropout",
          "name": "layer2_attn_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_33",
          "type": "Dropout",
          "name": "layer2_resid_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_34",
          "type": "Linear",
          "name": "layer3_c_attn",
          "layer": "layer_3",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_35",
          "type": "Linear",
          "name": "layer3_c_proj",
          "layer": "layer_3",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_36",
          "type": "Linear",
          "name": "layer3_c_fc",
          "layer": "layer_3",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_37",
          "type": "Linear",
          "name": "layer3_c_proj_mlp",
          "layer": "layer_3",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_38",
          "type": "LayerNorm",
          "name": "layer3_ln_1",
          "layer": "layer_3",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_39",
          "type": "LayerNorm",
          "name": "layer3_ln_2",
          "layer": "layer_3",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_40",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_41",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_42",
          "type": "Dropout",
          "name": "layer3_attn_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_43",
          "type": "Dropout",
          "name": "layer3_resid_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_44",
          "type": "Linear",
          "name": "layer4_c_attn",
          "layer": "layer_4",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_45",
          "type": "Linear",
          "name": "layer4_c_proj",
          "layer": "layer_4",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_46",
          "type": "Linear",
          "name": "layer4_c_fc",
          "layer": "layer_4",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_47",
          "type": "Linear",
          "name": "layer4_c_proj_mlp",
          "layer": "layer_4",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_48",
          "type": "LayerNorm",
          "name": "layer4_ln_1",
          "layer": "layer_4",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_49",
          "type": "LayerNorm",
          "name": "layer4_ln_2",
          "layer": "layer_4",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_50",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_51",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_52",
          "type": "Dropout",
          "name": "layer4_attn_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_53",
          "type": "Dropout",
          "name": "layer4_resid_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_54",
          "type": "Linear",
          "name": "layer5_c_attn",
          "layer": "layer_5",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_55",
          "type": "Linear",
          "name": "layer5_c_proj",
          "layer": "layer_5",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_56",
          "type": "Linear",
          "name": "layer5_c_fc",
          "layer": "layer_5",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_57",
          "type": "Linear",
          "name": "layer5_c_proj_mlp",
          "layer": "layer_5",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_58",
          "type": "LayerNorm",
          "name": "layer5_ln_1",
          "layer": "layer_5",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_59",
          "type": "LayerNorm",
          "name": "layer5_ln_2",
          "layer": "layer_5",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_60",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_61",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_62",
          "type": "Dropout",
          "name": "layer5_attn_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_63",
          "type": "Dropout",
          "name": "layer5_resid_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_64",
          "type": "Linear",
          "name": "layer6_c_attn",
          "layer": "layer_6",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_65",
          "type": "Linear",
          "name": "layer6_c_proj",
          "layer": "layer_6",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_66",
          "type": "Linear",
          "name": "layer6_c_fc",
          "layer": "layer_6",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_67",
          "type": "Linear",
          "name": "layer6_c_proj_mlp",
          "layer": "layer_6",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_68",
          "type": "LayerNorm",
          "name": "layer6_ln_1",
          "layer": "layer_6",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_69",
          "type": "LayerNorm",
          "name": "layer6_ln_2",
          "layer": "layer_6",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_70",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_71",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_72",
          "type": "Dropout",
          "name": "layer6_attn_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_73",
          "type": "Dropout",
          "name": "layer6_resid_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_74",
          "type": "Linear",
          "name": "layer7_c_attn",
          "layer": "layer_7",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_75",
          "type": "Linear",
          "name": "layer7_c_proj",
          "layer": "layer_7",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_76",
          "type": "Linear",
          "name": "layer7_c_fc",
          "layer": "layer_7",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_77",
          "type": "Linear",
          "name": "layer7_c_proj_mlp",
          "layer": "layer_7",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_78",
          "type": "LayerNorm",
          "name": "layer7_ln_1",
          "layer": "layer_7",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_79",
          "type": "LayerNorm",
          "name": "layer7_ln_2",
          "layer": "layer_7",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_80",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_81",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_82",
          "type": "Dropout",
          "name": "layer7_attn_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_83",
          "type": "Dropout",
          "name": "layer7_resid_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_84",
          "type": "Linear",
          "name": "layer8_c_attn",
          "layer": "layer_8",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_85",
          "type": "Linear",
          "name": "layer8_c_proj",
          "layer": "layer_8",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_86",
          "type": "Linear",
          "name": "layer8_c_fc",
          "layer": "layer_8",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_87",
          "type": "Linear",
          "name": "layer8_c_proj_mlp",
          "layer": "layer_8",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_88",
          "type": "LayerNorm",
          "name": "layer8_ln_1",
          "layer": "layer_8",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_89",
          "type": "LayerNorm",
          "name": "layer8_ln_2",
          "layer": "layer_8",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_90",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_91",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_92",
          "type": "Dropout",
          "name": "layer8_attn_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_93",
          "type": "Dropout",
          "name": "layer8_resid_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_94",
          "type": "Linear",
          "name": "layer9_c_attn",
          "layer": "layer_9",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_95",
          "type": "Linear",
          "name": "layer9_c_proj",
          "layer": "layer_9",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_96",
          "type": "Linear",
          "name": "layer9_c_fc",
          "layer": "layer_9",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_97",
          "type": "Linear",
          "name": "layer9_c_proj_mlp",
          "layer": "layer_9",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_98",
          "type": "LayerNorm",
          "name": "layer9_ln_1",
          "layer": "layer_9",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_99",
          "type": "LayerNorm",
          "name": "layer9_ln_2",
          "layer": "layer_9",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_100",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_101",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_102",
          "type": "Dropout",
          "name": "layer9_attn_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_103",
          "type": "Dropout",
          "name": "layer9_resid_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_104",
          "type": "Linear",
          "name": "layer10_c_attn",
          "layer": "layer_10",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_105",
          "type": "Linear",
          "name": "layer10_c_proj",
          "layer": "layer_10",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_106",
          "type": "Linear",
          "name": "layer10_c_fc",
          "layer": "layer_10",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_107",
          "type": "Linear",
          "name": "layer10_c_proj_mlp",
          "layer": "layer_10",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_108",
          "type": "LayerNorm",
          "name": "layer10_ln_1",
          "layer": "layer_10",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_109",
          "type": "LayerNorm",
          "name": "layer10_ln_2",
          "layer": "layer_10",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_110",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_111",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_112",
          "type": "Dropout",
          "name": "layer10_attn_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_113",
          "type": "Dropout",
          "name": "layer10_resid_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_114",
          "type": "Linear",
          "name": "layer11_c_attn",
          "layer": "layer_11",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_115",
          "type": "Linear",
          "name": "layer11_c_proj",
          "layer": "layer_11",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_116",
          "type": "Linear",
          "name": "layer11_c_fc",
          "layer": "layer_11",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_117",
          "type": "Linear",
          "name": "layer11_c_proj_mlp",
          "layer": "layer_11",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_118",
          "type": "LayerNorm",
          "name": "layer11_ln_1",
          "layer": "layer_11",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_119",
          "type": "LayerNorm",
          "name": "layer11_ln_2",
          "layer": "layer_11",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_120",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_121",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_122",
          "type": "Dropout",
          "name": "layer11_attn_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_123",
          "type": "Dropout",
          "name": "layer11_resid_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_124",
          "type": "Linear",
          "name": "layer12_c_attn",
          "layer": "layer_12",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_125",
          "type": "Linear",
          "name": "layer12_c_proj",
          "layer": "layer_12",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_126",
          "type": "Linear",
          "name": "layer12_c_fc",
          "layer": "layer_12",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_127",
          "type": "Linear",
          "name": "layer12_c_proj_mlp",
          "layer": "layer_12",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_128",
          "type": "LayerNorm",
          "name": "layer12_ln_1",
          "layer": "layer_12",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_129",
          "type": "LayerNorm",
          "name": "layer12_ln_2",
          "layer": "layer_12",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_130",
          "type": "Softmax",
          "name": "layer12_attention_softmax",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_131",
          "type": "GELU",
          "name": "layer12_activation",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_132",
          "type": "Dropout",
          "name": "layer12_attn_dropout",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_133",
          "type": "Dropout",
          "name": "layer12_resid_dropout",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_134",
          "type": "Linear",
          "name": "layer13_c_attn",
          "layer": "layer_13",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_135",
          "type": "Linear",
          "name": "layer13_c_proj",
          "layer": "layer_13",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_136",
          "type": "Linear",
          "name": "layer13_c_fc",
          "layer": "layer_13",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_137",
          "type": "Linear",
          "name": "layer13_c_proj_mlp",
          "layer": "layer_13",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_138",
          "type": "LayerNorm",
          "name": "layer13_ln_1",
          "layer": "layer_13",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_139",
          "type": "LayerNorm",
          "name": "layer13_ln_2",
          "layer": "layer_13",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_140",
          "type": "Softmax",
          "name": "layer13_attention_softmax",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_141",
          "type": "GELU",
          "name": "layer13_activation",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_142",
          "type": "Dropout",
          "name": "layer13_attn_dropout",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_143",
          "type": "Dropout",
          "name": "layer13_resid_dropout",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_144",
          "type": "Linear",
          "name": "layer14_c_attn",
          "layer": "layer_14",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_145",
          "type": "Linear",
          "name": "layer14_c_proj",
          "layer": "layer_14",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_146",
          "type": "Linear",
          "name": "layer14_c_fc",
          "layer": "layer_14",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_147",
          "type": "Linear",
          "name": "layer14_c_proj_mlp",
          "layer": "layer_14",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_148",
          "type": "LayerNorm",
          "name": "layer14_ln_1",
          "layer": "layer_14",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_149",
          "type": "LayerNorm",
          "name": "layer14_ln_2",
          "layer": "layer_14",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_150",
          "type": "Softmax",
          "name": "layer14_attention_softmax",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_151",
          "type": "GELU",
          "name": "layer14_activation",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_152",
          "type": "Dropout",
          "name": "layer14_attn_dropout",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_153",
          "type": "Dropout",
          "name": "layer14_resid_dropout",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_154",
          "type": "Linear",
          "name": "layer15_c_attn",
          "layer": "layer_15",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_155",
          "type": "Linear",
          "name": "layer15_c_proj",
          "layer": "layer_15",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_156",
          "type": "Linear",
          "name": "layer15_c_fc",
          "layer": "layer_15",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_157",
          "type": "Linear",
          "name": "layer15_c_proj_mlp",
          "layer": "layer_15",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_158",
          "type": "LayerNorm",
          "name": "layer15_ln_1",
          "layer": "layer_15",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_159",
          "type": "LayerNorm",
          "name": "layer15_ln_2",
          "layer": "layer_15",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_160",
          "type": "Softmax",
          "name": "layer15_attention_softmax",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_161",
          "type": "GELU",
          "name": "layer15_activation",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_162",
          "type": "Dropout",
          "name": "layer15_attn_dropout",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_163",
          "type": "Dropout",
          "name": "layer15_resid_dropout",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_164",
          "type": "Linear",
          "name": "layer16_c_attn",
          "layer": "layer_16",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_165",
          "type": "Linear",
          "name": "layer16_c_proj",
          "layer": "layer_16",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_166",
          "type": "Linear",
          "name": "layer16_c_fc",
          "layer": "layer_16",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_167",
          "type": "Linear",
          "name": "layer16_c_proj_mlp",
          "layer": "layer_16",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_168",
          "type": "LayerNorm",
          "name": "layer16_ln_1",
          "layer": "layer_16",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_169",
          "type": "LayerNorm",
          "name": "layer16_ln_2",
          "layer": "layer_16",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_170",
          "type": "Softmax",
          "name": "layer16_attention_softmax",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_171",
          "type": "GELU",
          "name": "layer16_activation",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_172",
          "type": "Dropout",
          "name": "layer16_attn_dropout",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_173",
          "type": "Dropout",
          "name": "layer16_resid_dropout",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_174",
          "type": "Linear",
          "name": "layer17_c_attn",
          "layer": "layer_17",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_175",
          "type": "Linear",
          "name": "layer17_c_proj",
          "layer": "layer_17",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_176",
          "type": "Linear",
          "name": "layer17_c_fc",
          "layer": "layer_17",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_177",
          "type": "Linear",
          "name": "layer17_c_proj_mlp",
          "layer": "layer_17",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_178",
          "type": "LayerNorm",
          "name": "layer17_ln_1",
          "layer": "layer_17",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_179",
          "type": "LayerNorm",
          "name": "layer17_ln_2",
          "layer": "layer_17",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_180",
          "type": "Softmax",
          "name": "layer17_attention_softmax",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_181",
          "type": "GELU",
          "name": "layer17_activation",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_182",
          "type": "Dropout",
          "name": "layer17_attn_dropout",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_183",
          "type": "Dropout",
          "name": "layer17_resid_dropout",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_184",
          "type": "Linear",
          "name": "layer18_c_attn",
          "layer": "layer_18",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_185",
          "type": "Linear",
          "name": "layer18_c_proj",
          "layer": "layer_18",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_186",
          "type": "Linear",
          "name": "layer18_c_fc",
          "layer": "layer_18",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_187",
          "type": "Linear",
          "name": "layer18_c_proj_mlp",
          "layer": "layer_18",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_188",
          "type": "LayerNorm",
          "name": "layer18_ln_1",
          "layer": "layer_18",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_189",
          "type": "LayerNorm",
          "name": "layer18_ln_2",
          "layer": "layer_18",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_190",
          "type": "Softmax",
          "name": "layer18_attention_softmax",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_191",
          "type": "GELU",
          "name": "layer18_activation",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_192",
          "type": "Dropout",
          "name": "layer18_attn_dropout",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_193",
          "type": "Dropout",
          "name": "layer18_resid_dropout",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_194",
          "type": "Linear",
          "name": "layer19_c_attn",
          "layer": "layer_19",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_195",
          "type": "Linear",
          "name": "layer19_c_proj",
          "layer": "layer_19",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_196",
          "type": "Linear",
          "name": "layer19_c_fc",
          "layer": "layer_19",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_197",
          "type": "Linear",
          "name": "layer19_c_proj_mlp",
          "layer": "layer_19",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_198",
          "type": "LayerNorm",
          "name": "layer19_ln_1",
          "layer": "layer_19",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_199",
          "type": "LayerNorm",
          "name": "layer19_ln_2",
          "layer": "layer_19",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_200",
          "type": "Softmax",
          "name": "layer19_attention_softmax",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_201",
          "type": "GELU",
          "name": "layer19_activation",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_202",
          "type": "Dropout",
          "name": "layer19_attn_dropout",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_203",
          "type": "Dropout",
          "name": "layer19_resid_dropout",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_204",
          "type": "Linear",
          "name": "layer20_c_attn",
          "layer": "layer_20",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_205",
          "type": "Linear",
          "name": "layer20_c_proj",
          "layer": "layer_20",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_206",
          "type": "Linear",
          "name": "layer20_c_fc",
          "layer": "layer_20",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_207",
          "type": "Linear",
          "name": "layer20_c_proj_mlp",
          "layer": "layer_20",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_208",
          "type": "LayerNorm",
          "name": "layer20_ln_1",
          "layer": "layer_20",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_209",
          "type": "LayerNorm",
          "name": "layer20_ln_2",
          "layer": "layer_20",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_210",
          "type": "Softmax",
          "name": "layer20_attention_softmax",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_211",
          "type": "GELU",
          "name": "layer20_activation",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_212",
          "type": "Dropout",
          "name": "layer20_attn_dropout",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_213",
          "type": "Dropout",
          "name": "layer20_resid_dropout",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_214",
          "type": "Linear",
          "name": "layer21_c_attn",
          "layer": "layer_21",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_215",
          "type": "Linear",
          "name": "layer21_c_proj",
          "layer": "layer_21",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_216",
          "type": "Linear",
          "name": "layer21_c_fc",
          "layer": "layer_21",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_217",
          "type": "Linear",
          "name": "layer21_c_proj_mlp",
          "layer": "layer_21",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_218",
          "type": "LayerNorm",
          "name": "layer21_ln_1",
          "layer": "layer_21",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_219",
          "type": "LayerNorm",
          "name": "layer21_ln_2",
          "layer": "layer_21",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_220",
          "type": "Softmax",
          "name": "layer21_attention_softmax",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_221",
          "type": "GELU",
          "name": "layer21_activation",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_222",
          "type": "Dropout",
          "name": "layer21_attn_dropout",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_223",
          "type": "Dropout",
          "name": "layer21_resid_dropout",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_224",
          "type": "Linear",
          "name": "layer22_c_attn",
          "layer": "layer_22",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_225",
          "type": "Linear",
          "name": "layer22_c_proj",
          "layer": "layer_22",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_226",
          "type": "Linear",
          "name": "layer22_c_fc",
          "layer": "layer_22",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_227",
          "type": "Linear",
          "name": "layer22_c_proj_mlp",
          "layer": "layer_22",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_228",
          "type": "LayerNorm",
          "name": "layer22_ln_1",
          "layer": "layer_22",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_229",
          "type": "LayerNorm",
          "name": "layer22_ln_2",
          "layer": "layer_22",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_230",
          "type": "Softmax",
          "name": "layer22_attention_softmax",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_231",
          "type": "GELU",
          "name": "layer22_activation",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_232",
          "type": "Dropout",
          "name": "layer22_attn_dropout",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_233",
          "type": "Dropout",
          "name": "layer22_resid_dropout",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_234",
          "type": "Linear",
          "name": "layer23_c_attn",
          "layer": "layer_23",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_235",
          "type": "Linear",
          "name": "layer23_c_proj",
          "layer": "layer_23",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_236",
          "type": "Linear",
          "name": "layer23_c_fc",
          "layer": "layer_23",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_237",
          "type": "Linear",
          "name": "layer23_c_proj_mlp",
          "layer": "layer_23",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Pythia_1.4B_op_238",
          "type": "LayerNorm",
          "name": "layer23_ln_1",
          "layer": "layer_23",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_239",
          "type": "LayerNorm",
          "name": "layer23_ln_2",
          "layer": "layer_23",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_240",
          "type": "Softmax",
          "name": "layer23_attention_softmax",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Pythia_1.4B_op_241",
          "type": "GELU",
          "name": "layer23_activation",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "Pythia_1.4B_op_242",
          "type": "Dropout",
          "name": "layer23_attn_dropout",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Pythia_1.4B_op_243",
          "type": "Dropout",
          "name": "layer23_resid_dropout",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        }
      ],
      "parameters": 1400000000
    },
    {
      "model_id": "ResNet_50",
      "name": "ResNet-50",
      "source": "huggingface/resnet-50",
      "category": "CV",
      "architecture": "resnet_style",
      "num_params": 25600000,
      "num_layers": 16,
      "hidden_size": 768,
      "vocab_size": 0,
      "intermediate_size": 3072,
      "operator_count": 118,
      "operators": [
        {
          "op_id": "ResNet_50_op_1",
          "type": "Conv2d",
          "name": "conv1",
          "layer": "global",
          "parameters": 9408,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_2",
          "type": "BatchNorm2d",
          "name": "bn1",
          "layer": "global",
          "parameters": 256,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, H, W]"
        },
        {
          "op_id": "ResNet_50_op_3",
          "type": "ReLU",
          "name": "relu",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, H, W]"
        },
        {
          "op_id": "ResNet_50_op_4",
          "type": "MaxPool2d",
          "name": "maxpool",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, H/2, W/2]"
        },
        {
          "op_id": "ResNet_50_op_5",
          "type": "AdaptiveAvgPool2d",
          "name": "avgpool",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, 1, 1]"
        },
        {
          "op_id": "ResNet_50_op_6",
          "type": "Linear",
          "name": "fc",
          "layer": "global",
          "parameters": 2048000,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ResNet_50_op_7",
          "type": "Conv2d",
          "name": "stage1_block1_conv1",
          "layer": "stage1_block1",
          "parameters": 8192,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_8",
          "type": "Conv2d",
          "name": "stage1_block1_conv2",
          "layer": "stage1_block1",
          "parameters": 147456,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_9",
          "type": "Conv2d",
          "name": "stage1_block1_conv3",
          "layer": "stage1_block1",
          "parameters": 65536,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_10",
          "type": "BatchNorm2d",
          "name": "stage1_block1_bn1",
          "layer": "stage1_block1",
          "parameters": 512,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "ResNet_50_op_11",
          "type": "BatchNorm2d",
          "name": "stage1_block1_bn2",
          "layer": "stage1_block1",
          "parameters": 512,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "ResNet_50_op_12",
          "type": "BatchNorm2d",
          "name": "stage1_block1_bn3",
          "layer": "stage1_block1",
          "parameters": 2048,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "ResNet_50_op_13",
          "type": "ReLU",
          "name": "stage1_block1_relu",
          "layer": "stage1_block1",
          "parameters": 0,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "ResNet_50_op_14",
          "type": "Conv2d",
          "name": "stage1_block2_conv1",
          "layer": "stage1_block2",
          "parameters": 8192,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_15",
          "type": "Conv2d",
          "name": "stage1_block2_conv2",
          "layer": "stage1_block2",
          "parameters": 147456,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_16",
          "type": "Conv2d",
          "name": "stage1_block2_conv3",
          "layer": "stage1_block2",
          "parameters": 65536,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_17",
          "type": "BatchNorm2d",
          "name": "stage1_block2_bn1",
          "layer": "stage1_block2",
          "parameters": 512,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "ResNet_50_op_18",
          "type": "BatchNorm2d",
          "name": "stage1_block2_bn2",
          "layer": "stage1_block2",
          "parameters": 512,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "ResNet_50_op_19",
          "type": "BatchNorm2d",
          "name": "stage1_block2_bn3",
          "layer": "stage1_block2",
          "parameters": 2048,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "ResNet_50_op_20",
          "type": "ReLU",
          "name": "stage1_block2_relu",
          "layer": "stage1_block2",
          "parameters": 0,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "ResNet_50_op_21",
          "type": "Conv2d",
          "name": "stage1_block3_conv1",
          "layer": "stage1_block3",
          "parameters": 8192,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_22",
          "type": "Conv2d",
          "name": "stage1_block3_conv2",
          "layer": "stage1_block3",
          "parameters": 147456,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_23",
          "type": "Conv2d",
          "name": "stage1_block3_conv3",
          "layer": "stage1_block3",
          "parameters": 65536,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_24",
          "type": "BatchNorm2d",
          "name": "stage1_block3_bn1",
          "layer": "stage1_block3",
          "parameters": 512,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "ResNet_50_op_25",
          "type": "BatchNorm2d",
          "name": "stage1_block3_bn2",
          "layer": "stage1_block3",
          "parameters": 512,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "ResNet_50_op_26",
          "type": "BatchNorm2d",
          "name": "stage1_block3_bn3",
          "layer": "stage1_block3",
          "parameters": 2048,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "ResNet_50_op_27",
          "type": "ReLU",
          "name": "stage1_block3_relu",
          "layer": "stage1_block3",
          "parameters": 0,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "ResNet_50_op_28",
          "type": "Conv2d",
          "name": "stage2_block1_conv1",
          "layer": "stage2_block1",
          "parameters": 32768,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_29",
          "type": "Conv2d",
          "name": "stage2_block1_conv2",
          "layer": "stage2_block1",
          "parameters": 589824,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_30",
          "type": "Conv2d",
          "name": "stage2_block1_conv3",
          "layer": "stage2_block1",
          "parameters": 262144,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_31",
          "type": "BatchNorm2d",
          "name": "stage2_block1_bn1",
          "layer": "stage2_block1",
          "parameters": 1024,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ResNet_50_op_32",
          "type": "BatchNorm2d",
          "name": "stage2_block1_bn2",
          "layer": "stage2_block1",
          "parameters": 1024,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ResNet_50_op_33",
          "type": "BatchNorm2d",
          "name": "stage2_block1_bn3",
          "layer": "stage2_block1",
          "parameters": 4096,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ResNet_50_op_34",
          "type": "ReLU",
          "name": "stage2_block1_relu",
          "layer": "stage2_block1",
          "parameters": 0,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ResNet_50_op_35",
          "type": "Conv2d",
          "name": "stage2_block2_conv1",
          "layer": "stage2_block2",
          "parameters": 32768,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_36",
          "type": "Conv2d",
          "name": "stage2_block2_conv2",
          "layer": "stage2_block2",
          "parameters": 589824,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_37",
          "type": "Conv2d",
          "name": "stage2_block2_conv3",
          "layer": "stage2_block2",
          "parameters": 262144,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_38",
          "type": "BatchNorm2d",
          "name": "stage2_block2_bn1",
          "layer": "stage2_block2",
          "parameters": 1024,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ResNet_50_op_39",
          "type": "BatchNorm2d",
          "name": "stage2_block2_bn2",
          "layer": "stage2_block2",
          "parameters": 1024,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ResNet_50_op_40",
          "type": "BatchNorm2d",
          "name": "stage2_block2_bn3",
          "layer": "stage2_block2",
          "parameters": 4096,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ResNet_50_op_41",
          "type": "ReLU",
          "name": "stage2_block2_relu",
          "layer": "stage2_block2",
          "parameters": 0,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ResNet_50_op_42",
          "type": "Conv2d",
          "name": "stage2_block3_conv1",
          "layer": "stage2_block3",
          "parameters": 32768,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_43",
          "type": "Conv2d",
          "name": "stage2_block3_conv2",
          "layer": "stage2_block3",
          "parameters": 589824,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_44",
          "type": "Conv2d",
          "name": "stage2_block3_conv3",
          "layer": "stage2_block3",
          "parameters": 262144,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_45",
          "type": "BatchNorm2d",
          "name": "stage2_block3_bn1",
          "layer": "stage2_block3",
          "parameters": 1024,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ResNet_50_op_46",
          "type": "BatchNorm2d",
          "name": "stage2_block3_bn2",
          "layer": "stage2_block3",
          "parameters": 1024,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ResNet_50_op_47",
          "type": "BatchNorm2d",
          "name": "stage2_block3_bn3",
          "layer": "stage2_block3",
          "parameters": 4096,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ResNet_50_op_48",
          "type": "ReLU",
          "name": "stage2_block3_relu",
          "layer": "stage2_block3",
          "parameters": 0,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ResNet_50_op_49",
          "type": "Conv2d",
          "name": "stage2_block4_conv1",
          "layer": "stage2_block4",
          "parameters": 32768,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_50",
          "type": "Conv2d",
          "name": "stage2_block4_conv2",
          "layer": "stage2_block4",
          "parameters": 589824,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_51",
          "type": "Conv2d",
          "name": "stage2_block4_conv3",
          "layer": "stage2_block4",
          "parameters": 262144,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_52",
          "type": "BatchNorm2d",
          "name": "stage2_block4_bn1",
          "layer": "stage2_block4",
          "parameters": 1024,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ResNet_50_op_53",
          "type": "BatchNorm2d",
          "name": "stage2_block4_bn2",
          "layer": "stage2_block4",
          "parameters": 1024,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ResNet_50_op_54",
          "type": "BatchNorm2d",
          "name": "stage2_block4_bn3",
          "layer": "stage2_block4",
          "parameters": 4096,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ResNet_50_op_55",
          "type": "ReLU",
          "name": "stage2_block4_relu",
          "layer": "stage2_block4",
          "parameters": 0,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ResNet_50_op_56",
          "type": "Conv2d",
          "name": "stage3_block1_conv1",
          "layer": "stage3_block1",
          "parameters": 131072,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_57",
          "type": "Conv2d",
          "name": "stage3_block1_conv2",
          "layer": "stage3_block1",
          "parameters": 2359296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_58",
          "type": "Conv2d",
          "name": "stage3_block1_conv3",
          "layer": "stage3_block1",
          "parameters": 1048576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_59",
          "type": "BatchNorm2d",
          "name": "stage3_block1_bn1",
          "layer": "stage3_block1",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_60",
          "type": "BatchNorm2d",
          "name": "stage3_block1_bn2",
          "layer": "stage3_block1",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_61",
          "type": "BatchNorm2d",
          "name": "stage3_block1_bn3",
          "layer": "stage3_block1",
          "parameters": 8192,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_62",
          "type": "ReLU",
          "name": "stage3_block1_relu",
          "layer": "stage3_block1",
          "parameters": 0,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_63",
          "type": "Conv2d",
          "name": "stage3_block2_conv1",
          "layer": "stage3_block2",
          "parameters": 131072,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_64",
          "type": "Conv2d",
          "name": "stage3_block2_conv2",
          "layer": "stage3_block2",
          "parameters": 2359296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_65",
          "type": "Conv2d",
          "name": "stage3_block2_conv3",
          "layer": "stage3_block2",
          "parameters": 1048576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_66",
          "type": "BatchNorm2d",
          "name": "stage3_block2_bn1",
          "layer": "stage3_block2",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_67",
          "type": "BatchNorm2d",
          "name": "stage3_block2_bn2",
          "layer": "stage3_block2",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_68",
          "type": "BatchNorm2d",
          "name": "stage3_block2_bn3",
          "layer": "stage3_block2",
          "parameters": 8192,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_69",
          "type": "ReLU",
          "name": "stage3_block2_relu",
          "layer": "stage3_block2",
          "parameters": 0,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_70",
          "type": "Conv2d",
          "name": "stage3_block3_conv1",
          "layer": "stage3_block3",
          "parameters": 131072,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_71",
          "type": "Conv2d",
          "name": "stage3_block3_conv2",
          "layer": "stage3_block3",
          "parameters": 2359296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_72",
          "type": "Conv2d",
          "name": "stage3_block3_conv3",
          "layer": "stage3_block3",
          "parameters": 1048576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_73",
          "type": "BatchNorm2d",
          "name": "stage3_block3_bn1",
          "layer": "stage3_block3",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_74",
          "type": "BatchNorm2d",
          "name": "stage3_block3_bn2",
          "layer": "stage3_block3",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_75",
          "type": "BatchNorm2d",
          "name": "stage3_block3_bn3",
          "layer": "stage3_block3",
          "parameters": 8192,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_76",
          "type": "ReLU",
          "name": "stage3_block3_relu",
          "layer": "stage3_block3",
          "parameters": 0,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_77",
          "type": "Conv2d",
          "name": "stage3_block4_conv1",
          "layer": "stage3_block4",
          "parameters": 131072,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_78",
          "type": "Conv2d",
          "name": "stage3_block4_conv2",
          "layer": "stage3_block4",
          "parameters": 2359296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_79",
          "type": "Conv2d",
          "name": "stage3_block4_conv3",
          "layer": "stage3_block4",
          "parameters": 1048576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_80",
          "type": "BatchNorm2d",
          "name": "stage3_block4_bn1",
          "layer": "stage3_block4",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_81",
          "type": "BatchNorm2d",
          "name": "stage3_block4_bn2",
          "layer": "stage3_block4",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_82",
          "type": "BatchNorm2d",
          "name": "stage3_block4_bn3",
          "layer": "stage3_block4",
          "parameters": 8192,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_83",
          "type": "ReLU",
          "name": "stage3_block4_relu",
          "layer": "stage3_block4",
          "parameters": 0,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_84",
          "type": "Conv2d",
          "name": "stage3_block5_conv1",
          "layer": "stage3_block5",
          "parameters": 131072,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_85",
          "type": "Conv2d",
          "name": "stage3_block5_conv2",
          "layer": "stage3_block5",
          "parameters": 2359296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_86",
          "type": "Conv2d",
          "name": "stage3_block5_conv3",
          "layer": "stage3_block5",
          "parameters": 1048576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_87",
          "type": "BatchNorm2d",
          "name": "stage3_block5_bn1",
          "layer": "stage3_block5",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_88",
          "type": "BatchNorm2d",
          "name": "stage3_block5_bn2",
          "layer": "stage3_block5",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_89",
          "type": "BatchNorm2d",
          "name": "stage3_block5_bn3",
          "layer": "stage3_block5",
          "parameters": 8192,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_90",
          "type": "ReLU",
          "name": "stage3_block5_relu",
          "layer": "stage3_block5",
          "parameters": 0,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_91",
          "type": "Conv2d",
          "name": "stage3_block6_conv1",
          "layer": "stage3_block6",
          "parameters": 131072,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_92",
          "type": "Conv2d",
          "name": "stage3_block6_conv2",
          "layer": "stage3_block6",
          "parameters": 2359296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_93",
          "type": "Conv2d",
          "name": "stage3_block6_conv3",
          "layer": "stage3_block6",
          "parameters": 1048576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_94",
          "type": "BatchNorm2d",
          "name": "stage3_block6_bn1",
          "layer": "stage3_block6",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_95",
          "type": "BatchNorm2d",
          "name": "stage3_block6_bn2",
          "layer": "stage3_block6",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_96",
          "type": "BatchNorm2d",
          "name": "stage3_block6_bn3",
          "layer": "stage3_block6",
          "parameters": 8192,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_97",
          "type": "ReLU",
          "name": "stage3_block6_relu",
          "layer": "stage3_block6",
          "parameters": 0,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ResNet_50_op_98",
          "type": "Conv2d",
          "name": "stage4_block1_conv1",
          "layer": "stage4_block1",
          "parameters": 262144,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_99",
          "type": "Conv2d",
          "name": "stage4_block1_conv2",
          "layer": "stage4_block1",
          "parameters": 2359296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_100",
          "type": "Conv2d",
          "name": "stage4_block1_conv3",
          "layer": "stage4_block1",
          "parameters": 1048576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_101",
          "type": "BatchNorm2d",
          "name": "stage4_block1_bn1",
          "layer": "stage4_block1",
          "parameters": 2048,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ResNet_50_op_102",
          "type": "BatchNorm2d",
          "name": "stage4_block1_bn2",
          "layer": "stage4_block1",
          "parameters": 2048,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ResNet_50_op_103",
          "type": "BatchNorm2d",
          "name": "stage4_block1_bn3",
          "layer": "stage4_block1",
          "parameters": 8192,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ResNet_50_op_104",
          "type": "ReLU",
          "name": "stage4_block1_relu",
          "layer": "stage4_block1",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ResNet_50_op_105",
          "type": "Conv2d",
          "name": "stage4_block2_conv1",
          "layer": "stage4_block2",
          "parameters": 262144,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_106",
          "type": "Conv2d",
          "name": "stage4_block2_conv2",
          "layer": "stage4_block2",
          "parameters": 2359296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_107",
          "type": "Conv2d",
          "name": "stage4_block2_conv3",
          "layer": "stage4_block2",
          "parameters": 1048576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_108",
          "type": "BatchNorm2d",
          "name": "stage4_block2_bn1",
          "layer": "stage4_block2",
          "parameters": 2048,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ResNet_50_op_109",
          "type": "BatchNorm2d",
          "name": "stage4_block2_bn2",
          "layer": "stage4_block2",
          "parameters": 2048,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ResNet_50_op_110",
          "type": "BatchNorm2d",
          "name": "stage4_block2_bn3",
          "layer": "stage4_block2",
          "parameters": 8192,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ResNet_50_op_111",
          "type": "ReLU",
          "name": "stage4_block2_relu",
          "layer": "stage4_block2",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ResNet_50_op_112",
          "type": "Conv2d",
          "name": "stage4_block3_conv1",
          "layer": "stage4_block3",
          "parameters": 262144,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_113",
          "type": "Conv2d",
          "name": "stage4_block3_conv2",
          "layer": "stage4_block3",
          "parameters": 2359296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_114",
          "type": "Conv2d",
          "name": "stage4_block3_conv3",
          "layer": "stage4_block3",
          "parameters": 1048576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ResNet_50_op_115",
          "type": "BatchNorm2d",
          "name": "stage4_block3_bn1",
          "layer": "stage4_block3",
          "parameters": 2048,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ResNet_50_op_116",
          "type": "BatchNorm2d",
          "name": "stage4_block3_bn2",
          "layer": "stage4_block3",
          "parameters": 2048,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ResNet_50_op_117",
          "type": "BatchNorm2d",
          "name": "stage4_block3_bn3",
          "layer": "stage4_block3",
          "parameters": 8192,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ResNet_50_op_118",
          "type": "ReLU",
          "name": "stage4_block3_relu",
          "layer": "stage4_block3",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        }
      ],
      "parameters": 25600000
    },
    {
      "model_id": "ViT_Base",
      "name": "ViT-Base",
      "source": "huggingface/vit-base",
      "category": "CV",
      "architecture": "vit_style",
      "num_params": 86000000,
      "num_layers": 12,
      "hidden_size": 768,
      "vocab_size": 0,
      "intermediate_size": 3072,
      "operator_count": 123,
      "operators": [
        {
          "op_id": "ViT_Base_op_1",
          "type": "Conv2d",
          "name": "patch_embed",
          "layer": "global",
          "parameters": 2073600000000,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ViT_Base_op_2",
          "type": "LayerNorm",
          "name": "norm",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_3",
          "type": "Linear",
          "name": "head",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_4",
          "type": "Linear",
          "name": "layer0_qkv",
          "layer": "layer_0",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_5",
          "type": "Linear",
          "name": "layer0_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_6",
          "type": "Linear",
          "name": "layer0_fc1",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_7",
          "type": "Linear",
          "name": "layer0_fc2",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_8",
          "type": "LayerNorm",
          "name": "layer0_norm1",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_9",
          "type": "LayerNorm",
          "name": "layer0_norm2",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_10",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ViT_Base_op_11",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ViT_Base_op_12",
          "type": "Dropout",
          "name": "layer0_attn_drop",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_13",
          "type": "Dropout",
          "name": "layer0_proj_drop",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_14",
          "type": "Linear",
          "name": "layer1_qkv",
          "layer": "layer_1",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_15",
          "type": "Linear",
          "name": "layer1_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_16",
          "type": "Linear",
          "name": "layer1_fc1",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_17",
          "type": "Linear",
          "name": "layer1_fc2",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_18",
          "type": "LayerNorm",
          "name": "layer1_norm1",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_19",
          "type": "LayerNorm",
          "name": "layer1_norm2",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_20",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ViT_Base_op_21",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ViT_Base_op_22",
          "type": "Dropout",
          "name": "layer1_attn_drop",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_23",
          "type": "Dropout",
          "name": "layer1_proj_drop",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_24",
          "type": "Linear",
          "name": "layer2_qkv",
          "layer": "layer_2",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_25",
          "type": "Linear",
          "name": "layer2_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_26",
          "type": "Linear",
          "name": "layer2_fc1",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_27",
          "type": "Linear",
          "name": "layer2_fc2",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_28",
          "type": "LayerNorm",
          "name": "layer2_norm1",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_29",
          "type": "LayerNorm",
          "name": "layer2_norm2",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_30",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ViT_Base_op_31",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ViT_Base_op_32",
          "type": "Dropout",
          "name": "layer2_attn_drop",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_33",
          "type": "Dropout",
          "name": "layer2_proj_drop",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_34",
          "type": "Linear",
          "name": "layer3_qkv",
          "layer": "layer_3",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_35",
          "type": "Linear",
          "name": "layer3_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_36",
          "type": "Linear",
          "name": "layer3_fc1",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_37",
          "type": "Linear",
          "name": "layer3_fc2",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_38",
          "type": "LayerNorm",
          "name": "layer3_norm1",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_39",
          "type": "LayerNorm",
          "name": "layer3_norm2",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_40",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ViT_Base_op_41",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ViT_Base_op_42",
          "type": "Dropout",
          "name": "layer3_attn_drop",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_43",
          "type": "Dropout",
          "name": "layer3_proj_drop",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_44",
          "type": "Linear",
          "name": "layer4_qkv",
          "layer": "layer_4",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_45",
          "type": "Linear",
          "name": "layer4_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_46",
          "type": "Linear",
          "name": "layer4_fc1",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_47",
          "type": "Linear",
          "name": "layer4_fc2",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_48",
          "type": "LayerNorm",
          "name": "layer4_norm1",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_49",
          "type": "LayerNorm",
          "name": "layer4_norm2",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_50",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ViT_Base_op_51",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ViT_Base_op_52",
          "type": "Dropout",
          "name": "layer4_attn_drop",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_53",
          "type": "Dropout",
          "name": "layer4_proj_drop",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_54",
          "type": "Linear",
          "name": "layer5_qkv",
          "layer": "layer_5",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_55",
          "type": "Linear",
          "name": "layer5_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_56",
          "type": "Linear",
          "name": "layer5_fc1",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_57",
          "type": "Linear",
          "name": "layer5_fc2",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_58",
          "type": "LayerNorm",
          "name": "layer5_norm1",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_59",
          "type": "LayerNorm",
          "name": "layer5_norm2",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_60",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ViT_Base_op_61",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ViT_Base_op_62",
          "type": "Dropout",
          "name": "layer5_attn_drop",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_63",
          "type": "Dropout",
          "name": "layer5_proj_drop",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_64",
          "type": "Linear",
          "name": "layer6_qkv",
          "layer": "layer_6",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_65",
          "type": "Linear",
          "name": "layer6_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_66",
          "type": "Linear",
          "name": "layer6_fc1",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_67",
          "type": "Linear",
          "name": "layer6_fc2",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_68",
          "type": "LayerNorm",
          "name": "layer6_norm1",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_69",
          "type": "LayerNorm",
          "name": "layer6_norm2",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_70",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ViT_Base_op_71",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ViT_Base_op_72",
          "type": "Dropout",
          "name": "layer6_attn_drop",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_73",
          "type": "Dropout",
          "name": "layer6_proj_drop",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_74",
          "type": "Linear",
          "name": "layer7_qkv",
          "layer": "layer_7",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_75",
          "type": "Linear",
          "name": "layer7_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_76",
          "type": "Linear",
          "name": "layer7_fc1",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_77",
          "type": "Linear",
          "name": "layer7_fc2",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_78",
          "type": "LayerNorm",
          "name": "layer7_norm1",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_79",
          "type": "LayerNorm",
          "name": "layer7_norm2",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_80",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ViT_Base_op_81",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ViT_Base_op_82",
          "type": "Dropout",
          "name": "layer7_attn_drop",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_83",
          "type": "Dropout",
          "name": "layer7_proj_drop",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_84",
          "type": "Linear",
          "name": "layer8_qkv",
          "layer": "layer_8",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_85",
          "type": "Linear",
          "name": "layer8_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_86",
          "type": "Linear",
          "name": "layer8_fc1",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_87",
          "type": "Linear",
          "name": "layer8_fc2",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_88",
          "type": "LayerNorm",
          "name": "layer8_norm1",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_89",
          "type": "LayerNorm",
          "name": "layer8_norm2",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_90",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ViT_Base_op_91",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ViT_Base_op_92",
          "type": "Dropout",
          "name": "layer8_attn_drop",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_93",
          "type": "Dropout",
          "name": "layer8_proj_drop",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_94",
          "type": "Linear",
          "name": "layer9_qkv",
          "layer": "layer_9",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_95",
          "type": "Linear",
          "name": "layer9_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_96",
          "type": "Linear",
          "name": "layer9_fc1",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_97",
          "type": "Linear",
          "name": "layer9_fc2",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_98",
          "type": "LayerNorm",
          "name": "layer9_norm1",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_99",
          "type": "LayerNorm",
          "name": "layer9_norm2",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_100",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ViT_Base_op_101",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ViT_Base_op_102",
          "type": "Dropout",
          "name": "layer9_attn_drop",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_103",
          "type": "Dropout",
          "name": "layer9_proj_drop",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_104",
          "type": "Linear",
          "name": "layer10_qkv",
          "layer": "layer_10",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_105",
          "type": "Linear",
          "name": "layer10_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_106",
          "type": "Linear",
          "name": "layer10_fc1",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_107",
          "type": "Linear",
          "name": "layer10_fc2",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_108",
          "type": "LayerNorm",
          "name": "layer10_norm1",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_109",
          "type": "LayerNorm",
          "name": "layer10_norm2",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_110",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ViT_Base_op_111",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ViT_Base_op_112",
          "type": "Dropout",
          "name": "layer10_attn_drop",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_113",
          "type": "Dropout",
          "name": "layer10_proj_drop",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_114",
          "type": "Linear",
          "name": "layer11_qkv",
          "layer": "layer_11",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_115",
          "type": "Linear",
          "name": "layer11_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_116",
          "type": "Linear",
          "name": "layer11_fc1",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_117",
          "type": "Linear",
          "name": "layer11_fc2",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ViT_Base_op_118",
          "type": "LayerNorm",
          "name": "layer11_norm1",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_119",
          "type": "LayerNorm",
          "name": "layer11_norm2",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_120",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ViT_Base_op_121",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ViT_Base_op_122",
          "type": "Dropout",
          "name": "layer11_attn_drop",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ViT_Base_op_123",
          "type": "Dropout",
          "name": "layer11_proj_drop",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        }
      ],
      "parameters": 86000000
    },
    {
      "model_id": "Swin_Base",
      "name": "Swin-Base",
      "source": "huggingface/swin-base",
      "category": "CV",
      "architecture": "swin_style",
      "num_params": 88000000,
      "num_layers": 24,
      "hidden_size": 1024,
      "vocab_size": 0,
      "intermediate_size": 4096,
      "operator_count": 243,
      "operators": [
        {
          "op_id": "Swin_Base_op_1",
          "type": "Conv2d",
          "name": "patch_embed",
          "layer": "global",
          "parameters": 4608,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "Swin_Base_op_2",
          "type": "LayerNorm",
          "name": "norm",
          "layer": "global",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_3",
          "type": "Linear",
          "name": "head",
          "layer": "global",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_4",
          "type": "Linear",
          "name": "layer0_qkv",
          "layer": "layer_0",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_5",
          "type": "Linear",
          "name": "layer0_proj",
          "layer": "layer_0",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_6",
          "type": "Linear",
          "name": "layer0_fc1",
          "layer": "layer_0",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_7",
          "type": "Linear",
          "name": "layer0_fc2",
          "layer": "layer_0",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_8",
          "type": "LayerNorm",
          "name": "layer0_norm1",
          "layer": "layer_0",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_9",
          "type": "LayerNorm",
          "name": "layer0_norm2",
          "layer": "layer_0",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_10",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_11",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_12",
          "type": "Dropout",
          "name": "layer0_attn_drop",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_13",
          "type": "Dropout",
          "name": "layer0_proj_drop",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_14",
          "type": "Linear",
          "name": "layer1_qkv",
          "layer": "layer_1",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_15",
          "type": "Linear",
          "name": "layer1_proj",
          "layer": "layer_1",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_16",
          "type": "Linear",
          "name": "layer1_fc1",
          "layer": "layer_1",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_17",
          "type": "Linear",
          "name": "layer1_fc2",
          "layer": "layer_1",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_18",
          "type": "LayerNorm",
          "name": "layer1_norm1",
          "layer": "layer_1",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_19",
          "type": "LayerNorm",
          "name": "layer1_norm2",
          "layer": "layer_1",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_20",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_21",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_22",
          "type": "Dropout",
          "name": "layer1_attn_drop",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_23",
          "type": "Dropout",
          "name": "layer1_proj_drop",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_24",
          "type": "Linear",
          "name": "layer2_qkv",
          "layer": "layer_2",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_25",
          "type": "Linear",
          "name": "layer2_proj",
          "layer": "layer_2",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_26",
          "type": "Linear",
          "name": "layer2_fc1",
          "layer": "layer_2",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_27",
          "type": "Linear",
          "name": "layer2_fc2",
          "layer": "layer_2",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_28",
          "type": "LayerNorm",
          "name": "layer2_norm1",
          "layer": "layer_2",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_29",
          "type": "LayerNorm",
          "name": "layer2_norm2",
          "layer": "layer_2",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_30",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_31",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_32",
          "type": "Dropout",
          "name": "layer2_attn_drop",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_33",
          "type": "Dropout",
          "name": "layer2_proj_drop",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_34",
          "type": "Linear",
          "name": "layer3_qkv",
          "layer": "layer_3",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_35",
          "type": "Linear",
          "name": "layer3_proj",
          "layer": "layer_3",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_36",
          "type": "Linear",
          "name": "layer3_fc1",
          "layer": "layer_3",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_37",
          "type": "Linear",
          "name": "layer3_fc2",
          "layer": "layer_3",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_38",
          "type": "LayerNorm",
          "name": "layer3_norm1",
          "layer": "layer_3",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_39",
          "type": "LayerNorm",
          "name": "layer3_norm2",
          "layer": "layer_3",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_40",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_41",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_42",
          "type": "Dropout",
          "name": "layer3_attn_drop",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_43",
          "type": "Dropout",
          "name": "layer3_proj_drop",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_44",
          "type": "Linear",
          "name": "layer4_qkv",
          "layer": "layer_4",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_45",
          "type": "Linear",
          "name": "layer4_proj",
          "layer": "layer_4",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_46",
          "type": "Linear",
          "name": "layer4_fc1",
          "layer": "layer_4",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_47",
          "type": "Linear",
          "name": "layer4_fc2",
          "layer": "layer_4",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_48",
          "type": "LayerNorm",
          "name": "layer4_norm1",
          "layer": "layer_4",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_49",
          "type": "LayerNorm",
          "name": "layer4_norm2",
          "layer": "layer_4",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_50",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_51",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_52",
          "type": "Dropout",
          "name": "layer4_attn_drop",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_53",
          "type": "Dropout",
          "name": "layer4_proj_drop",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_54",
          "type": "Linear",
          "name": "layer5_qkv",
          "layer": "layer_5",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_55",
          "type": "Linear",
          "name": "layer5_proj",
          "layer": "layer_5",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_56",
          "type": "Linear",
          "name": "layer5_fc1",
          "layer": "layer_5",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_57",
          "type": "Linear",
          "name": "layer5_fc2",
          "layer": "layer_5",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_58",
          "type": "LayerNorm",
          "name": "layer5_norm1",
          "layer": "layer_5",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_59",
          "type": "LayerNorm",
          "name": "layer5_norm2",
          "layer": "layer_5",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_60",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_61",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_62",
          "type": "Dropout",
          "name": "layer5_attn_drop",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_63",
          "type": "Dropout",
          "name": "layer5_proj_drop",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_64",
          "type": "Linear",
          "name": "layer6_qkv",
          "layer": "layer_6",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_65",
          "type": "Linear",
          "name": "layer6_proj",
          "layer": "layer_6",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_66",
          "type": "Linear",
          "name": "layer6_fc1",
          "layer": "layer_6",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_67",
          "type": "Linear",
          "name": "layer6_fc2",
          "layer": "layer_6",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_68",
          "type": "LayerNorm",
          "name": "layer6_norm1",
          "layer": "layer_6",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_69",
          "type": "LayerNorm",
          "name": "layer6_norm2",
          "layer": "layer_6",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_70",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_71",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_72",
          "type": "Dropout",
          "name": "layer6_attn_drop",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_73",
          "type": "Dropout",
          "name": "layer6_proj_drop",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_74",
          "type": "Linear",
          "name": "layer7_qkv",
          "layer": "layer_7",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_75",
          "type": "Linear",
          "name": "layer7_proj",
          "layer": "layer_7",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_76",
          "type": "Linear",
          "name": "layer7_fc1",
          "layer": "layer_7",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_77",
          "type": "Linear",
          "name": "layer7_fc2",
          "layer": "layer_7",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_78",
          "type": "LayerNorm",
          "name": "layer7_norm1",
          "layer": "layer_7",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_79",
          "type": "LayerNorm",
          "name": "layer7_norm2",
          "layer": "layer_7",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_80",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_81",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_82",
          "type": "Dropout",
          "name": "layer7_attn_drop",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_83",
          "type": "Dropout",
          "name": "layer7_proj_drop",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_84",
          "type": "Linear",
          "name": "layer8_qkv",
          "layer": "layer_8",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_85",
          "type": "Linear",
          "name": "layer8_proj",
          "layer": "layer_8",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_86",
          "type": "Linear",
          "name": "layer8_fc1",
          "layer": "layer_8",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_87",
          "type": "Linear",
          "name": "layer8_fc2",
          "layer": "layer_8",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_88",
          "type": "LayerNorm",
          "name": "layer8_norm1",
          "layer": "layer_8",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_89",
          "type": "LayerNorm",
          "name": "layer8_norm2",
          "layer": "layer_8",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_90",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_91",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_92",
          "type": "Dropout",
          "name": "layer8_attn_drop",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_93",
          "type": "Dropout",
          "name": "layer8_proj_drop",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_94",
          "type": "Linear",
          "name": "layer9_qkv",
          "layer": "layer_9",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_95",
          "type": "Linear",
          "name": "layer9_proj",
          "layer": "layer_9",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_96",
          "type": "Linear",
          "name": "layer9_fc1",
          "layer": "layer_9",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_97",
          "type": "Linear",
          "name": "layer9_fc2",
          "layer": "layer_9",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_98",
          "type": "LayerNorm",
          "name": "layer9_norm1",
          "layer": "layer_9",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_99",
          "type": "LayerNorm",
          "name": "layer9_norm2",
          "layer": "layer_9",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_100",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_101",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_102",
          "type": "Dropout",
          "name": "layer9_attn_drop",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_103",
          "type": "Dropout",
          "name": "layer9_proj_drop",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_104",
          "type": "Linear",
          "name": "layer10_qkv",
          "layer": "layer_10",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_105",
          "type": "Linear",
          "name": "layer10_proj",
          "layer": "layer_10",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_106",
          "type": "Linear",
          "name": "layer10_fc1",
          "layer": "layer_10",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_107",
          "type": "Linear",
          "name": "layer10_fc2",
          "layer": "layer_10",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_108",
          "type": "LayerNorm",
          "name": "layer10_norm1",
          "layer": "layer_10",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_109",
          "type": "LayerNorm",
          "name": "layer10_norm2",
          "layer": "layer_10",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_110",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_111",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_112",
          "type": "Dropout",
          "name": "layer10_attn_drop",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_113",
          "type": "Dropout",
          "name": "layer10_proj_drop",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_114",
          "type": "Linear",
          "name": "layer11_qkv",
          "layer": "layer_11",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_115",
          "type": "Linear",
          "name": "layer11_proj",
          "layer": "layer_11",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_116",
          "type": "Linear",
          "name": "layer11_fc1",
          "layer": "layer_11",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_117",
          "type": "Linear",
          "name": "layer11_fc2",
          "layer": "layer_11",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_118",
          "type": "LayerNorm",
          "name": "layer11_norm1",
          "layer": "layer_11",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_119",
          "type": "LayerNorm",
          "name": "layer11_norm2",
          "layer": "layer_11",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_120",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_121",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_122",
          "type": "Dropout",
          "name": "layer11_attn_drop",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_123",
          "type": "Dropout",
          "name": "layer11_proj_drop",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_124",
          "type": "Linear",
          "name": "layer12_qkv",
          "layer": "layer_12",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_125",
          "type": "Linear",
          "name": "layer12_proj",
          "layer": "layer_12",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_126",
          "type": "Linear",
          "name": "layer12_fc1",
          "layer": "layer_12",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_127",
          "type": "Linear",
          "name": "layer12_fc2",
          "layer": "layer_12",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_128",
          "type": "LayerNorm",
          "name": "layer12_norm1",
          "layer": "layer_12",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_129",
          "type": "LayerNorm",
          "name": "layer12_norm2",
          "layer": "layer_12",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_130",
          "type": "Softmax",
          "name": "layer12_attention_softmax",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_131",
          "type": "GELU",
          "name": "layer12_activation",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_132",
          "type": "Dropout",
          "name": "layer12_attn_drop",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_133",
          "type": "Dropout",
          "name": "layer12_proj_drop",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_134",
          "type": "Linear",
          "name": "layer13_qkv",
          "layer": "layer_13",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_135",
          "type": "Linear",
          "name": "layer13_proj",
          "layer": "layer_13",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_136",
          "type": "Linear",
          "name": "layer13_fc1",
          "layer": "layer_13",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_137",
          "type": "Linear",
          "name": "layer13_fc2",
          "layer": "layer_13",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_138",
          "type": "LayerNorm",
          "name": "layer13_norm1",
          "layer": "layer_13",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_139",
          "type": "LayerNorm",
          "name": "layer13_norm2",
          "layer": "layer_13",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_140",
          "type": "Softmax",
          "name": "layer13_attention_softmax",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_141",
          "type": "GELU",
          "name": "layer13_activation",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_142",
          "type": "Dropout",
          "name": "layer13_attn_drop",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_143",
          "type": "Dropout",
          "name": "layer13_proj_drop",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_144",
          "type": "Linear",
          "name": "layer14_qkv",
          "layer": "layer_14",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_145",
          "type": "Linear",
          "name": "layer14_proj",
          "layer": "layer_14",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_146",
          "type": "Linear",
          "name": "layer14_fc1",
          "layer": "layer_14",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_147",
          "type": "Linear",
          "name": "layer14_fc2",
          "layer": "layer_14",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_148",
          "type": "LayerNorm",
          "name": "layer14_norm1",
          "layer": "layer_14",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_149",
          "type": "LayerNorm",
          "name": "layer14_norm2",
          "layer": "layer_14",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_150",
          "type": "Softmax",
          "name": "layer14_attention_softmax",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_151",
          "type": "GELU",
          "name": "layer14_activation",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_152",
          "type": "Dropout",
          "name": "layer14_attn_drop",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_153",
          "type": "Dropout",
          "name": "layer14_proj_drop",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_154",
          "type": "Linear",
          "name": "layer15_qkv",
          "layer": "layer_15",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_155",
          "type": "Linear",
          "name": "layer15_proj",
          "layer": "layer_15",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_156",
          "type": "Linear",
          "name": "layer15_fc1",
          "layer": "layer_15",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_157",
          "type": "Linear",
          "name": "layer15_fc2",
          "layer": "layer_15",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_158",
          "type": "LayerNorm",
          "name": "layer15_norm1",
          "layer": "layer_15",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_159",
          "type": "LayerNorm",
          "name": "layer15_norm2",
          "layer": "layer_15",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_160",
          "type": "Softmax",
          "name": "layer15_attention_softmax",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_161",
          "type": "GELU",
          "name": "layer15_activation",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_162",
          "type": "Dropout",
          "name": "layer15_attn_drop",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_163",
          "type": "Dropout",
          "name": "layer15_proj_drop",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_164",
          "type": "Linear",
          "name": "layer16_qkv",
          "layer": "layer_16",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_165",
          "type": "Linear",
          "name": "layer16_proj",
          "layer": "layer_16",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_166",
          "type": "Linear",
          "name": "layer16_fc1",
          "layer": "layer_16",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_167",
          "type": "Linear",
          "name": "layer16_fc2",
          "layer": "layer_16",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_168",
          "type": "LayerNorm",
          "name": "layer16_norm1",
          "layer": "layer_16",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_169",
          "type": "LayerNorm",
          "name": "layer16_norm2",
          "layer": "layer_16",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_170",
          "type": "Softmax",
          "name": "layer16_attention_softmax",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_171",
          "type": "GELU",
          "name": "layer16_activation",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_172",
          "type": "Dropout",
          "name": "layer16_attn_drop",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_173",
          "type": "Dropout",
          "name": "layer16_proj_drop",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_174",
          "type": "Linear",
          "name": "layer17_qkv",
          "layer": "layer_17",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_175",
          "type": "Linear",
          "name": "layer17_proj",
          "layer": "layer_17",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_176",
          "type": "Linear",
          "name": "layer17_fc1",
          "layer": "layer_17",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_177",
          "type": "Linear",
          "name": "layer17_fc2",
          "layer": "layer_17",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_178",
          "type": "LayerNorm",
          "name": "layer17_norm1",
          "layer": "layer_17",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_179",
          "type": "LayerNorm",
          "name": "layer17_norm2",
          "layer": "layer_17",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_180",
          "type": "Softmax",
          "name": "layer17_attention_softmax",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_181",
          "type": "GELU",
          "name": "layer17_activation",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_182",
          "type": "Dropout",
          "name": "layer17_attn_drop",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_183",
          "type": "Dropout",
          "name": "layer17_proj_drop",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_184",
          "type": "Linear",
          "name": "layer18_qkv",
          "layer": "layer_18",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_185",
          "type": "Linear",
          "name": "layer18_proj",
          "layer": "layer_18",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_186",
          "type": "Linear",
          "name": "layer18_fc1",
          "layer": "layer_18",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_187",
          "type": "Linear",
          "name": "layer18_fc2",
          "layer": "layer_18",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_188",
          "type": "LayerNorm",
          "name": "layer18_norm1",
          "layer": "layer_18",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_189",
          "type": "LayerNorm",
          "name": "layer18_norm2",
          "layer": "layer_18",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_190",
          "type": "Softmax",
          "name": "layer18_attention_softmax",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_191",
          "type": "GELU",
          "name": "layer18_activation",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_192",
          "type": "Dropout",
          "name": "layer18_attn_drop",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_193",
          "type": "Dropout",
          "name": "layer18_proj_drop",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_194",
          "type": "Linear",
          "name": "layer19_qkv",
          "layer": "layer_19",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_195",
          "type": "Linear",
          "name": "layer19_proj",
          "layer": "layer_19",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_196",
          "type": "Linear",
          "name": "layer19_fc1",
          "layer": "layer_19",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_197",
          "type": "Linear",
          "name": "layer19_fc2",
          "layer": "layer_19",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_198",
          "type": "LayerNorm",
          "name": "layer19_norm1",
          "layer": "layer_19",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_199",
          "type": "LayerNorm",
          "name": "layer19_norm2",
          "layer": "layer_19",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_200",
          "type": "Softmax",
          "name": "layer19_attention_softmax",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_201",
          "type": "GELU",
          "name": "layer19_activation",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_202",
          "type": "Dropout",
          "name": "layer19_attn_drop",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_203",
          "type": "Dropout",
          "name": "layer19_proj_drop",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_204",
          "type": "Linear",
          "name": "layer20_qkv",
          "layer": "layer_20",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_205",
          "type": "Linear",
          "name": "layer20_proj",
          "layer": "layer_20",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_206",
          "type": "Linear",
          "name": "layer20_fc1",
          "layer": "layer_20",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_207",
          "type": "Linear",
          "name": "layer20_fc2",
          "layer": "layer_20",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_208",
          "type": "LayerNorm",
          "name": "layer20_norm1",
          "layer": "layer_20",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_209",
          "type": "LayerNorm",
          "name": "layer20_norm2",
          "layer": "layer_20",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_210",
          "type": "Softmax",
          "name": "layer20_attention_softmax",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_211",
          "type": "GELU",
          "name": "layer20_activation",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_212",
          "type": "Dropout",
          "name": "layer20_attn_drop",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_213",
          "type": "Dropout",
          "name": "layer20_proj_drop",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_214",
          "type": "Linear",
          "name": "layer21_qkv",
          "layer": "layer_21",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_215",
          "type": "Linear",
          "name": "layer21_proj",
          "layer": "layer_21",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_216",
          "type": "Linear",
          "name": "layer21_fc1",
          "layer": "layer_21",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_217",
          "type": "Linear",
          "name": "layer21_fc2",
          "layer": "layer_21",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_218",
          "type": "LayerNorm",
          "name": "layer21_norm1",
          "layer": "layer_21",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_219",
          "type": "LayerNorm",
          "name": "layer21_norm2",
          "layer": "layer_21",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_220",
          "type": "Softmax",
          "name": "layer21_attention_softmax",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_221",
          "type": "GELU",
          "name": "layer21_activation",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_222",
          "type": "Dropout",
          "name": "layer21_attn_drop",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_223",
          "type": "Dropout",
          "name": "layer21_proj_drop",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_224",
          "type": "Linear",
          "name": "layer22_qkv",
          "layer": "layer_22",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_225",
          "type": "Linear",
          "name": "layer22_proj",
          "layer": "layer_22",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_226",
          "type": "Linear",
          "name": "layer22_fc1",
          "layer": "layer_22",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_227",
          "type": "Linear",
          "name": "layer22_fc2",
          "layer": "layer_22",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_228",
          "type": "LayerNorm",
          "name": "layer22_norm1",
          "layer": "layer_22",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_229",
          "type": "LayerNorm",
          "name": "layer22_norm2",
          "layer": "layer_22",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_230",
          "type": "Softmax",
          "name": "layer22_attention_softmax",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_231",
          "type": "GELU",
          "name": "layer22_activation",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_232",
          "type": "Dropout",
          "name": "layer22_attn_drop",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_233",
          "type": "Dropout",
          "name": "layer22_proj_drop",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_234",
          "type": "Linear",
          "name": "layer23_qkv",
          "layer": "layer_23",
          "parameters": 3145728,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_235",
          "type": "Linear",
          "name": "layer23_proj",
          "layer": "layer_23",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_236",
          "type": "Linear",
          "name": "layer23_fc1",
          "layer": "layer_23",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_237",
          "type": "Linear",
          "name": "layer23_fc2",
          "layer": "layer_23",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Swin_Base_op_238",
          "type": "LayerNorm",
          "name": "layer23_norm1",
          "layer": "layer_23",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_239",
          "type": "LayerNorm",
          "name": "layer23_norm2",
          "layer": "layer_23",
          "parameters": 2048,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_240",
          "type": "Softmax",
          "name": "layer23_attention_softmax",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Swin_Base_op_241",
          "type": "GELU",
          "name": "layer23_activation",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 4096]",
          "output_shape": "[batch, seq, 4096]"
        },
        {
          "op_id": "Swin_Base_op_242",
          "type": "Dropout",
          "name": "layer23_attn_drop",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "Swin_Base_op_243",
          "type": "Dropout",
          "name": "layer23_proj_drop",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        }
      ],
      "parameters": 88000000
    },
    {
      "model_id": "DINOv2_Base",
      "name": "DINOv2-Base",
      "source": "huggingface/dinov2-base",
      "category": "CV",
      "architecture": "vit_style",
      "num_params": 86000000,
      "num_layers": 12,
      "hidden_size": 768,
      "vocab_size": 0,
      "intermediate_size": 3072,
      "operator_count": 123,
      "operators": [
        {
          "op_id": "DINOv2_Base_op_1",
          "type": "Conv2d",
          "name": "patch_embed",
          "layer": "global",
          "parameters": 2073600000000,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "DINOv2_Base_op_2",
          "type": "LayerNorm",
          "name": "norm",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_3",
          "type": "Linear",
          "name": "head",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_4",
          "type": "Linear",
          "name": "layer0_qkv",
          "layer": "layer_0",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_5",
          "type": "Linear",
          "name": "layer0_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_6",
          "type": "Linear",
          "name": "layer0_fc1",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_7",
          "type": "Linear",
          "name": "layer0_fc2",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_8",
          "type": "LayerNorm",
          "name": "layer0_norm1",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_9",
          "type": "LayerNorm",
          "name": "layer0_norm2",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_10",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DINOv2_Base_op_11",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DINOv2_Base_op_12",
          "type": "Dropout",
          "name": "layer0_attn_drop",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_13",
          "type": "Dropout",
          "name": "layer0_proj_drop",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_14",
          "type": "Linear",
          "name": "layer1_qkv",
          "layer": "layer_1",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_15",
          "type": "Linear",
          "name": "layer1_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_16",
          "type": "Linear",
          "name": "layer1_fc1",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_17",
          "type": "Linear",
          "name": "layer1_fc2",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_18",
          "type": "LayerNorm",
          "name": "layer1_norm1",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_19",
          "type": "LayerNorm",
          "name": "layer1_norm2",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_20",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DINOv2_Base_op_21",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DINOv2_Base_op_22",
          "type": "Dropout",
          "name": "layer1_attn_drop",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_23",
          "type": "Dropout",
          "name": "layer1_proj_drop",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_24",
          "type": "Linear",
          "name": "layer2_qkv",
          "layer": "layer_2",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_25",
          "type": "Linear",
          "name": "layer2_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_26",
          "type": "Linear",
          "name": "layer2_fc1",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_27",
          "type": "Linear",
          "name": "layer2_fc2",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_28",
          "type": "LayerNorm",
          "name": "layer2_norm1",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_29",
          "type": "LayerNorm",
          "name": "layer2_norm2",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_30",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DINOv2_Base_op_31",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DINOv2_Base_op_32",
          "type": "Dropout",
          "name": "layer2_attn_drop",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_33",
          "type": "Dropout",
          "name": "layer2_proj_drop",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_34",
          "type": "Linear",
          "name": "layer3_qkv",
          "layer": "layer_3",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_35",
          "type": "Linear",
          "name": "layer3_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_36",
          "type": "Linear",
          "name": "layer3_fc1",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_37",
          "type": "Linear",
          "name": "layer3_fc2",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_38",
          "type": "LayerNorm",
          "name": "layer3_norm1",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_39",
          "type": "LayerNorm",
          "name": "layer3_norm2",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_40",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DINOv2_Base_op_41",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DINOv2_Base_op_42",
          "type": "Dropout",
          "name": "layer3_attn_drop",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_43",
          "type": "Dropout",
          "name": "layer3_proj_drop",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_44",
          "type": "Linear",
          "name": "layer4_qkv",
          "layer": "layer_4",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_45",
          "type": "Linear",
          "name": "layer4_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_46",
          "type": "Linear",
          "name": "layer4_fc1",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_47",
          "type": "Linear",
          "name": "layer4_fc2",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_48",
          "type": "LayerNorm",
          "name": "layer4_norm1",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_49",
          "type": "LayerNorm",
          "name": "layer4_norm2",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_50",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DINOv2_Base_op_51",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DINOv2_Base_op_52",
          "type": "Dropout",
          "name": "layer4_attn_drop",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_53",
          "type": "Dropout",
          "name": "layer4_proj_drop",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_54",
          "type": "Linear",
          "name": "layer5_qkv",
          "layer": "layer_5",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_55",
          "type": "Linear",
          "name": "layer5_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_56",
          "type": "Linear",
          "name": "layer5_fc1",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_57",
          "type": "Linear",
          "name": "layer5_fc2",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_58",
          "type": "LayerNorm",
          "name": "layer5_norm1",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_59",
          "type": "LayerNorm",
          "name": "layer5_norm2",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_60",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DINOv2_Base_op_61",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DINOv2_Base_op_62",
          "type": "Dropout",
          "name": "layer5_attn_drop",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_63",
          "type": "Dropout",
          "name": "layer5_proj_drop",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_64",
          "type": "Linear",
          "name": "layer6_qkv",
          "layer": "layer_6",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_65",
          "type": "Linear",
          "name": "layer6_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_66",
          "type": "Linear",
          "name": "layer6_fc1",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_67",
          "type": "Linear",
          "name": "layer6_fc2",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_68",
          "type": "LayerNorm",
          "name": "layer6_norm1",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_69",
          "type": "LayerNorm",
          "name": "layer6_norm2",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_70",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DINOv2_Base_op_71",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DINOv2_Base_op_72",
          "type": "Dropout",
          "name": "layer6_attn_drop",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_73",
          "type": "Dropout",
          "name": "layer6_proj_drop",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_74",
          "type": "Linear",
          "name": "layer7_qkv",
          "layer": "layer_7",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_75",
          "type": "Linear",
          "name": "layer7_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_76",
          "type": "Linear",
          "name": "layer7_fc1",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_77",
          "type": "Linear",
          "name": "layer7_fc2",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_78",
          "type": "LayerNorm",
          "name": "layer7_norm1",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_79",
          "type": "LayerNorm",
          "name": "layer7_norm2",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_80",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DINOv2_Base_op_81",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DINOv2_Base_op_82",
          "type": "Dropout",
          "name": "layer7_attn_drop",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_83",
          "type": "Dropout",
          "name": "layer7_proj_drop",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_84",
          "type": "Linear",
          "name": "layer8_qkv",
          "layer": "layer_8",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_85",
          "type": "Linear",
          "name": "layer8_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_86",
          "type": "Linear",
          "name": "layer8_fc1",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_87",
          "type": "Linear",
          "name": "layer8_fc2",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_88",
          "type": "LayerNorm",
          "name": "layer8_norm1",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_89",
          "type": "LayerNorm",
          "name": "layer8_norm2",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_90",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DINOv2_Base_op_91",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DINOv2_Base_op_92",
          "type": "Dropout",
          "name": "layer8_attn_drop",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_93",
          "type": "Dropout",
          "name": "layer8_proj_drop",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_94",
          "type": "Linear",
          "name": "layer9_qkv",
          "layer": "layer_9",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_95",
          "type": "Linear",
          "name": "layer9_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_96",
          "type": "Linear",
          "name": "layer9_fc1",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_97",
          "type": "Linear",
          "name": "layer9_fc2",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_98",
          "type": "LayerNorm",
          "name": "layer9_norm1",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_99",
          "type": "LayerNorm",
          "name": "layer9_norm2",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_100",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DINOv2_Base_op_101",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DINOv2_Base_op_102",
          "type": "Dropout",
          "name": "layer9_attn_drop",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_103",
          "type": "Dropout",
          "name": "layer9_proj_drop",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_104",
          "type": "Linear",
          "name": "layer10_qkv",
          "layer": "layer_10",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_105",
          "type": "Linear",
          "name": "layer10_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_106",
          "type": "Linear",
          "name": "layer10_fc1",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_107",
          "type": "Linear",
          "name": "layer10_fc2",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_108",
          "type": "LayerNorm",
          "name": "layer10_norm1",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_109",
          "type": "LayerNorm",
          "name": "layer10_norm2",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_110",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DINOv2_Base_op_111",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DINOv2_Base_op_112",
          "type": "Dropout",
          "name": "layer10_attn_drop",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_113",
          "type": "Dropout",
          "name": "layer10_proj_drop",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_114",
          "type": "Linear",
          "name": "layer11_qkv",
          "layer": "layer_11",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_115",
          "type": "Linear",
          "name": "layer11_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_116",
          "type": "Linear",
          "name": "layer11_fc1",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_117",
          "type": "Linear",
          "name": "layer11_fc2",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DINOv2_Base_op_118",
          "type": "LayerNorm",
          "name": "layer11_norm1",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_119",
          "type": "LayerNorm",
          "name": "layer11_norm2",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_120",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DINOv2_Base_op_121",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DINOv2_Base_op_122",
          "type": "Dropout",
          "name": "layer11_attn_drop",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DINOv2_Base_op_123",
          "type": "Dropout",
          "name": "layer11_proj_drop",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        }
      ],
      "parameters": 86000000
    },
    {
      "model_id": "MobileNet_V2",
      "name": "MobileNet-V2",
      "source": "huggingface/mobilenet-v2",
      "category": "CV",
      "architecture": "mobilenet_style",
      "num_params": 3500000,
      "num_layers": 17,
      "hidden_size": 768,
      "vocab_size": 0,
      "intermediate_size": 3072,
      "operator_count": 126,
      "operators": [
        {
          "op_id": "MobileNet_V2_op_1",
          "type": "Conv2d",
          "name": "conv_stem",
          "layer": "global",
          "parameters": 864,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_2",
          "type": "BatchNorm2d",
          "name": "bn1",
          "layer": "global",
          "parameters": 128,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_3",
          "type": "Conv2d",
          "name": "conv_head",
          "layer": "global",
          "parameters": 409600,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_4",
          "type": "BatchNorm2d",
          "name": "bn2",
          "layer": "global",
          "parameters": 5120,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_5",
          "type": "AdaptiveAvgPool2d",
          "name": "avgpool",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, 1, 1]"
        },
        {
          "op_id": "MobileNet_V2_op_6",
          "type": "Linear",
          "name": "classifier",
          "layer": "global",
          "parameters": 1280000,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "MobileNet_V2_op_7",
          "type": "ReLU6",
          "name": "relu6",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_8",
          "type": "Conv2d",
          "name": "stage1_block1_expand_conv",
          "layer": "stage1_block1",
          "parameters": 1536,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_9",
          "type": "Conv2d",
          "name": "stage1_block1_depthwise_conv",
          "layer": "stage1_block1",
          "parameters": 864,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_10",
          "type": "Conv2d",
          "name": "stage1_block1_project_conv",
          "layer": "stage1_block1",
          "parameters": 2304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_11",
          "type": "BatchNorm2d",
          "name": "stage1_block1_bn1",
          "layer": "stage1_block1",
          "parameters": 384,
          "input_shape": "[batch, 16, H, W]",
          "output_shape": "[batch, 16, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_12",
          "type": "BatchNorm2d",
          "name": "stage1_block1_bn2",
          "layer": "stage1_block1",
          "parameters": 384,
          "input_shape": "[batch, 16, H, W]",
          "output_shape": "[batch, 16, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_13",
          "type": "BatchNorm2d",
          "name": "stage1_block1_bn3",
          "layer": "stage1_block1",
          "parameters": 96,
          "input_shape": "[batch, 16, H, W]",
          "output_shape": "[batch, 16, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_14",
          "type": "ReLU6",
          "name": "stage1_block1_relu6",
          "layer": "stage1_block1",
          "parameters": 0,
          "input_shape": "[batch, 16, H, W]",
          "output_shape": "[batch, 16, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_15",
          "type": "Conv2d",
          "name": "stage2_block1_expand_conv",
          "layer": "stage2_block1",
          "parameters": 3456,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_16",
          "type": "Conv2d",
          "name": "stage2_block1_depthwise_conv",
          "layer": "stage2_block1",
          "parameters": 1296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_17",
          "type": "Conv2d",
          "name": "stage2_block1_project_conv",
          "layer": "stage2_block1",
          "parameters": 4608,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_18",
          "type": "BatchNorm2d",
          "name": "stage2_block1_bn1",
          "layer": "stage2_block1",
          "parameters": 576,
          "input_shape": "[batch, 24, H, W]",
          "output_shape": "[batch, 24, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_19",
          "type": "BatchNorm2d",
          "name": "stage2_block1_bn2",
          "layer": "stage2_block1",
          "parameters": 576,
          "input_shape": "[batch, 24, H, W]",
          "output_shape": "[batch, 24, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_20",
          "type": "BatchNorm2d",
          "name": "stage2_block1_bn3",
          "layer": "stage2_block1",
          "parameters": 128,
          "input_shape": "[batch, 24, H, W]",
          "output_shape": "[batch, 24, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_21",
          "type": "ReLU6",
          "name": "stage2_block1_relu6",
          "layer": "stage2_block1",
          "parameters": 0,
          "input_shape": "[batch, 24, H, W]",
          "output_shape": "[batch, 24, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_22",
          "type": "Conv2d",
          "name": "stage2_block2_expand_conv",
          "layer": "stage2_block2",
          "parameters": 3456,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_23",
          "type": "Conv2d",
          "name": "stage2_block2_depthwise_conv",
          "layer": "stage2_block2",
          "parameters": 1296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_24",
          "type": "Conv2d",
          "name": "stage2_block2_project_conv",
          "layer": "stage2_block2",
          "parameters": 4608,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_25",
          "type": "BatchNorm2d",
          "name": "stage2_block2_bn1",
          "layer": "stage2_block2",
          "parameters": 576,
          "input_shape": "[batch, 24, H, W]",
          "output_shape": "[batch, 24, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_26",
          "type": "BatchNorm2d",
          "name": "stage2_block2_bn2",
          "layer": "stage2_block2",
          "parameters": 576,
          "input_shape": "[batch, 24, H, W]",
          "output_shape": "[batch, 24, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_27",
          "type": "BatchNorm2d",
          "name": "stage2_block2_bn3",
          "layer": "stage2_block2",
          "parameters": 128,
          "input_shape": "[batch, 24, H, W]",
          "output_shape": "[batch, 24, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_28",
          "type": "ReLU6",
          "name": "stage2_block2_relu6",
          "layer": "stage2_block2",
          "parameters": 0,
          "input_shape": "[batch, 24, H, W]",
          "output_shape": "[batch, 24, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_29",
          "type": "Conv2d",
          "name": "stage3_block1_expand_conv",
          "layer": "stage3_block1",
          "parameters": 6144,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_30",
          "type": "Conv2d",
          "name": "stage3_block1_depthwise_conv",
          "layer": "stage3_block1",
          "parameters": 1728,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_31",
          "type": "Conv2d",
          "name": "stage3_block1_project_conv",
          "layer": "stage3_block1",
          "parameters": 12288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_32",
          "type": "BatchNorm2d",
          "name": "stage3_block1_bn1",
          "layer": "stage3_block1",
          "parameters": 768,
          "input_shape": "[batch, 32, H, W]",
          "output_shape": "[batch, 32, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_33",
          "type": "BatchNorm2d",
          "name": "stage3_block1_bn2",
          "layer": "stage3_block1",
          "parameters": 768,
          "input_shape": "[batch, 32, H, W]",
          "output_shape": "[batch, 32, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_34",
          "type": "BatchNorm2d",
          "name": "stage3_block1_bn3",
          "layer": "stage3_block1",
          "parameters": 256,
          "input_shape": "[batch, 32, H, W]",
          "output_shape": "[batch, 32, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_35",
          "type": "ReLU6",
          "name": "stage3_block1_relu6",
          "layer": "stage3_block1",
          "parameters": 0,
          "input_shape": "[batch, 32, H, W]",
          "output_shape": "[batch, 32, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_36",
          "type": "Conv2d",
          "name": "stage3_block2_expand_conv",
          "layer": "stage3_block2",
          "parameters": 6144,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_37",
          "type": "Conv2d",
          "name": "stage3_block2_depthwise_conv",
          "layer": "stage3_block2",
          "parameters": 1728,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_38",
          "type": "Conv2d",
          "name": "stage3_block2_project_conv",
          "layer": "stage3_block2",
          "parameters": 12288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_39",
          "type": "BatchNorm2d",
          "name": "stage3_block2_bn1",
          "layer": "stage3_block2",
          "parameters": 768,
          "input_shape": "[batch, 32, H, W]",
          "output_shape": "[batch, 32, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_40",
          "type": "BatchNorm2d",
          "name": "stage3_block2_bn2",
          "layer": "stage3_block2",
          "parameters": 768,
          "input_shape": "[batch, 32, H, W]",
          "output_shape": "[batch, 32, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_41",
          "type": "BatchNorm2d",
          "name": "stage3_block2_bn3",
          "layer": "stage3_block2",
          "parameters": 256,
          "input_shape": "[batch, 32, H, W]",
          "output_shape": "[batch, 32, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_42",
          "type": "ReLU6",
          "name": "stage3_block2_relu6",
          "layer": "stage3_block2",
          "parameters": 0,
          "input_shape": "[batch, 32, H, W]",
          "output_shape": "[batch, 32, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_43",
          "type": "Conv2d",
          "name": "stage3_block3_expand_conv",
          "layer": "stage3_block3",
          "parameters": 6144,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_44",
          "type": "Conv2d",
          "name": "stage3_block3_depthwise_conv",
          "layer": "stage3_block3",
          "parameters": 1728,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_45",
          "type": "Conv2d",
          "name": "stage3_block3_project_conv",
          "layer": "stage3_block3",
          "parameters": 12288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_46",
          "type": "BatchNorm2d",
          "name": "stage3_block3_bn1",
          "layer": "stage3_block3",
          "parameters": 768,
          "input_shape": "[batch, 32, H, W]",
          "output_shape": "[batch, 32, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_47",
          "type": "BatchNorm2d",
          "name": "stage3_block3_bn2",
          "layer": "stage3_block3",
          "parameters": 768,
          "input_shape": "[batch, 32, H, W]",
          "output_shape": "[batch, 32, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_48",
          "type": "BatchNorm2d",
          "name": "stage3_block3_bn3",
          "layer": "stage3_block3",
          "parameters": 256,
          "input_shape": "[batch, 32, H, W]",
          "output_shape": "[batch, 32, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_49",
          "type": "ReLU6",
          "name": "stage3_block3_relu6",
          "layer": "stage3_block3",
          "parameters": 0,
          "input_shape": "[batch, 32, H, W]",
          "output_shape": "[batch, 32, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_50",
          "type": "Conv2d",
          "name": "stage4_block1_expand_conv",
          "layer": "stage4_block1",
          "parameters": 24576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_51",
          "type": "Conv2d",
          "name": "stage4_block1_depthwise_conv",
          "layer": "stage4_block1",
          "parameters": 3456,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_52",
          "type": "Conv2d",
          "name": "stage4_block1_project_conv",
          "layer": "stage4_block1",
          "parameters": 36864,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_53",
          "type": "BatchNorm2d",
          "name": "stage4_block1_bn1",
          "layer": "stage4_block1",
          "parameters": 1536,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_54",
          "type": "BatchNorm2d",
          "name": "stage4_block1_bn2",
          "layer": "stage4_block1",
          "parameters": 1536,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_55",
          "type": "BatchNorm2d",
          "name": "stage4_block1_bn3",
          "layer": "stage4_block1",
          "parameters": 384,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_56",
          "type": "ReLU6",
          "name": "stage4_block1_relu6",
          "layer": "stage4_block1",
          "parameters": 0,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_57",
          "type": "Conv2d",
          "name": "stage4_block2_expand_conv",
          "layer": "stage4_block2",
          "parameters": 24576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_58",
          "type": "Conv2d",
          "name": "stage4_block2_depthwise_conv",
          "layer": "stage4_block2",
          "parameters": 3456,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_59",
          "type": "Conv2d",
          "name": "stage4_block2_project_conv",
          "layer": "stage4_block2",
          "parameters": 36864,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_60",
          "type": "BatchNorm2d",
          "name": "stage4_block2_bn1",
          "layer": "stage4_block2",
          "parameters": 1536,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_61",
          "type": "BatchNorm2d",
          "name": "stage4_block2_bn2",
          "layer": "stage4_block2",
          "parameters": 1536,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_62",
          "type": "BatchNorm2d",
          "name": "stage4_block2_bn3",
          "layer": "stage4_block2",
          "parameters": 384,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_63",
          "type": "ReLU6",
          "name": "stage4_block2_relu6",
          "layer": "stage4_block2",
          "parameters": 0,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_64",
          "type": "Conv2d",
          "name": "stage4_block3_expand_conv",
          "layer": "stage4_block3",
          "parameters": 24576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_65",
          "type": "Conv2d",
          "name": "stage4_block3_depthwise_conv",
          "layer": "stage4_block3",
          "parameters": 3456,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_66",
          "type": "Conv2d",
          "name": "stage4_block3_project_conv",
          "layer": "stage4_block3",
          "parameters": 36864,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_67",
          "type": "BatchNorm2d",
          "name": "stage4_block3_bn1",
          "layer": "stage4_block3",
          "parameters": 1536,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_68",
          "type": "BatchNorm2d",
          "name": "stage4_block3_bn2",
          "layer": "stage4_block3",
          "parameters": 1536,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_69",
          "type": "BatchNorm2d",
          "name": "stage4_block3_bn3",
          "layer": "stage4_block3",
          "parameters": 384,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_70",
          "type": "ReLU6",
          "name": "stage4_block3_relu6",
          "layer": "stage4_block3",
          "parameters": 0,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_71",
          "type": "Conv2d",
          "name": "stage4_block4_expand_conv",
          "layer": "stage4_block4",
          "parameters": 24576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_72",
          "type": "Conv2d",
          "name": "stage4_block4_depthwise_conv",
          "layer": "stage4_block4",
          "parameters": 3456,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_73",
          "type": "Conv2d",
          "name": "stage4_block4_project_conv",
          "layer": "stage4_block4",
          "parameters": 36864,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_74",
          "type": "BatchNorm2d",
          "name": "stage4_block4_bn1",
          "layer": "stage4_block4",
          "parameters": 1536,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_75",
          "type": "BatchNorm2d",
          "name": "stage4_block4_bn2",
          "layer": "stage4_block4",
          "parameters": 1536,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_76",
          "type": "BatchNorm2d",
          "name": "stage4_block4_bn3",
          "layer": "stage4_block4",
          "parameters": 384,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_77",
          "type": "ReLU6",
          "name": "stage4_block4_relu6",
          "layer": "stage4_block4",
          "parameters": 0,
          "input_shape": "[batch, 64, H, W]",
          "output_shape": "[batch, 64, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_78",
          "type": "Conv2d",
          "name": "stage5_block1_expand_conv",
          "layer": "stage5_block1",
          "parameters": 55296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_79",
          "type": "Conv2d",
          "name": "stage5_block1_depthwise_conv",
          "layer": "stage5_block1",
          "parameters": 5184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_80",
          "type": "Conv2d",
          "name": "stage5_block1_project_conv",
          "layer": "stage5_block1",
          "parameters": 92160,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_81",
          "type": "BatchNorm2d",
          "name": "stage5_block1_bn1",
          "layer": "stage5_block1",
          "parameters": 2304,
          "input_shape": "[batch, 96, H, W]",
          "output_shape": "[batch, 96, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_82",
          "type": "BatchNorm2d",
          "name": "stage5_block1_bn2",
          "layer": "stage5_block1",
          "parameters": 2304,
          "input_shape": "[batch, 96, H, W]",
          "output_shape": "[batch, 96, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_83",
          "type": "BatchNorm2d",
          "name": "stage5_block1_bn3",
          "layer": "stage5_block1",
          "parameters": 640,
          "input_shape": "[batch, 96, H, W]",
          "output_shape": "[batch, 96, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_84",
          "type": "ReLU6",
          "name": "stage5_block1_relu6",
          "layer": "stage5_block1",
          "parameters": 0,
          "input_shape": "[batch, 96, H, W]",
          "output_shape": "[batch, 96, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_85",
          "type": "Conv2d",
          "name": "stage5_block2_expand_conv",
          "layer": "stage5_block2",
          "parameters": 55296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_86",
          "type": "Conv2d",
          "name": "stage5_block2_depthwise_conv",
          "layer": "stage5_block2",
          "parameters": 5184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_87",
          "type": "Conv2d",
          "name": "stage5_block2_project_conv",
          "layer": "stage5_block2",
          "parameters": 92160,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_88",
          "type": "BatchNorm2d",
          "name": "stage5_block2_bn1",
          "layer": "stage5_block2",
          "parameters": 2304,
          "input_shape": "[batch, 96, H, W]",
          "output_shape": "[batch, 96, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_89",
          "type": "BatchNorm2d",
          "name": "stage5_block2_bn2",
          "layer": "stage5_block2",
          "parameters": 2304,
          "input_shape": "[batch, 96, H, W]",
          "output_shape": "[batch, 96, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_90",
          "type": "BatchNorm2d",
          "name": "stage5_block2_bn3",
          "layer": "stage5_block2",
          "parameters": 640,
          "input_shape": "[batch, 96, H, W]",
          "output_shape": "[batch, 96, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_91",
          "type": "ReLU6",
          "name": "stage5_block2_relu6",
          "layer": "stage5_block2",
          "parameters": 0,
          "input_shape": "[batch, 96, H, W]",
          "output_shape": "[batch, 96, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_92",
          "type": "Conv2d",
          "name": "stage5_block3_expand_conv",
          "layer": "stage5_block3",
          "parameters": 55296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_93",
          "type": "Conv2d",
          "name": "stage5_block3_depthwise_conv",
          "layer": "stage5_block3",
          "parameters": 5184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_94",
          "type": "Conv2d",
          "name": "stage5_block3_project_conv",
          "layer": "stage5_block3",
          "parameters": 92160,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_95",
          "type": "BatchNorm2d",
          "name": "stage5_block3_bn1",
          "layer": "stage5_block3",
          "parameters": 2304,
          "input_shape": "[batch, 96, H, W]",
          "output_shape": "[batch, 96, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_96",
          "type": "BatchNorm2d",
          "name": "stage5_block3_bn2",
          "layer": "stage5_block3",
          "parameters": 2304,
          "input_shape": "[batch, 96, H, W]",
          "output_shape": "[batch, 96, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_97",
          "type": "BatchNorm2d",
          "name": "stage5_block3_bn3",
          "layer": "stage5_block3",
          "parameters": 640,
          "input_shape": "[batch, 96, H, W]",
          "output_shape": "[batch, 96, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_98",
          "type": "ReLU6",
          "name": "stage5_block3_relu6",
          "layer": "stage5_block3",
          "parameters": 0,
          "input_shape": "[batch, 96, H, W]",
          "output_shape": "[batch, 96, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_99",
          "type": "Conv2d",
          "name": "stage6_block1_expand_conv",
          "layer": "stage6_block1",
          "parameters": 153600,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_100",
          "type": "Conv2d",
          "name": "stage6_block1_depthwise_conv",
          "layer": "stage6_block1",
          "parameters": 8640,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_101",
          "type": "Conv2d",
          "name": "stage6_block1_project_conv",
          "layer": "stage6_block1",
          "parameters": 307200,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_102",
          "type": "BatchNorm2d",
          "name": "stage6_block1_bn1",
          "layer": "stage6_block1",
          "parameters": 3840,
          "input_shape": "[batch, 160, H, W]",
          "output_shape": "[batch, 160, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_103",
          "type": "BatchNorm2d",
          "name": "stage6_block1_bn2",
          "layer": "stage6_block1",
          "parameters": 3840,
          "input_shape": "[batch, 160, H, W]",
          "output_shape": "[batch, 160, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_104",
          "type": "BatchNorm2d",
          "name": "stage6_block1_bn3",
          "layer": "stage6_block1",
          "parameters": 1280,
          "input_shape": "[batch, 160, H, W]",
          "output_shape": "[batch, 160, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_105",
          "type": "ReLU6",
          "name": "stage6_block1_relu6",
          "layer": "stage6_block1",
          "parameters": 0,
          "input_shape": "[batch, 160, H, W]",
          "output_shape": "[batch, 160, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_106",
          "type": "Conv2d",
          "name": "stage6_block2_expand_conv",
          "layer": "stage6_block2",
          "parameters": 153600,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_107",
          "type": "Conv2d",
          "name": "stage6_block2_depthwise_conv",
          "layer": "stage6_block2",
          "parameters": 8640,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_108",
          "type": "Conv2d",
          "name": "stage6_block2_project_conv",
          "layer": "stage6_block2",
          "parameters": 307200,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_109",
          "type": "BatchNorm2d",
          "name": "stage6_block2_bn1",
          "layer": "stage6_block2",
          "parameters": 3840,
          "input_shape": "[batch, 160, H, W]",
          "output_shape": "[batch, 160, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_110",
          "type": "BatchNorm2d",
          "name": "stage6_block2_bn2",
          "layer": "stage6_block2",
          "parameters": 3840,
          "input_shape": "[batch, 160, H, W]",
          "output_shape": "[batch, 160, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_111",
          "type": "BatchNorm2d",
          "name": "stage6_block2_bn3",
          "layer": "stage6_block2",
          "parameters": 1280,
          "input_shape": "[batch, 160, H, W]",
          "output_shape": "[batch, 160, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_112",
          "type": "ReLU6",
          "name": "stage6_block2_relu6",
          "layer": "stage6_block2",
          "parameters": 0,
          "input_shape": "[batch, 160, H, W]",
          "output_shape": "[batch, 160, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_113",
          "type": "Conv2d",
          "name": "stage6_block3_expand_conv",
          "layer": "stage6_block3",
          "parameters": 153600,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_114",
          "type": "Conv2d",
          "name": "stage6_block3_depthwise_conv",
          "layer": "stage6_block3",
          "parameters": 8640,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_115",
          "type": "Conv2d",
          "name": "stage6_block3_project_conv",
          "layer": "stage6_block3",
          "parameters": 307200,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_116",
          "type": "BatchNorm2d",
          "name": "stage6_block3_bn1",
          "layer": "stage6_block3",
          "parameters": 3840,
          "input_shape": "[batch, 160, H, W]",
          "output_shape": "[batch, 160, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_117",
          "type": "BatchNorm2d",
          "name": "stage6_block3_bn2",
          "layer": "stage6_block3",
          "parameters": 3840,
          "input_shape": "[batch, 160, H, W]",
          "output_shape": "[batch, 160, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_118",
          "type": "BatchNorm2d",
          "name": "stage6_block3_bn3",
          "layer": "stage6_block3",
          "parameters": 1280,
          "input_shape": "[batch, 160, H, W]",
          "output_shape": "[batch, 160, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_119",
          "type": "ReLU6",
          "name": "stage6_block3_relu6",
          "layer": "stage6_block3",
          "parameters": 0,
          "input_shape": "[batch, 160, H, W]",
          "output_shape": "[batch, 160, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_120",
          "type": "Conv2d",
          "name": "stage7_block1_expand_conv",
          "layer": "stage7_block1",
          "parameters": 614400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_121",
          "type": "Conv2d",
          "name": "stage7_block1_depthwise_conv",
          "layer": "stage7_block1",
          "parameters": 17280,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_122",
          "type": "Conv2d",
          "name": "stage7_block1_project_conv",
          "layer": "stage7_block1",
          "parameters": 614400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "MobileNet_V2_op_123",
          "type": "BatchNorm2d",
          "name": "stage7_block1_bn1",
          "layer": "stage7_block1",
          "parameters": 7680,
          "input_shape": "[batch, 320, H, W]",
          "output_shape": "[batch, 320, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_124",
          "type": "BatchNorm2d",
          "name": "stage7_block1_bn2",
          "layer": "stage7_block1",
          "parameters": 7680,
          "input_shape": "[batch, 320, H, W]",
          "output_shape": "[batch, 320, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_125",
          "type": "BatchNorm2d",
          "name": "stage7_block1_bn3",
          "layer": "stage7_block1",
          "parameters": 1280,
          "input_shape": "[batch, 320, H, W]",
          "output_shape": "[batch, 320, H, W]"
        },
        {
          "op_id": "MobileNet_V2_op_126",
          "type": "ReLU6",
          "name": "stage7_block1_relu6",
          "layer": "stage7_block1",
          "parameters": 0,
          "input_shape": "[batch, 320, H, W]",
          "output_shape": "[batch, 320, H, W]"
        }
      ],
      "parameters": 3500000
    },
    {
      "model_id": "EfficientNet_B0",
      "name": "EfficientNet-B0",
      "source": "huggingface/efficientnet-b0",
      "category": "CV",
      "architecture": "mobilenet_style",
      "num_params": 5300000,
      "num_layers": 16,
      "hidden_size": 768,
      "vocab_size": 0,
      "intermediate_size": 3072,
      "operator_count": 119,
      "operators": [
        {
          "op_id": "EfficientNet_B0_op_1",
          "type": "Conv2d",
          "name": "conv_stem",
          "layer": "global",
          "parameters": 864,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_2",
          "type": "BatchNorm2d",
          "name": "bn1",
          "layer": "global",
          "parameters": 128,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_3",
          "type": "Conv2d",
          "name": "conv_head",
          "layer": "global",
          "parameters": 409600,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_4",
          "type": "BatchNorm2d",
          "name": "bn2",
          "layer": "global",
          "parameters": 5120,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_5",
          "type": "AdaptiveAvgPool2d",
          "name": "avgpool",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, 1, 1]"
        },
        {
          "op_id": "EfficientNet_B0_op_6",
          "type": "Linear",
          "name": "classifier",
          "layer": "global",
          "parameters": 1280000,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "EfficientNet_B0_op_7",
          "type": "ReLU6",
          "name": "relu6",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_8",
          "type": "Conv2d",
          "name": "stage1_block1_expand_conv",
          "layer": "stage1_block1",
          "parameters": 1536,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_9",
          "type": "Conv2d",
          "name": "stage1_block1_depthwise_conv",
          "layer": "stage1_block1",
          "parameters": 864,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_10",
          "type": "Conv2d",
          "name": "stage1_block1_project_conv",
          "layer": "stage1_block1",
          "parameters": 2304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_11",
          "type": "BatchNorm2d",
          "name": "stage1_block1_bn1",
          "layer": "stage1_block1",
          "parameters": 384,
          "input_shape": "[batch, 16, H, W]",
          "output_shape": "[batch, 16, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_12",
          "type": "BatchNorm2d",
          "name": "stage1_block1_bn2",
          "layer": "stage1_block1",
          "parameters": 384,
          "input_shape": "[batch, 16, H, W]",
          "output_shape": "[batch, 16, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_13",
          "type": "BatchNorm2d",
          "name": "stage1_block1_bn3",
          "layer": "stage1_block1",
          "parameters": 96,
          "input_shape": "[batch, 16, H, W]",
          "output_shape": "[batch, 16, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_14",
          "type": "ReLU6",
          "name": "stage1_block1_relu6",
          "layer": "stage1_block1",
          "parameters": 0,
          "input_shape": "[batch, 16, H, W]",
          "output_shape": "[batch, 16, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_15",
          "type": "Conv2d",
          "name": "stage2_block1_expand_conv",
          "layer": "stage2_block1",
          "parameters": 3456,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_16",
          "type": "Conv2d",
          "name": "stage2_block1_depthwise_conv",
          "layer": "stage2_block1",
          "parameters": 1296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_17",
          "type": "Conv2d",
          "name": "stage2_block1_project_conv",
          "layer": "stage2_block1",
          "parameters": 5760,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_18",
          "type": "BatchNorm2d",
          "name": "stage2_block1_bn1",
          "layer": "stage2_block1",
          "parameters": 576,
          "input_shape": "[batch, 24, H, W]",
          "output_shape": "[batch, 24, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_19",
          "type": "BatchNorm2d",
          "name": "stage2_block1_bn2",
          "layer": "stage2_block1",
          "parameters": 576,
          "input_shape": "[batch, 24, H, W]",
          "output_shape": "[batch, 24, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_20",
          "type": "BatchNorm2d",
          "name": "stage2_block1_bn3",
          "layer": "stage2_block1",
          "parameters": 160,
          "input_shape": "[batch, 24, H, W]",
          "output_shape": "[batch, 24, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_21",
          "type": "ReLU6",
          "name": "stage2_block1_relu6",
          "layer": "stage2_block1",
          "parameters": 0,
          "input_shape": "[batch, 24, H, W]",
          "output_shape": "[batch, 24, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_22",
          "type": "Conv2d",
          "name": "stage2_block2_expand_conv",
          "layer": "stage2_block2",
          "parameters": 3456,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_23",
          "type": "Conv2d",
          "name": "stage2_block2_depthwise_conv",
          "layer": "stage2_block2",
          "parameters": 1296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_24",
          "type": "Conv2d",
          "name": "stage2_block2_project_conv",
          "layer": "stage2_block2",
          "parameters": 5760,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_25",
          "type": "BatchNorm2d",
          "name": "stage2_block2_bn1",
          "layer": "stage2_block2",
          "parameters": 576,
          "input_shape": "[batch, 24, H, W]",
          "output_shape": "[batch, 24, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_26",
          "type": "BatchNorm2d",
          "name": "stage2_block2_bn2",
          "layer": "stage2_block2",
          "parameters": 576,
          "input_shape": "[batch, 24, H, W]",
          "output_shape": "[batch, 24, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_27",
          "type": "BatchNorm2d",
          "name": "stage2_block2_bn3",
          "layer": "stage2_block2",
          "parameters": 160,
          "input_shape": "[batch, 24, H, W]",
          "output_shape": "[batch, 24, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_28",
          "type": "ReLU6",
          "name": "stage2_block2_relu6",
          "layer": "stage2_block2",
          "parameters": 0,
          "input_shape": "[batch, 24, H, W]",
          "output_shape": "[batch, 24, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_29",
          "type": "Conv2d",
          "name": "stage3_block1_expand_conv",
          "layer": "stage3_block1",
          "parameters": 9600,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_30",
          "type": "Conv2d",
          "name": "stage3_block1_depthwise_conv",
          "layer": "stage3_block1",
          "parameters": 2160,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_31",
          "type": "Conv2d",
          "name": "stage3_block1_project_conv",
          "layer": "stage3_block1",
          "parameters": 19200,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_32",
          "type": "BatchNorm2d",
          "name": "stage3_block1_bn1",
          "layer": "stage3_block1",
          "parameters": 960,
          "input_shape": "[batch, 40, H, W]",
          "output_shape": "[batch, 40, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_33",
          "type": "BatchNorm2d",
          "name": "stage3_block1_bn2",
          "layer": "stage3_block1",
          "parameters": 960,
          "input_shape": "[batch, 40, H, W]",
          "output_shape": "[batch, 40, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_34",
          "type": "BatchNorm2d",
          "name": "stage3_block1_bn3",
          "layer": "stage3_block1",
          "parameters": 320,
          "input_shape": "[batch, 40, H, W]",
          "output_shape": "[batch, 40, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_35",
          "type": "ReLU6",
          "name": "stage3_block1_relu6",
          "layer": "stage3_block1",
          "parameters": 0,
          "input_shape": "[batch, 40, H, W]",
          "output_shape": "[batch, 40, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_36",
          "type": "Conv2d",
          "name": "stage3_block2_expand_conv",
          "layer": "stage3_block2",
          "parameters": 9600,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_37",
          "type": "Conv2d",
          "name": "stage3_block2_depthwise_conv",
          "layer": "stage3_block2",
          "parameters": 2160,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_38",
          "type": "Conv2d",
          "name": "stage3_block2_project_conv",
          "layer": "stage3_block2",
          "parameters": 19200,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_39",
          "type": "BatchNorm2d",
          "name": "stage3_block2_bn1",
          "layer": "stage3_block2",
          "parameters": 960,
          "input_shape": "[batch, 40, H, W]",
          "output_shape": "[batch, 40, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_40",
          "type": "BatchNorm2d",
          "name": "stage3_block2_bn2",
          "layer": "stage3_block2",
          "parameters": 960,
          "input_shape": "[batch, 40, H, W]",
          "output_shape": "[batch, 40, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_41",
          "type": "BatchNorm2d",
          "name": "stage3_block2_bn3",
          "layer": "stage3_block2",
          "parameters": 320,
          "input_shape": "[batch, 40, H, W]",
          "output_shape": "[batch, 40, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_42",
          "type": "ReLU6",
          "name": "stage3_block2_relu6",
          "layer": "stage3_block2",
          "parameters": 0,
          "input_shape": "[batch, 40, H, W]",
          "output_shape": "[batch, 40, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_43",
          "type": "Conv2d",
          "name": "stage4_block1_expand_conv",
          "layer": "stage4_block1",
          "parameters": 38400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_44",
          "type": "Conv2d",
          "name": "stage4_block1_depthwise_conv",
          "layer": "stage4_block1",
          "parameters": 4320,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_45",
          "type": "Conv2d",
          "name": "stage4_block1_project_conv",
          "layer": "stage4_block1",
          "parameters": 53760,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_46",
          "type": "BatchNorm2d",
          "name": "stage4_block1_bn1",
          "layer": "stage4_block1",
          "parameters": 1920,
          "input_shape": "[batch, 80, H, W]",
          "output_shape": "[batch, 80, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_47",
          "type": "BatchNorm2d",
          "name": "stage4_block1_bn2",
          "layer": "stage4_block1",
          "parameters": 1920,
          "input_shape": "[batch, 80, H, W]",
          "output_shape": "[batch, 80, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_48",
          "type": "BatchNorm2d",
          "name": "stage4_block1_bn3",
          "layer": "stage4_block1",
          "parameters": 448,
          "input_shape": "[batch, 80, H, W]",
          "output_shape": "[batch, 80, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_49",
          "type": "ReLU6",
          "name": "stage4_block1_relu6",
          "layer": "stage4_block1",
          "parameters": 0,
          "input_shape": "[batch, 80, H, W]",
          "output_shape": "[batch, 80, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_50",
          "type": "Conv2d",
          "name": "stage4_block2_expand_conv",
          "layer": "stage4_block2",
          "parameters": 38400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_51",
          "type": "Conv2d",
          "name": "stage4_block2_depthwise_conv",
          "layer": "stage4_block2",
          "parameters": 4320,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_52",
          "type": "Conv2d",
          "name": "stage4_block2_project_conv",
          "layer": "stage4_block2",
          "parameters": 53760,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_53",
          "type": "BatchNorm2d",
          "name": "stage4_block2_bn1",
          "layer": "stage4_block2",
          "parameters": 1920,
          "input_shape": "[batch, 80, H, W]",
          "output_shape": "[batch, 80, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_54",
          "type": "BatchNorm2d",
          "name": "stage4_block2_bn2",
          "layer": "stage4_block2",
          "parameters": 1920,
          "input_shape": "[batch, 80, H, W]",
          "output_shape": "[batch, 80, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_55",
          "type": "BatchNorm2d",
          "name": "stage4_block2_bn3",
          "layer": "stage4_block2",
          "parameters": 448,
          "input_shape": "[batch, 80, H, W]",
          "output_shape": "[batch, 80, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_56",
          "type": "ReLU6",
          "name": "stage4_block2_relu6",
          "layer": "stage4_block2",
          "parameters": 0,
          "input_shape": "[batch, 80, H, W]",
          "output_shape": "[batch, 80, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_57",
          "type": "Conv2d",
          "name": "stage4_block3_expand_conv",
          "layer": "stage4_block3",
          "parameters": 38400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_58",
          "type": "Conv2d",
          "name": "stage4_block3_depthwise_conv",
          "layer": "stage4_block3",
          "parameters": 4320,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_59",
          "type": "Conv2d",
          "name": "stage4_block3_project_conv",
          "layer": "stage4_block3",
          "parameters": 53760,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_60",
          "type": "BatchNorm2d",
          "name": "stage4_block3_bn1",
          "layer": "stage4_block3",
          "parameters": 1920,
          "input_shape": "[batch, 80, H, W]",
          "output_shape": "[batch, 80, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_61",
          "type": "BatchNorm2d",
          "name": "stage4_block3_bn2",
          "layer": "stage4_block3",
          "parameters": 1920,
          "input_shape": "[batch, 80, H, W]",
          "output_shape": "[batch, 80, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_62",
          "type": "BatchNorm2d",
          "name": "stage4_block3_bn3",
          "layer": "stage4_block3",
          "parameters": 448,
          "input_shape": "[batch, 80, H, W]",
          "output_shape": "[batch, 80, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_63",
          "type": "ReLU6",
          "name": "stage4_block3_relu6",
          "layer": "stage4_block3",
          "parameters": 0,
          "input_shape": "[batch, 80, H, W]",
          "output_shape": "[batch, 80, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_64",
          "type": "Conv2d",
          "name": "stage5_block1_expand_conv",
          "layer": "stage5_block1",
          "parameters": 75264,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_65",
          "type": "Conv2d",
          "name": "stage5_block1_depthwise_conv",
          "layer": "stage5_block1",
          "parameters": 6048,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_66",
          "type": "Conv2d",
          "name": "stage5_block1_project_conv",
          "layer": "stage5_block1",
          "parameters": 129024,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_67",
          "type": "BatchNorm2d",
          "name": "stage5_block1_bn1",
          "layer": "stage5_block1",
          "parameters": 2688,
          "input_shape": "[batch, 112, H, W]",
          "output_shape": "[batch, 112, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_68",
          "type": "BatchNorm2d",
          "name": "stage5_block1_bn2",
          "layer": "stage5_block1",
          "parameters": 2688,
          "input_shape": "[batch, 112, H, W]",
          "output_shape": "[batch, 112, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_69",
          "type": "BatchNorm2d",
          "name": "stage5_block1_bn3",
          "layer": "stage5_block1",
          "parameters": 768,
          "input_shape": "[batch, 112, H, W]",
          "output_shape": "[batch, 112, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_70",
          "type": "ReLU6",
          "name": "stage5_block1_relu6",
          "layer": "stage5_block1",
          "parameters": 0,
          "input_shape": "[batch, 112, H, W]",
          "output_shape": "[batch, 112, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_71",
          "type": "Conv2d",
          "name": "stage5_block2_expand_conv",
          "layer": "stage5_block2",
          "parameters": 75264,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_72",
          "type": "Conv2d",
          "name": "stage5_block2_depthwise_conv",
          "layer": "stage5_block2",
          "parameters": 6048,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_73",
          "type": "Conv2d",
          "name": "stage5_block2_project_conv",
          "layer": "stage5_block2",
          "parameters": 129024,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_74",
          "type": "BatchNorm2d",
          "name": "stage5_block2_bn1",
          "layer": "stage5_block2",
          "parameters": 2688,
          "input_shape": "[batch, 112, H, W]",
          "output_shape": "[batch, 112, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_75",
          "type": "BatchNorm2d",
          "name": "stage5_block2_bn2",
          "layer": "stage5_block2",
          "parameters": 2688,
          "input_shape": "[batch, 112, H, W]",
          "output_shape": "[batch, 112, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_76",
          "type": "BatchNorm2d",
          "name": "stage5_block2_bn3",
          "layer": "stage5_block2",
          "parameters": 768,
          "input_shape": "[batch, 112, H, W]",
          "output_shape": "[batch, 112, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_77",
          "type": "ReLU6",
          "name": "stage5_block2_relu6",
          "layer": "stage5_block2",
          "parameters": 0,
          "input_shape": "[batch, 112, H, W]",
          "output_shape": "[batch, 112, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_78",
          "type": "Conv2d",
          "name": "stage5_block3_expand_conv",
          "layer": "stage5_block3",
          "parameters": 75264,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_79",
          "type": "Conv2d",
          "name": "stage5_block3_depthwise_conv",
          "layer": "stage5_block3",
          "parameters": 6048,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_80",
          "type": "Conv2d",
          "name": "stage5_block3_project_conv",
          "layer": "stage5_block3",
          "parameters": 129024,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_81",
          "type": "BatchNorm2d",
          "name": "stage5_block3_bn1",
          "layer": "stage5_block3",
          "parameters": 2688,
          "input_shape": "[batch, 112, H, W]",
          "output_shape": "[batch, 112, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_82",
          "type": "BatchNorm2d",
          "name": "stage5_block3_bn2",
          "layer": "stage5_block3",
          "parameters": 2688,
          "input_shape": "[batch, 112, H, W]",
          "output_shape": "[batch, 112, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_83",
          "type": "BatchNorm2d",
          "name": "stage5_block3_bn3",
          "layer": "stage5_block3",
          "parameters": 768,
          "input_shape": "[batch, 112, H, W]",
          "output_shape": "[batch, 112, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_84",
          "type": "ReLU6",
          "name": "stage5_block3_relu6",
          "layer": "stage5_block3",
          "parameters": 0,
          "input_shape": "[batch, 112, H, W]",
          "output_shape": "[batch, 112, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_85",
          "type": "Conv2d",
          "name": "stage6_block1_expand_conv",
          "layer": "stage6_block1",
          "parameters": 221184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_86",
          "type": "Conv2d",
          "name": "stage6_block1_depthwise_conv",
          "layer": "stage6_block1",
          "parameters": 10368,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_87",
          "type": "Conv2d",
          "name": "stage6_block1_project_conv",
          "layer": "stage6_block1",
          "parameters": 368640,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_88",
          "type": "BatchNorm2d",
          "name": "stage6_block1_bn1",
          "layer": "stage6_block1",
          "parameters": 4608,
          "input_shape": "[batch, 192, H, W]",
          "output_shape": "[batch, 192, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_89",
          "type": "BatchNorm2d",
          "name": "stage6_block1_bn2",
          "layer": "stage6_block1",
          "parameters": 4608,
          "input_shape": "[batch, 192, H, W]",
          "output_shape": "[batch, 192, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_90",
          "type": "BatchNorm2d",
          "name": "stage6_block1_bn3",
          "layer": "stage6_block1",
          "parameters": 1280,
          "input_shape": "[batch, 192, H, W]",
          "output_shape": "[batch, 192, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_91",
          "type": "ReLU6",
          "name": "stage6_block1_relu6",
          "layer": "stage6_block1",
          "parameters": 0,
          "input_shape": "[batch, 192, H, W]",
          "output_shape": "[batch, 192, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_92",
          "type": "Conv2d",
          "name": "stage6_block2_expand_conv",
          "layer": "stage6_block2",
          "parameters": 221184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_93",
          "type": "Conv2d",
          "name": "stage6_block2_depthwise_conv",
          "layer": "stage6_block2",
          "parameters": 10368,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_94",
          "type": "Conv2d",
          "name": "stage6_block2_project_conv",
          "layer": "stage6_block2",
          "parameters": 368640,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_95",
          "type": "BatchNorm2d",
          "name": "stage6_block2_bn1",
          "layer": "stage6_block2",
          "parameters": 4608,
          "input_shape": "[batch, 192, H, W]",
          "output_shape": "[batch, 192, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_96",
          "type": "BatchNorm2d",
          "name": "stage6_block2_bn2",
          "layer": "stage6_block2",
          "parameters": 4608,
          "input_shape": "[batch, 192, H, W]",
          "output_shape": "[batch, 192, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_97",
          "type": "BatchNorm2d",
          "name": "stage6_block2_bn3",
          "layer": "stage6_block2",
          "parameters": 1280,
          "input_shape": "[batch, 192, H, W]",
          "output_shape": "[batch, 192, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_98",
          "type": "ReLU6",
          "name": "stage6_block2_relu6",
          "layer": "stage6_block2",
          "parameters": 0,
          "input_shape": "[batch, 192, H, W]",
          "output_shape": "[batch, 192, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_99",
          "type": "Conv2d",
          "name": "stage6_block3_expand_conv",
          "layer": "stage6_block3",
          "parameters": 221184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_100",
          "type": "Conv2d",
          "name": "stage6_block3_depthwise_conv",
          "layer": "stage6_block3",
          "parameters": 10368,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_101",
          "type": "Conv2d",
          "name": "stage6_block3_project_conv",
          "layer": "stage6_block3",
          "parameters": 368640,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_102",
          "type": "BatchNorm2d",
          "name": "stage6_block3_bn1",
          "layer": "stage6_block3",
          "parameters": 4608,
          "input_shape": "[batch, 192, H, W]",
          "output_shape": "[batch, 192, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_103",
          "type": "BatchNorm2d",
          "name": "stage6_block3_bn2",
          "layer": "stage6_block3",
          "parameters": 4608,
          "input_shape": "[batch, 192, H, W]",
          "output_shape": "[batch, 192, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_104",
          "type": "BatchNorm2d",
          "name": "stage6_block3_bn3",
          "layer": "stage6_block3",
          "parameters": 1280,
          "input_shape": "[batch, 192, H, W]",
          "output_shape": "[batch, 192, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_105",
          "type": "ReLU6",
          "name": "stage6_block3_relu6",
          "layer": "stage6_block3",
          "parameters": 0,
          "input_shape": "[batch, 192, H, W]",
          "output_shape": "[batch, 192, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_106",
          "type": "Conv2d",
          "name": "stage6_block4_expand_conv",
          "layer": "stage6_block4",
          "parameters": 221184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_107",
          "type": "Conv2d",
          "name": "stage6_block4_depthwise_conv",
          "layer": "stage6_block4",
          "parameters": 10368,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_108",
          "type": "Conv2d",
          "name": "stage6_block4_project_conv",
          "layer": "stage6_block4",
          "parameters": 368640,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_109",
          "type": "BatchNorm2d",
          "name": "stage6_block4_bn1",
          "layer": "stage6_block4",
          "parameters": 4608,
          "input_shape": "[batch, 192, H, W]",
          "output_shape": "[batch, 192, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_110",
          "type": "BatchNorm2d",
          "name": "stage6_block4_bn2",
          "layer": "stage6_block4",
          "parameters": 4608,
          "input_shape": "[batch, 192, H, W]",
          "output_shape": "[batch, 192, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_111",
          "type": "BatchNorm2d",
          "name": "stage6_block4_bn3",
          "layer": "stage6_block4",
          "parameters": 1280,
          "input_shape": "[batch, 192, H, W]",
          "output_shape": "[batch, 192, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_112",
          "type": "ReLU6",
          "name": "stage6_block4_relu6",
          "layer": "stage6_block4",
          "parameters": 0,
          "input_shape": "[batch, 192, H, W]",
          "output_shape": "[batch, 192, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_113",
          "type": "Conv2d",
          "name": "stage7_block1_expand_conv",
          "layer": "stage7_block1",
          "parameters": 614400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_114",
          "type": "Conv2d",
          "name": "stage7_block1_depthwise_conv",
          "layer": "stage7_block1",
          "parameters": 17280,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_115",
          "type": "Conv2d",
          "name": "stage7_block1_project_conv",
          "layer": "stage7_block1",
          "parameters": 614400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "EfficientNet_B0_op_116",
          "type": "BatchNorm2d",
          "name": "stage7_block1_bn1",
          "layer": "stage7_block1",
          "parameters": 7680,
          "input_shape": "[batch, 320, H, W]",
          "output_shape": "[batch, 320, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_117",
          "type": "BatchNorm2d",
          "name": "stage7_block1_bn2",
          "layer": "stage7_block1",
          "parameters": 7680,
          "input_shape": "[batch, 320, H, W]",
          "output_shape": "[batch, 320, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_118",
          "type": "BatchNorm2d",
          "name": "stage7_block1_bn3",
          "layer": "stage7_block1",
          "parameters": 1280,
          "input_shape": "[batch, 320, H, W]",
          "output_shape": "[batch, 320, H, W]"
        },
        {
          "op_id": "EfficientNet_B0_op_119",
          "type": "ReLU6",
          "name": "stage7_block1_relu6",
          "layer": "stage7_block1",
          "parameters": 0,
          "input_shape": "[batch, 320, H, W]",
          "output_shape": "[batch, 320, H, W]"
        }
      ],
      "parameters": 5300000
    },
    {
      "model_id": "ConvNeXt_Base",
      "name": "ConvNeXt-Base",
      "source": "huggingface/convnext-base",
      "category": "CV",
      "architecture": "resnet_style",
      "num_params": 89000000,
      "num_layers": 36,
      "hidden_size": 768,
      "vocab_size": 0,
      "intermediate_size": 3072,
      "operator_count": 258,
      "operators": [
        {
          "op_id": "ConvNeXt_Base_op_1",
          "type": "Conv2d",
          "name": "conv1",
          "layer": "global",
          "parameters": 9408,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_2",
          "type": "BatchNorm2d",
          "name": "bn1",
          "layer": "global",
          "parameters": 256,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_3",
          "type": "ReLU",
          "name": "relu",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_4",
          "type": "MaxPool2d",
          "name": "maxpool",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, H/2, W/2]"
        },
        {
          "op_id": "ConvNeXt_Base_op_5",
          "type": "AdaptiveAvgPool2d",
          "name": "avgpool",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, 1, 1]"
        },
        {
          "op_id": "ConvNeXt_Base_op_6",
          "type": "Linear",
          "name": "fc",
          "layer": "global",
          "parameters": 2048000,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ConvNeXt_Base_op_7",
          "type": "Conv2d",
          "name": "stage1_block1_conv1",
          "layer": "stage1_block1",
          "parameters": 32768,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_8",
          "type": "Conv2d",
          "name": "stage1_block1_conv2",
          "layer": "stage1_block1",
          "parameters": 589824,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_9",
          "type": "Conv2d",
          "name": "stage1_block1_conv3",
          "layer": "stage1_block1",
          "parameters": 262144,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_10",
          "type": "BatchNorm2d",
          "name": "stage1_block1_bn1",
          "layer": "stage1_block1",
          "parameters": 1024,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_11",
          "type": "BatchNorm2d",
          "name": "stage1_block1_bn2",
          "layer": "stage1_block1",
          "parameters": 1024,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_12",
          "type": "BatchNorm2d",
          "name": "stage1_block1_bn3",
          "layer": "stage1_block1",
          "parameters": 4096,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_13",
          "type": "ReLU",
          "name": "stage1_block1_relu",
          "layer": "stage1_block1",
          "parameters": 0,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_14",
          "type": "Conv2d",
          "name": "stage1_block2_conv1",
          "layer": "stage1_block2",
          "parameters": 32768,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_15",
          "type": "Conv2d",
          "name": "stage1_block2_conv2",
          "layer": "stage1_block2",
          "parameters": 589824,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_16",
          "type": "Conv2d",
          "name": "stage1_block2_conv3",
          "layer": "stage1_block2",
          "parameters": 262144,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_17",
          "type": "BatchNorm2d",
          "name": "stage1_block2_bn1",
          "layer": "stage1_block2",
          "parameters": 1024,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_18",
          "type": "BatchNorm2d",
          "name": "stage1_block2_bn2",
          "layer": "stage1_block2",
          "parameters": 1024,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_19",
          "type": "BatchNorm2d",
          "name": "stage1_block2_bn3",
          "layer": "stage1_block2",
          "parameters": 4096,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_20",
          "type": "ReLU",
          "name": "stage1_block2_relu",
          "layer": "stage1_block2",
          "parameters": 0,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_21",
          "type": "Conv2d",
          "name": "stage1_block3_conv1",
          "layer": "stage1_block3",
          "parameters": 32768,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_22",
          "type": "Conv2d",
          "name": "stage1_block3_conv2",
          "layer": "stage1_block3",
          "parameters": 589824,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_23",
          "type": "Conv2d",
          "name": "stage1_block3_conv3",
          "layer": "stage1_block3",
          "parameters": 262144,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_24",
          "type": "BatchNorm2d",
          "name": "stage1_block3_bn1",
          "layer": "stage1_block3",
          "parameters": 1024,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_25",
          "type": "BatchNorm2d",
          "name": "stage1_block3_bn2",
          "layer": "stage1_block3",
          "parameters": 1024,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_26",
          "type": "BatchNorm2d",
          "name": "stage1_block3_bn3",
          "layer": "stage1_block3",
          "parameters": 4096,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_27",
          "type": "ReLU",
          "name": "stage1_block3_relu",
          "layer": "stage1_block3",
          "parameters": 0,
          "input_shape": "[batch, 128, H, W]",
          "output_shape": "[batch, 128, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_28",
          "type": "Conv2d",
          "name": "stage2_block1_conv1",
          "layer": "stage2_block1",
          "parameters": 131072,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_29",
          "type": "Conv2d",
          "name": "stage2_block1_conv2",
          "layer": "stage2_block1",
          "parameters": 2359296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_30",
          "type": "Conv2d",
          "name": "stage2_block1_conv3",
          "layer": "stage2_block1",
          "parameters": 1048576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_31",
          "type": "BatchNorm2d",
          "name": "stage2_block1_bn1",
          "layer": "stage2_block1",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_32",
          "type": "BatchNorm2d",
          "name": "stage2_block1_bn2",
          "layer": "stage2_block1",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_33",
          "type": "BatchNorm2d",
          "name": "stage2_block1_bn3",
          "layer": "stage2_block1",
          "parameters": 8192,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_34",
          "type": "ReLU",
          "name": "stage2_block1_relu",
          "layer": "stage2_block1",
          "parameters": 0,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_35",
          "type": "Conv2d",
          "name": "stage2_block2_conv1",
          "layer": "stage2_block2",
          "parameters": 131072,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_36",
          "type": "Conv2d",
          "name": "stage2_block2_conv2",
          "layer": "stage2_block2",
          "parameters": 2359296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_37",
          "type": "Conv2d",
          "name": "stage2_block2_conv3",
          "layer": "stage2_block2",
          "parameters": 1048576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_38",
          "type": "BatchNorm2d",
          "name": "stage2_block2_bn1",
          "layer": "stage2_block2",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_39",
          "type": "BatchNorm2d",
          "name": "stage2_block2_bn2",
          "layer": "stage2_block2",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_40",
          "type": "BatchNorm2d",
          "name": "stage2_block2_bn3",
          "layer": "stage2_block2",
          "parameters": 8192,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_41",
          "type": "ReLU",
          "name": "stage2_block2_relu",
          "layer": "stage2_block2",
          "parameters": 0,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_42",
          "type": "Conv2d",
          "name": "stage2_block3_conv1",
          "layer": "stage2_block3",
          "parameters": 131072,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_43",
          "type": "Conv2d",
          "name": "stage2_block3_conv2",
          "layer": "stage2_block3",
          "parameters": 2359296,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_44",
          "type": "Conv2d",
          "name": "stage2_block3_conv3",
          "layer": "stage2_block3",
          "parameters": 1048576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_45",
          "type": "BatchNorm2d",
          "name": "stage2_block3_bn1",
          "layer": "stage2_block3",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_46",
          "type": "BatchNorm2d",
          "name": "stage2_block3_bn2",
          "layer": "stage2_block3",
          "parameters": 2048,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_47",
          "type": "BatchNorm2d",
          "name": "stage2_block3_bn3",
          "layer": "stage2_block3",
          "parameters": 8192,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_48",
          "type": "ReLU",
          "name": "stage2_block3_relu",
          "layer": "stage2_block3",
          "parameters": 0,
          "input_shape": "[batch, 256, H, W]",
          "output_shape": "[batch, 256, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_49",
          "type": "Conv2d",
          "name": "stage3_block1_conv1",
          "layer": "stage3_block1",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_50",
          "type": "Conv2d",
          "name": "stage3_block1_conv2",
          "layer": "stage3_block1",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_51",
          "type": "Conv2d",
          "name": "stage3_block1_conv3",
          "layer": "stage3_block1",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_52",
          "type": "BatchNorm2d",
          "name": "stage3_block1_bn1",
          "layer": "stage3_block1",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_53",
          "type": "BatchNorm2d",
          "name": "stage3_block1_bn2",
          "layer": "stage3_block1",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_54",
          "type": "BatchNorm2d",
          "name": "stage3_block1_bn3",
          "layer": "stage3_block1",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_55",
          "type": "ReLU",
          "name": "stage3_block1_relu",
          "layer": "stage3_block1",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_56",
          "type": "Conv2d",
          "name": "stage3_block2_conv1",
          "layer": "stage3_block2",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_57",
          "type": "Conv2d",
          "name": "stage3_block2_conv2",
          "layer": "stage3_block2",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_58",
          "type": "Conv2d",
          "name": "stage3_block2_conv3",
          "layer": "stage3_block2",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_59",
          "type": "BatchNorm2d",
          "name": "stage3_block2_bn1",
          "layer": "stage3_block2",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_60",
          "type": "BatchNorm2d",
          "name": "stage3_block2_bn2",
          "layer": "stage3_block2",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_61",
          "type": "BatchNorm2d",
          "name": "stage3_block2_bn3",
          "layer": "stage3_block2",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_62",
          "type": "ReLU",
          "name": "stage3_block2_relu",
          "layer": "stage3_block2",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_63",
          "type": "Conv2d",
          "name": "stage3_block3_conv1",
          "layer": "stage3_block3",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_64",
          "type": "Conv2d",
          "name": "stage3_block3_conv2",
          "layer": "stage3_block3",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_65",
          "type": "Conv2d",
          "name": "stage3_block3_conv3",
          "layer": "stage3_block3",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_66",
          "type": "BatchNorm2d",
          "name": "stage3_block3_bn1",
          "layer": "stage3_block3",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_67",
          "type": "BatchNorm2d",
          "name": "stage3_block3_bn2",
          "layer": "stage3_block3",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_68",
          "type": "BatchNorm2d",
          "name": "stage3_block3_bn3",
          "layer": "stage3_block3",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_69",
          "type": "ReLU",
          "name": "stage3_block3_relu",
          "layer": "stage3_block3",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_70",
          "type": "Conv2d",
          "name": "stage3_block4_conv1",
          "layer": "stage3_block4",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_71",
          "type": "Conv2d",
          "name": "stage3_block4_conv2",
          "layer": "stage3_block4",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_72",
          "type": "Conv2d",
          "name": "stage3_block4_conv3",
          "layer": "stage3_block4",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_73",
          "type": "BatchNorm2d",
          "name": "stage3_block4_bn1",
          "layer": "stage3_block4",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_74",
          "type": "BatchNorm2d",
          "name": "stage3_block4_bn2",
          "layer": "stage3_block4",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_75",
          "type": "BatchNorm2d",
          "name": "stage3_block4_bn3",
          "layer": "stage3_block4",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_76",
          "type": "ReLU",
          "name": "stage3_block4_relu",
          "layer": "stage3_block4",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_77",
          "type": "Conv2d",
          "name": "stage3_block5_conv1",
          "layer": "stage3_block5",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_78",
          "type": "Conv2d",
          "name": "stage3_block5_conv2",
          "layer": "stage3_block5",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_79",
          "type": "Conv2d",
          "name": "stage3_block5_conv3",
          "layer": "stage3_block5",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_80",
          "type": "BatchNorm2d",
          "name": "stage3_block5_bn1",
          "layer": "stage3_block5",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_81",
          "type": "BatchNorm2d",
          "name": "stage3_block5_bn2",
          "layer": "stage3_block5",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_82",
          "type": "BatchNorm2d",
          "name": "stage3_block5_bn3",
          "layer": "stage3_block5",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_83",
          "type": "ReLU",
          "name": "stage3_block5_relu",
          "layer": "stage3_block5",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_84",
          "type": "Conv2d",
          "name": "stage3_block6_conv1",
          "layer": "stage3_block6",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_85",
          "type": "Conv2d",
          "name": "stage3_block6_conv2",
          "layer": "stage3_block6",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_86",
          "type": "Conv2d",
          "name": "stage3_block6_conv3",
          "layer": "stage3_block6",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_87",
          "type": "BatchNorm2d",
          "name": "stage3_block6_bn1",
          "layer": "stage3_block6",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_88",
          "type": "BatchNorm2d",
          "name": "stage3_block6_bn2",
          "layer": "stage3_block6",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_89",
          "type": "BatchNorm2d",
          "name": "stage3_block6_bn3",
          "layer": "stage3_block6",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_90",
          "type": "ReLU",
          "name": "stage3_block6_relu",
          "layer": "stage3_block6",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_91",
          "type": "Conv2d",
          "name": "stage3_block7_conv1",
          "layer": "stage3_block7",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_92",
          "type": "Conv2d",
          "name": "stage3_block7_conv2",
          "layer": "stage3_block7",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_93",
          "type": "Conv2d",
          "name": "stage3_block7_conv3",
          "layer": "stage3_block7",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_94",
          "type": "BatchNorm2d",
          "name": "stage3_block7_bn1",
          "layer": "stage3_block7",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_95",
          "type": "BatchNorm2d",
          "name": "stage3_block7_bn2",
          "layer": "stage3_block7",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_96",
          "type": "BatchNorm2d",
          "name": "stage3_block7_bn3",
          "layer": "stage3_block7",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_97",
          "type": "ReLU",
          "name": "stage3_block7_relu",
          "layer": "stage3_block7",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_98",
          "type": "Conv2d",
          "name": "stage3_block8_conv1",
          "layer": "stage3_block8",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_99",
          "type": "Conv2d",
          "name": "stage3_block8_conv2",
          "layer": "stage3_block8",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_100",
          "type": "Conv2d",
          "name": "stage3_block8_conv3",
          "layer": "stage3_block8",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_101",
          "type": "BatchNorm2d",
          "name": "stage3_block8_bn1",
          "layer": "stage3_block8",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_102",
          "type": "BatchNorm2d",
          "name": "stage3_block8_bn2",
          "layer": "stage3_block8",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_103",
          "type": "BatchNorm2d",
          "name": "stage3_block8_bn3",
          "layer": "stage3_block8",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_104",
          "type": "ReLU",
          "name": "stage3_block8_relu",
          "layer": "stage3_block8",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_105",
          "type": "Conv2d",
          "name": "stage3_block9_conv1",
          "layer": "stage3_block9",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_106",
          "type": "Conv2d",
          "name": "stage3_block9_conv2",
          "layer": "stage3_block9",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_107",
          "type": "Conv2d",
          "name": "stage3_block9_conv3",
          "layer": "stage3_block9",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_108",
          "type": "BatchNorm2d",
          "name": "stage3_block9_bn1",
          "layer": "stage3_block9",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_109",
          "type": "BatchNorm2d",
          "name": "stage3_block9_bn2",
          "layer": "stage3_block9",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_110",
          "type": "BatchNorm2d",
          "name": "stage3_block9_bn3",
          "layer": "stage3_block9",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_111",
          "type": "ReLU",
          "name": "stage3_block9_relu",
          "layer": "stage3_block9",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_112",
          "type": "Conv2d",
          "name": "stage3_block10_conv1",
          "layer": "stage3_block10",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_113",
          "type": "Conv2d",
          "name": "stage3_block10_conv2",
          "layer": "stage3_block10",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_114",
          "type": "Conv2d",
          "name": "stage3_block10_conv3",
          "layer": "stage3_block10",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_115",
          "type": "BatchNorm2d",
          "name": "stage3_block10_bn1",
          "layer": "stage3_block10",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_116",
          "type": "BatchNorm2d",
          "name": "stage3_block10_bn2",
          "layer": "stage3_block10",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_117",
          "type": "BatchNorm2d",
          "name": "stage3_block10_bn3",
          "layer": "stage3_block10",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_118",
          "type": "ReLU",
          "name": "stage3_block10_relu",
          "layer": "stage3_block10",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_119",
          "type": "Conv2d",
          "name": "stage3_block11_conv1",
          "layer": "stage3_block11",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_120",
          "type": "Conv2d",
          "name": "stage3_block11_conv2",
          "layer": "stage3_block11",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_121",
          "type": "Conv2d",
          "name": "stage3_block11_conv3",
          "layer": "stage3_block11",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_122",
          "type": "BatchNorm2d",
          "name": "stage3_block11_bn1",
          "layer": "stage3_block11",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_123",
          "type": "BatchNorm2d",
          "name": "stage3_block11_bn2",
          "layer": "stage3_block11",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_124",
          "type": "BatchNorm2d",
          "name": "stage3_block11_bn3",
          "layer": "stage3_block11",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_125",
          "type": "ReLU",
          "name": "stage3_block11_relu",
          "layer": "stage3_block11",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_126",
          "type": "Conv2d",
          "name": "stage3_block12_conv1",
          "layer": "stage3_block12",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_127",
          "type": "Conv2d",
          "name": "stage3_block12_conv2",
          "layer": "stage3_block12",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_128",
          "type": "Conv2d",
          "name": "stage3_block12_conv3",
          "layer": "stage3_block12",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_129",
          "type": "BatchNorm2d",
          "name": "stage3_block12_bn1",
          "layer": "stage3_block12",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_130",
          "type": "BatchNorm2d",
          "name": "stage3_block12_bn2",
          "layer": "stage3_block12",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_131",
          "type": "BatchNorm2d",
          "name": "stage3_block12_bn3",
          "layer": "stage3_block12",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_132",
          "type": "ReLU",
          "name": "stage3_block12_relu",
          "layer": "stage3_block12",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_133",
          "type": "Conv2d",
          "name": "stage3_block13_conv1",
          "layer": "stage3_block13",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_134",
          "type": "Conv2d",
          "name": "stage3_block13_conv2",
          "layer": "stage3_block13",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_135",
          "type": "Conv2d",
          "name": "stage3_block13_conv3",
          "layer": "stage3_block13",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_136",
          "type": "BatchNorm2d",
          "name": "stage3_block13_bn1",
          "layer": "stage3_block13",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_137",
          "type": "BatchNorm2d",
          "name": "stage3_block13_bn2",
          "layer": "stage3_block13",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_138",
          "type": "BatchNorm2d",
          "name": "stage3_block13_bn3",
          "layer": "stage3_block13",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_139",
          "type": "ReLU",
          "name": "stage3_block13_relu",
          "layer": "stage3_block13",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_140",
          "type": "Conv2d",
          "name": "stage3_block14_conv1",
          "layer": "stage3_block14",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_141",
          "type": "Conv2d",
          "name": "stage3_block14_conv2",
          "layer": "stage3_block14",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_142",
          "type": "Conv2d",
          "name": "stage3_block14_conv3",
          "layer": "stage3_block14",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_143",
          "type": "BatchNorm2d",
          "name": "stage3_block14_bn1",
          "layer": "stage3_block14",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_144",
          "type": "BatchNorm2d",
          "name": "stage3_block14_bn2",
          "layer": "stage3_block14",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_145",
          "type": "BatchNorm2d",
          "name": "stage3_block14_bn3",
          "layer": "stage3_block14",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_146",
          "type": "ReLU",
          "name": "stage3_block14_relu",
          "layer": "stage3_block14",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_147",
          "type": "Conv2d",
          "name": "stage3_block15_conv1",
          "layer": "stage3_block15",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_148",
          "type": "Conv2d",
          "name": "stage3_block15_conv2",
          "layer": "stage3_block15",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_149",
          "type": "Conv2d",
          "name": "stage3_block15_conv3",
          "layer": "stage3_block15",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_150",
          "type": "BatchNorm2d",
          "name": "stage3_block15_bn1",
          "layer": "stage3_block15",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_151",
          "type": "BatchNorm2d",
          "name": "stage3_block15_bn2",
          "layer": "stage3_block15",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_152",
          "type": "BatchNorm2d",
          "name": "stage3_block15_bn3",
          "layer": "stage3_block15",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_153",
          "type": "ReLU",
          "name": "stage3_block15_relu",
          "layer": "stage3_block15",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_154",
          "type": "Conv2d",
          "name": "stage3_block16_conv1",
          "layer": "stage3_block16",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_155",
          "type": "Conv2d",
          "name": "stage3_block16_conv2",
          "layer": "stage3_block16",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_156",
          "type": "Conv2d",
          "name": "stage3_block16_conv3",
          "layer": "stage3_block16",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_157",
          "type": "BatchNorm2d",
          "name": "stage3_block16_bn1",
          "layer": "stage3_block16",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_158",
          "type": "BatchNorm2d",
          "name": "stage3_block16_bn2",
          "layer": "stage3_block16",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_159",
          "type": "BatchNorm2d",
          "name": "stage3_block16_bn3",
          "layer": "stage3_block16",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_160",
          "type": "ReLU",
          "name": "stage3_block16_relu",
          "layer": "stage3_block16",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_161",
          "type": "Conv2d",
          "name": "stage3_block17_conv1",
          "layer": "stage3_block17",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_162",
          "type": "Conv2d",
          "name": "stage3_block17_conv2",
          "layer": "stage3_block17",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_163",
          "type": "Conv2d",
          "name": "stage3_block17_conv3",
          "layer": "stage3_block17",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_164",
          "type": "BatchNorm2d",
          "name": "stage3_block17_bn1",
          "layer": "stage3_block17",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_165",
          "type": "BatchNorm2d",
          "name": "stage3_block17_bn2",
          "layer": "stage3_block17",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_166",
          "type": "BatchNorm2d",
          "name": "stage3_block17_bn3",
          "layer": "stage3_block17",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_167",
          "type": "ReLU",
          "name": "stage3_block17_relu",
          "layer": "stage3_block17",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_168",
          "type": "Conv2d",
          "name": "stage3_block18_conv1",
          "layer": "stage3_block18",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_169",
          "type": "Conv2d",
          "name": "stage3_block18_conv2",
          "layer": "stage3_block18",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_170",
          "type": "Conv2d",
          "name": "stage3_block18_conv3",
          "layer": "stage3_block18",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_171",
          "type": "BatchNorm2d",
          "name": "stage3_block18_bn1",
          "layer": "stage3_block18",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_172",
          "type": "BatchNorm2d",
          "name": "stage3_block18_bn2",
          "layer": "stage3_block18",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_173",
          "type": "BatchNorm2d",
          "name": "stage3_block18_bn3",
          "layer": "stage3_block18",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_174",
          "type": "ReLU",
          "name": "stage3_block18_relu",
          "layer": "stage3_block18",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_175",
          "type": "Conv2d",
          "name": "stage3_block19_conv1",
          "layer": "stage3_block19",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_176",
          "type": "Conv2d",
          "name": "stage3_block19_conv2",
          "layer": "stage3_block19",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_177",
          "type": "Conv2d",
          "name": "stage3_block19_conv3",
          "layer": "stage3_block19",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_178",
          "type": "BatchNorm2d",
          "name": "stage3_block19_bn1",
          "layer": "stage3_block19",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_179",
          "type": "BatchNorm2d",
          "name": "stage3_block19_bn2",
          "layer": "stage3_block19",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_180",
          "type": "BatchNorm2d",
          "name": "stage3_block19_bn3",
          "layer": "stage3_block19",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_181",
          "type": "ReLU",
          "name": "stage3_block19_relu",
          "layer": "stage3_block19",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_182",
          "type": "Conv2d",
          "name": "stage3_block20_conv1",
          "layer": "stage3_block20",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_183",
          "type": "Conv2d",
          "name": "stage3_block20_conv2",
          "layer": "stage3_block20",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_184",
          "type": "Conv2d",
          "name": "stage3_block20_conv3",
          "layer": "stage3_block20",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_185",
          "type": "BatchNorm2d",
          "name": "stage3_block20_bn1",
          "layer": "stage3_block20",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_186",
          "type": "BatchNorm2d",
          "name": "stage3_block20_bn2",
          "layer": "stage3_block20",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_187",
          "type": "BatchNorm2d",
          "name": "stage3_block20_bn3",
          "layer": "stage3_block20",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_188",
          "type": "ReLU",
          "name": "stage3_block20_relu",
          "layer": "stage3_block20",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_189",
          "type": "Conv2d",
          "name": "stage3_block21_conv1",
          "layer": "stage3_block21",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_190",
          "type": "Conv2d",
          "name": "stage3_block21_conv2",
          "layer": "stage3_block21",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_191",
          "type": "Conv2d",
          "name": "stage3_block21_conv3",
          "layer": "stage3_block21",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_192",
          "type": "BatchNorm2d",
          "name": "stage3_block21_bn1",
          "layer": "stage3_block21",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_193",
          "type": "BatchNorm2d",
          "name": "stage3_block21_bn2",
          "layer": "stage3_block21",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_194",
          "type": "BatchNorm2d",
          "name": "stage3_block21_bn3",
          "layer": "stage3_block21",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_195",
          "type": "ReLU",
          "name": "stage3_block21_relu",
          "layer": "stage3_block21",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_196",
          "type": "Conv2d",
          "name": "stage3_block22_conv1",
          "layer": "stage3_block22",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_197",
          "type": "Conv2d",
          "name": "stage3_block22_conv2",
          "layer": "stage3_block22",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_198",
          "type": "Conv2d",
          "name": "stage3_block22_conv3",
          "layer": "stage3_block22",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_199",
          "type": "BatchNorm2d",
          "name": "stage3_block22_bn1",
          "layer": "stage3_block22",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_200",
          "type": "BatchNorm2d",
          "name": "stage3_block22_bn2",
          "layer": "stage3_block22",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_201",
          "type": "BatchNorm2d",
          "name": "stage3_block22_bn3",
          "layer": "stage3_block22",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_202",
          "type": "ReLU",
          "name": "stage3_block22_relu",
          "layer": "stage3_block22",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_203",
          "type": "Conv2d",
          "name": "stage3_block23_conv1",
          "layer": "stage3_block23",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_204",
          "type": "Conv2d",
          "name": "stage3_block23_conv2",
          "layer": "stage3_block23",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_205",
          "type": "Conv2d",
          "name": "stage3_block23_conv3",
          "layer": "stage3_block23",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_206",
          "type": "BatchNorm2d",
          "name": "stage3_block23_bn1",
          "layer": "stage3_block23",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_207",
          "type": "BatchNorm2d",
          "name": "stage3_block23_bn2",
          "layer": "stage3_block23",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_208",
          "type": "BatchNorm2d",
          "name": "stage3_block23_bn3",
          "layer": "stage3_block23",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_209",
          "type": "ReLU",
          "name": "stage3_block23_relu",
          "layer": "stage3_block23",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_210",
          "type": "Conv2d",
          "name": "stage3_block24_conv1",
          "layer": "stage3_block24",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_211",
          "type": "Conv2d",
          "name": "stage3_block24_conv2",
          "layer": "stage3_block24",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_212",
          "type": "Conv2d",
          "name": "stage3_block24_conv3",
          "layer": "stage3_block24",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_213",
          "type": "BatchNorm2d",
          "name": "stage3_block24_bn1",
          "layer": "stage3_block24",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_214",
          "type": "BatchNorm2d",
          "name": "stage3_block24_bn2",
          "layer": "stage3_block24",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_215",
          "type": "BatchNorm2d",
          "name": "stage3_block24_bn3",
          "layer": "stage3_block24",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_216",
          "type": "ReLU",
          "name": "stage3_block24_relu",
          "layer": "stage3_block24",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_217",
          "type": "Conv2d",
          "name": "stage3_block25_conv1",
          "layer": "stage3_block25",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_218",
          "type": "Conv2d",
          "name": "stage3_block25_conv2",
          "layer": "stage3_block25",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_219",
          "type": "Conv2d",
          "name": "stage3_block25_conv3",
          "layer": "stage3_block25",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_220",
          "type": "BatchNorm2d",
          "name": "stage3_block25_bn1",
          "layer": "stage3_block25",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_221",
          "type": "BatchNorm2d",
          "name": "stage3_block25_bn2",
          "layer": "stage3_block25",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_222",
          "type": "BatchNorm2d",
          "name": "stage3_block25_bn3",
          "layer": "stage3_block25",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_223",
          "type": "ReLU",
          "name": "stage3_block25_relu",
          "layer": "stage3_block25",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_224",
          "type": "Conv2d",
          "name": "stage3_block26_conv1",
          "layer": "stage3_block26",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_225",
          "type": "Conv2d",
          "name": "stage3_block26_conv2",
          "layer": "stage3_block26",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_226",
          "type": "Conv2d",
          "name": "stage3_block26_conv3",
          "layer": "stage3_block26",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_227",
          "type": "BatchNorm2d",
          "name": "stage3_block26_bn1",
          "layer": "stage3_block26",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_228",
          "type": "BatchNorm2d",
          "name": "stage3_block26_bn2",
          "layer": "stage3_block26",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_229",
          "type": "BatchNorm2d",
          "name": "stage3_block26_bn3",
          "layer": "stage3_block26",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_230",
          "type": "ReLU",
          "name": "stage3_block26_relu",
          "layer": "stage3_block26",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_231",
          "type": "Conv2d",
          "name": "stage3_block27_conv1",
          "layer": "stage3_block27",
          "parameters": 524288,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_232",
          "type": "Conv2d",
          "name": "stage3_block27_conv2",
          "layer": "stage3_block27",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_233",
          "type": "Conv2d",
          "name": "stage3_block27_conv3",
          "layer": "stage3_block27",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_234",
          "type": "BatchNorm2d",
          "name": "stage3_block27_bn1",
          "layer": "stage3_block27",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_235",
          "type": "BatchNorm2d",
          "name": "stage3_block27_bn2",
          "layer": "stage3_block27",
          "parameters": 4096,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_236",
          "type": "BatchNorm2d",
          "name": "stage3_block27_bn3",
          "layer": "stage3_block27",
          "parameters": 16384,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_237",
          "type": "ReLU",
          "name": "stage3_block27_relu",
          "layer": "stage3_block27",
          "parameters": 0,
          "input_shape": "[batch, 512, H, W]",
          "output_shape": "[batch, 512, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_238",
          "type": "Conv2d",
          "name": "stage4_block1_conv1",
          "layer": "stage4_block1",
          "parameters": 1048576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_239",
          "type": "Conv2d",
          "name": "stage4_block1_conv2",
          "layer": "stage4_block1",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_240",
          "type": "Conv2d",
          "name": "stage4_block1_conv3",
          "layer": "stage4_block1",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_241",
          "type": "BatchNorm2d",
          "name": "stage4_block1_bn1",
          "layer": "stage4_block1",
          "parameters": 4096,
          "input_shape": "[batch, 1024, H, W]",
          "output_shape": "[batch, 1024, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_242",
          "type": "BatchNorm2d",
          "name": "stage4_block1_bn2",
          "layer": "stage4_block1",
          "parameters": 4096,
          "input_shape": "[batch, 1024, H, W]",
          "output_shape": "[batch, 1024, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_243",
          "type": "BatchNorm2d",
          "name": "stage4_block1_bn3",
          "layer": "stage4_block1",
          "parameters": 16384,
          "input_shape": "[batch, 1024, H, W]",
          "output_shape": "[batch, 1024, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_244",
          "type": "ReLU",
          "name": "stage4_block1_relu",
          "layer": "stage4_block1",
          "parameters": 0,
          "input_shape": "[batch, 1024, H, W]",
          "output_shape": "[batch, 1024, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_245",
          "type": "Conv2d",
          "name": "stage4_block2_conv1",
          "layer": "stage4_block2",
          "parameters": 1048576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_246",
          "type": "Conv2d",
          "name": "stage4_block2_conv2",
          "layer": "stage4_block2",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_247",
          "type": "Conv2d",
          "name": "stage4_block2_conv3",
          "layer": "stage4_block2",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_248",
          "type": "BatchNorm2d",
          "name": "stage4_block2_bn1",
          "layer": "stage4_block2",
          "parameters": 4096,
          "input_shape": "[batch, 1024, H, W]",
          "output_shape": "[batch, 1024, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_249",
          "type": "BatchNorm2d",
          "name": "stage4_block2_bn2",
          "layer": "stage4_block2",
          "parameters": 4096,
          "input_shape": "[batch, 1024, H, W]",
          "output_shape": "[batch, 1024, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_250",
          "type": "BatchNorm2d",
          "name": "stage4_block2_bn3",
          "layer": "stage4_block2",
          "parameters": 16384,
          "input_shape": "[batch, 1024, H, W]",
          "output_shape": "[batch, 1024, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_251",
          "type": "ReLU",
          "name": "stage4_block2_relu",
          "layer": "stage4_block2",
          "parameters": 0,
          "input_shape": "[batch, 1024, H, W]",
          "output_shape": "[batch, 1024, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_252",
          "type": "Conv2d",
          "name": "stage4_block3_conv1",
          "layer": "stage4_block3",
          "parameters": 1048576,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_253",
          "type": "Conv2d",
          "name": "stage4_block3_conv2",
          "layer": "stage4_block3",
          "parameters": 9437184,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_254",
          "type": "Conv2d",
          "name": "stage4_block3_conv3",
          "layer": "stage4_block3",
          "parameters": 4194304,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "ConvNeXt_Base_op_255",
          "type": "BatchNorm2d",
          "name": "stage4_block3_bn1",
          "layer": "stage4_block3",
          "parameters": 4096,
          "input_shape": "[batch, 1024, H, W]",
          "output_shape": "[batch, 1024, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_256",
          "type": "BatchNorm2d",
          "name": "stage4_block3_bn2",
          "layer": "stage4_block3",
          "parameters": 4096,
          "input_shape": "[batch, 1024, H, W]",
          "output_shape": "[batch, 1024, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_257",
          "type": "BatchNorm2d",
          "name": "stage4_block3_bn3",
          "layer": "stage4_block3",
          "parameters": 16384,
          "input_shape": "[batch, 1024, H, W]",
          "output_shape": "[batch, 1024, H, W]"
        },
        {
          "op_id": "ConvNeXt_Base_op_258",
          "type": "ReLU",
          "name": "stage4_block3_relu",
          "layer": "stage4_block3",
          "parameters": 0,
          "input_shape": "[batch, 1024, H, W]",
          "output_shape": "[batch, 1024, H, W]"
        }
      ],
      "parameters": 89000000
    },
    {
      "model_id": "RegNet_Y_4GF",
      "name": "RegNet-Y-4GF",
      "source": "huggingface/regnet-y-4gf",
      "category": "CV",
      "architecture": "resnet_style",
      "num_params": 20600000,
      "num_layers": 22,
      "hidden_size": 768,
      "vocab_size": 0,
      "intermediate_size": 3072,
      "operator_count": 160,
      "operators": [
        {
          "op_id": "RegNet_Y_4GF_op_1",
          "type": "Conv2d",
          "name": "conv1",
          "layer": "global",
          "parameters": 9408,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_2",
          "type": "BatchNorm2d",
          "name": "bn1",
          "layer": "global",
          "parameters": 256,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_3",
          "type": "ReLU",
          "name": "relu",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_4",
          "type": "MaxPool2d",
          "name": "maxpool",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, H/2, W/2]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_5",
          "type": "AdaptiveAvgPool2d",
          "name": "avgpool",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, 768, H, W]",
          "output_shape": "[batch, 768, 1, 1]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_6",
          "type": "Linear",
          "name": "fc",
          "layer": "global",
          "parameters": 2048000,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_7",
          "type": "Conv2d",
          "name": "stage1_block1_conv1",
          "layer": "stage1_block1",
          "parameters": 4992,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_8",
          "type": "Conv2d",
          "name": "stage1_block1_conv2",
          "layer": "stage1_block1",
          "parameters": 97344,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_9",
          "type": "Conv2d",
          "name": "stage1_block1_conv3",
          "layer": "stage1_block1",
          "parameters": 43264,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_10",
          "type": "BatchNorm2d",
          "name": "stage1_block1_bn1",
          "layer": "stage1_block1",
          "parameters": 416,
          "input_shape": "[batch, 48, H, W]",
          "output_shape": "[batch, 48, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_11",
          "type": "BatchNorm2d",
          "name": "stage1_block1_bn2",
          "layer": "stage1_block1",
          "parameters": 416,
          "input_shape": "[batch, 48, H, W]",
          "output_shape": "[batch, 48, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_12",
          "type": "BatchNorm2d",
          "name": "stage1_block1_bn3",
          "layer": "stage1_block1",
          "parameters": 1664,
          "input_shape": "[batch, 48, H, W]",
          "output_shape": "[batch, 48, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_13",
          "type": "ReLU",
          "name": "stage1_block1_relu",
          "layer": "stage1_block1",
          "parameters": 0,
          "input_shape": "[batch, 48, H, W]",
          "output_shape": "[batch, 48, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_14",
          "type": "Conv2d",
          "name": "stage1_block2_conv1",
          "layer": "stage1_block2",
          "parameters": 4992,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_15",
          "type": "Conv2d",
          "name": "stage1_block2_conv2",
          "layer": "stage1_block2",
          "parameters": 97344,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_16",
          "type": "Conv2d",
          "name": "stage1_block2_conv3",
          "layer": "stage1_block2",
          "parameters": 43264,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_17",
          "type": "BatchNorm2d",
          "name": "stage1_block2_bn1",
          "layer": "stage1_block2",
          "parameters": 416,
          "input_shape": "[batch, 48, H, W]",
          "output_shape": "[batch, 48, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_18",
          "type": "BatchNorm2d",
          "name": "stage1_block2_bn2",
          "layer": "stage1_block2",
          "parameters": 416,
          "input_shape": "[batch, 48, H, W]",
          "output_shape": "[batch, 48, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_19",
          "type": "BatchNorm2d",
          "name": "stage1_block2_bn3",
          "layer": "stage1_block2",
          "parameters": 1664,
          "input_shape": "[batch, 48, H, W]",
          "output_shape": "[batch, 48, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_20",
          "type": "ReLU",
          "name": "stage1_block2_relu",
          "layer": "stage1_block2",
          "parameters": 0,
          "input_shape": "[batch, 48, H, W]",
          "output_shape": "[batch, 48, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_21",
          "type": "Conv2d",
          "name": "stage2_block1_conv1",
          "layer": "stage2_block1",
          "parameters": 21632,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_22",
          "type": "Conv2d",
          "name": "stage2_block1_conv2",
          "layer": "stage2_block1",
          "parameters": 389376,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_23",
          "type": "Conv2d",
          "name": "stage2_block1_conv3",
          "layer": "stage2_block1",
          "parameters": 173056,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_24",
          "type": "BatchNorm2d",
          "name": "stage2_block1_bn1",
          "layer": "stage2_block1",
          "parameters": 832,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_25",
          "type": "BatchNorm2d",
          "name": "stage2_block1_bn2",
          "layer": "stage2_block1",
          "parameters": 832,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_26",
          "type": "BatchNorm2d",
          "name": "stage2_block1_bn3",
          "layer": "stage2_block1",
          "parameters": 3328,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_27",
          "type": "ReLU",
          "name": "stage2_block1_relu",
          "layer": "stage2_block1",
          "parameters": 0,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_28",
          "type": "Conv2d",
          "name": "stage2_block2_conv1",
          "layer": "stage2_block2",
          "parameters": 21632,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_29",
          "type": "Conv2d",
          "name": "stage2_block2_conv2",
          "layer": "stage2_block2",
          "parameters": 389376,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_30",
          "type": "Conv2d",
          "name": "stage2_block2_conv3",
          "layer": "stage2_block2",
          "parameters": 173056,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_31",
          "type": "BatchNorm2d",
          "name": "stage2_block2_bn1",
          "layer": "stage2_block2",
          "parameters": 832,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_32",
          "type": "BatchNorm2d",
          "name": "stage2_block2_bn2",
          "layer": "stage2_block2",
          "parameters": 832,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_33",
          "type": "BatchNorm2d",
          "name": "stage2_block2_bn3",
          "layer": "stage2_block2",
          "parameters": 3328,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_34",
          "type": "ReLU",
          "name": "stage2_block2_relu",
          "layer": "stage2_block2",
          "parameters": 0,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_35",
          "type": "Conv2d",
          "name": "stage2_block3_conv1",
          "layer": "stage2_block3",
          "parameters": 21632,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_36",
          "type": "Conv2d",
          "name": "stage2_block3_conv2",
          "layer": "stage2_block3",
          "parameters": 389376,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_37",
          "type": "Conv2d",
          "name": "stage2_block3_conv3",
          "layer": "stage2_block3",
          "parameters": 173056,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_38",
          "type": "BatchNorm2d",
          "name": "stage2_block3_bn1",
          "layer": "stage2_block3",
          "parameters": 832,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_39",
          "type": "BatchNorm2d",
          "name": "stage2_block3_bn2",
          "layer": "stage2_block3",
          "parameters": 832,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_40",
          "type": "BatchNorm2d",
          "name": "stage2_block3_bn3",
          "layer": "stage2_block3",
          "parameters": 3328,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_41",
          "type": "ReLU",
          "name": "stage2_block3_relu",
          "layer": "stage2_block3",
          "parameters": 0,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_42",
          "type": "Conv2d",
          "name": "stage2_block4_conv1",
          "layer": "stage2_block4",
          "parameters": 21632,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_43",
          "type": "Conv2d",
          "name": "stage2_block4_conv2",
          "layer": "stage2_block4",
          "parameters": 389376,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_44",
          "type": "Conv2d",
          "name": "stage2_block4_conv3",
          "layer": "stage2_block4",
          "parameters": 173056,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_45",
          "type": "BatchNorm2d",
          "name": "stage2_block4_bn1",
          "layer": "stage2_block4",
          "parameters": 832,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_46",
          "type": "BatchNorm2d",
          "name": "stage2_block4_bn2",
          "layer": "stage2_block4",
          "parameters": 832,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_47",
          "type": "BatchNorm2d",
          "name": "stage2_block4_bn3",
          "layer": "stage2_block4",
          "parameters": 3328,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_48",
          "type": "ReLU",
          "name": "stage2_block4_relu",
          "layer": "stage2_block4",
          "parameters": 0,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_49",
          "type": "Conv2d",
          "name": "stage2_block5_conv1",
          "layer": "stage2_block5",
          "parameters": 21632,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_50",
          "type": "Conv2d",
          "name": "stage2_block5_conv2",
          "layer": "stage2_block5",
          "parameters": 389376,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_51",
          "type": "Conv2d",
          "name": "stage2_block5_conv3",
          "layer": "stage2_block5",
          "parameters": 173056,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_52",
          "type": "BatchNorm2d",
          "name": "stage2_block5_bn1",
          "layer": "stage2_block5",
          "parameters": 832,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_53",
          "type": "BatchNorm2d",
          "name": "stage2_block5_bn2",
          "layer": "stage2_block5",
          "parameters": 832,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_54",
          "type": "BatchNorm2d",
          "name": "stage2_block5_bn3",
          "layer": "stage2_block5",
          "parameters": 3328,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_55",
          "type": "ReLU",
          "name": "stage2_block5_relu",
          "layer": "stage2_block5",
          "parameters": 0,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_56",
          "type": "Conv2d",
          "name": "stage2_block6_conv1",
          "layer": "stage2_block6",
          "parameters": 21632,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_57",
          "type": "Conv2d",
          "name": "stage2_block6_conv2",
          "layer": "stage2_block6",
          "parameters": 389376,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_58",
          "type": "Conv2d",
          "name": "stage2_block6_conv3",
          "layer": "stage2_block6",
          "parameters": 173056,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_59",
          "type": "BatchNorm2d",
          "name": "stage2_block6_bn1",
          "layer": "stage2_block6",
          "parameters": 832,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_60",
          "type": "BatchNorm2d",
          "name": "stage2_block6_bn2",
          "layer": "stage2_block6",
          "parameters": 832,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_61",
          "type": "BatchNorm2d",
          "name": "stage2_block6_bn3",
          "layer": "stage2_block6",
          "parameters": 3328,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_62",
          "type": "ReLU",
          "name": "stage2_block6_relu",
          "layer": "stage2_block6",
          "parameters": 0,
          "input_shape": "[batch, 104, H, W]",
          "output_shape": "[batch, 104, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_63",
          "type": "Conv2d",
          "name": "stage3_block1_conv1",
          "layer": "stage3_block1",
          "parameters": 91520,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_64",
          "type": "Conv2d",
          "name": "stage3_block1_conv2",
          "layer": "stage3_block1",
          "parameters": 1742400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_65",
          "type": "Conv2d",
          "name": "stage3_block1_conv3",
          "layer": "stage3_block1",
          "parameters": 774400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_66",
          "type": "BatchNorm2d",
          "name": "stage3_block1_bn1",
          "layer": "stage3_block1",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_67",
          "type": "BatchNorm2d",
          "name": "stage3_block1_bn2",
          "layer": "stage3_block1",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_68",
          "type": "BatchNorm2d",
          "name": "stage3_block1_bn3",
          "layer": "stage3_block1",
          "parameters": 7040,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_69",
          "type": "ReLU",
          "name": "stage3_block1_relu",
          "layer": "stage3_block1",
          "parameters": 0,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_70",
          "type": "Conv2d",
          "name": "stage3_block2_conv1",
          "layer": "stage3_block2",
          "parameters": 91520,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_71",
          "type": "Conv2d",
          "name": "stage3_block2_conv2",
          "layer": "stage3_block2",
          "parameters": 1742400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_72",
          "type": "Conv2d",
          "name": "stage3_block2_conv3",
          "layer": "stage3_block2",
          "parameters": 774400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_73",
          "type": "BatchNorm2d",
          "name": "stage3_block2_bn1",
          "layer": "stage3_block2",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_74",
          "type": "BatchNorm2d",
          "name": "stage3_block2_bn2",
          "layer": "stage3_block2",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_75",
          "type": "BatchNorm2d",
          "name": "stage3_block2_bn3",
          "layer": "stage3_block2",
          "parameters": 7040,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_76",
          "type": "ReLU",
          "name": "stage3_block2_relu",
          "layer": "stage3_block2",
          "parameters": 0,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_77",
          "type": "Conv2d",
          "name": "stage3_block3_conv1",
          "layer": "stage3_block3",
          "parameters": 91520,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_78",
          "type": "Conv2d",
          "name": "stage3_block3_conv2",
          "layer": "stage3_block3",
          "parameters": 1742400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_79",
          "type": "Conv2d",
          "name": "stage3_block3_conv3",
          "layer": "stage3_block3",
          "parameters": 774400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_80",
          "type": "BatchNorm2d",
          "name": "stage3_block3_bn1",
          "layer": "stage3_block3",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_81",
          "type": "BatchNorm2d",
          "name": "stage3_block3_bn2",
          "layer": "stage3_block3",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_82",
          "type": "BatchNorm2d",
          "name": "stage3_block3_bn3",
          "layer": "stage3_block3",
          "parameters": 7040,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_83",
          "type": "ReLU",
          "name": "stage3_block3_relu",
          "layer": "stage3_block3",
          "parameters": 0,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_84",
          "type": "Conv2d",
          "name": "stage3_block4_conv1",
          "layer": "stage3_block4",
          "parameters": 91520,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_85",
          "type": "Conv2d",
          "name": "stage3_block4_conv2",
          "layer": "stage3_block4",
          "parameters": 1742400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_86",
          "type": "Conv2d",
          "name": "stage3_block4_conv3",
          "layer": "stage3_block4",
          "parameters": 774400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_87",
          "type": "BatchNorm2d",
          "name": "stage3_block4_bn1",
          "layer": "stage3_block4",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_88",
          "type": "BatchNorm2d",
          "name": "stage3_block4_bn2",
          "layer": "stage3_block4",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_89",
          "type": "BatchNorm2d",
          "name": "stage3_block4_bn3",
          "layer": "stage3_block4",
          "parameters": 7040,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_90",
          "type": "ReLU",
          "name": "stage3_block4_relu",
          "layer": "stage3_block4",
          "parameters": 0,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_91",
          "type": "Conv2d",
          "name": "stage3_block5_conv1",
          "layer": "stage3_block5",
          "parameters": 91520,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_92",
          "type": "Conv2d",
          "name": "stage3_block5_conv2",
          "layer": "stage3_block5",
          "parameters": 1742400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_93",
          "type": "Conv2d",
          "name": "stage3_block5_conv3",
          "layer": "stage3_block5",
          "parameters": 774400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_94",
          "type": "BatchNorm2d",
          "name": "stage3_block5_bn1",
          "layer": "stage3_block5",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_95",
          "type": "BatchNorm2d",
          "name": "stage3_block5_bn2",
          "layer": "stage3_block5",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_96",
          "type": "BatchNorm2d",
          "name": "stage3_block5_bn3",
          "layer": "stage3_block5",
          "parameters": 7040,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_97",
          "type": "ReLU",
          "name": "stage3_block5_relu",
          "layer": "stage3_block5",
          "parameters": 0,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_98",
          "type": "Conv2d",
          "name": "stage3_block6_conv1",
          "layer": "stage3_block6",
          "parameters": 91520,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_99",
          "type": "Conv2d",
          "name": "stage3_block6_conv2",
          "layer": "stage3_block6",
          "parameters": 1742400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_100",
          "type": "Conv2d",
          "name": "stage3_block6_conv3",
          "layer": "stage3_block6",
          "parameters": 774400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_101",
          "type": "BatchNorm2d",
          "name": "stage3_block6_bn1",
          "layer": "stage3_block6",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_102",
          "type": "BatchNorm2d",
          "name": "stage3_block6_bn2",
          "layer": "stage3_block6",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_103",
          "type": "BatchNorm2d",
          "name": "stage3_block6_bn3",
          "layer": "stage3_block6",
          "parameters": 7040,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_104",
          "type": "ReLU",
          "name": "stage3_block6_relu",
          "layer": "stage3_block6",
          "parameters": 0,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_105",
          "type": "Conv2d",
          "name": "stage3_block7_conv1",
          "layer": "stage3_block7",
          "parameters": 91520,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_106",
          "type": "Conv2d",
          "name": "stage3_block7_conv2",
          "layer": "stage3_block7",
          "parameters": 1742400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_107",
          "type": "Conv2d",
          "name": "stage3_block7_conv3",
          "layer": "stage3_block7",
          "parameters": 774400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_108",
          "type": "BatchNorm2d",
          "name": "stage3_block7_bn1",
          "layer": "stage3_block7",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_109",
          "type": "BatchNorm2d",
          "name": "stage3_block7_bn2",
          "layer": "stage3_block7",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_110",
          "type": "BatchNorm2d",
          "name": "stage3_block7_bn3",
          "layer": "stage3_block7",
          "parameters": 7040,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_111",
          "type": "ReLU",
          "name": "stage3_block7_relu",
          "layer": "stage3_block7",
          "parameters": 0,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_112",
          "type": "Conv2d",
          "name": "stage3_block8_conv1",
          "layer": "stage3_block8",
          "parameters": 91520,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_113",
          "type": "Conv2d",
          "name": "stage3_block8_conv2",
          "layer": "stage3_block8",
          "parameters": 1742400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_114",
          "type": "Conv2d",
          "name": "stage3_block8_conv3",
          "layer": "stage3_block8",
          "parameters": 774400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_115",
          "type": "BatchNorm2d",
          "name": "stage3_block8_bn1",
          "layer": "stage3_block8",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_116",
          "type": "BatchNorm2d",
          "name": "stage3_block8_bn2",
          "layer": "stage3_block8",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_117",
          "type": "BatchNorm2d",
          "name": "stage3_block8_bn3",
          "layer": "stage3_block8",
          "parameters": 7040,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_118",
          "type": "ReLU",
          "name": "stage3_block8_relu",
          "layer": "stage3_block8",
          "parameters": 0,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_119",
          "type": "Conv2d",
          "name": "stage3_block9_conv1",
          "layer": "stage3_block9",
          "parameters": 91520,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_120",
          "type": "Conv2d",
          "name": "stage3_block9_conv2",
          "layer": "stage3_block9",
          "parameters": 1742400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_121",
          "type": "Conv2d",
          "name": "stage3_block9_conv3",
          "layer": "stage3_block9",
          "parameters": 774400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_122",
          "type": "BatchNorm2d",
          "name": "stage3_block9_bn1",
          "layer": "stage3_block9",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_123",
          "type": "BatchNorm2d",
          "name": "stage3_block9_bn2",
          "layer": "stage3_block9",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_124",
          "type": "BatchNorm2d",
          "name": "stage3_block9_bn3",
          "layer": "stage3_block9",
          "parameters": 7040,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_125",
          "type": "ReLU",
          "name": "stage3_block9_relu",
          "layer": "stage3_block9",
          "parameters": 0,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_126",
          "type": "Conv2d",
          "name": "stage3_block10_conv1",
          "layer": "stage3_block10",
          "parameters": 91520,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_127",
          "type": "Conv2d",
          "name": "stage3_block10_conv2",
          "layer": "stage3_block10",
          "parameters": 1742400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_128",
          "type": "Conv2d",
          "name": "stage3_block10_conv3",
          "layer": "stage3_block10",
          "parameters": 774400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_129",
          "type": "BatchNorm2d",
          "name": "stage3_block10_bn1",
          "layer": "stage3_block10",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_130",
          "type": "BatchNorm2d",
          "name": "stage3_block10_bn2",
          "layer": "stage3_block10",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_131",
          "type": "BatchNorm2d",
          "name": "stage3_block10_bn3",
          "layer": "stage3_block10",
          "parameters": 7040,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_132",
          "type": "ReLU",
          "name": "stage3_block10_relu",
          "layer": "stage3_block10",
          "parameters": 0,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_133",
          "type": "Conv2d",
          "name": "stage3_block11_conv1",
          "layer": "stage3_block11",
          "parameters": 91520,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_134",
          "type": "Conv2d",
          "name": "stage3_block11_conv2",
          "layer": "stage3_block11",
          "parameters": 1742400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_135",
          "type": "Conv2d",
          "name": "stage3_block11_conv3",
          "layer": "stage3_block11",
          "parameters": 774400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_136",
          "type": "BatchNorm2d",
          "name": "stage3_block11_bn1",
          "layer": "stage3_block11",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_137",
          "type": "BatchNorm2d",
          "name": "stage3_block11_bn2",
          "layer": "stage3_block11",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_138",
          "type": "BatchNorm2d",
          "name": "stage3_block11_bn3",
          "layer": "stage3_block11",
          "parameters": 7040,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_139",
          "type": "ReLU",
          "name": "stage3_block11_relu",
          "layer": "stage3_block11",
          "parameters": 0,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_140",
          "type": "Conv2d",
          "name": "stage3_block12_conv1",
          "layer": "stage3_block12",
          "parameters": 91520,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_141",
          "type": "Conv2d",
          "name": "stage3_block12_conv2",
          "layer": "stage3_block12",
          "parameters": 1742400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_142",
          "type": "Conv2d",
          "name": "stage3_block12_conv3",
          "layer": "stage3_block12",
          "parameters": 774400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_143",
          "type": "BatchNorm2d",
          "name": "stage3_block12_bn1",
          "layer": "stage3_block12",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_144",
          "type": "BatchNorm2d",
          "name": "stage3_block12_bn2",
          "layer": "stage3_block12",
          "parameters": 1760,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_145",
          "type": "BatchNorm2d",
          "name": "stage3_block12_bn3",
          "layer": "stage3_block12",
          "parameters": 7040,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_146",
          "type": "ReLU",
          "name": "stage3_block12_relu",
          "layer": "stage3_block12",
          "parameters": 0,
          "input_shape": "[batch, 208, H, W]",
          "output_shape": "[batch, 208, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_147",
          "type": "Conv2d",
          "name": "stage4_block1_conv1",
          "layer": "stage4_block1",
          "parameters": 193600,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_148",
          "type": "Conv2d",
          "name": "stage4_block1_conv2",
          "layer": "stage4_block1",
          "parameters": 1742400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_149",
          "type": "Conv2d",
          "name": "stage4_block1_conv3",
          "layer": "stage4_block1",
          "parameters": 774400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_150",
          "type": "BatchNorm2d",
          "name": "stage4_block1_bn1",
          "layer": "stage4_block1",
          "parameters": 1760,
          "input_shape": "[batch, 440, H, W]",
          "output_shape": "[batch, 440, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_151",
          "type": "BatchNorm2d",
          "name": "stage4_block1_bn2",
          "layer": "stage4_block1",
          "parameters": 1760,
          "input_shape": "[batch, 440, H, W]",
          "output_shape": "[batch, 440, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_152",
          "type": "BatchNorm2d",
          "name": "stage4_block1_bn3",
          "layer": "stage4_block1",
          "parameters": 7040,
          "input_shape": "[batch, 440, H, W]",
          "output_shape": "[batch, 440, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_153",
          "type": "ReLU",
          "name": "stage4_block1_relu",
          "layer": "stage4_block1",
          "parameters": 0,
          "input_shape": "[batch, 440, H, W]",
          "output_shape": "[batch, 440, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_154",
          "type": "Conv2d",
          "name": "stage4_block2_conv1",
          "layer": "stage4_block2",
          "parameters": 193600,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_155",
          "type": "Conv2d",
          "name": "stage4_block2_conv2",
          "layer": "stage4_block2",
          "parameters": 1742400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_156",
          "type": "Conv2d",
          "name": "stage4_block2_conv3",
          "layer": "stage4_block2",
          "parameters": 774400,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "RegNet_Y_4GF_op_157",
          "type": "BatchNorm2d",
          "name": "stage4_block2_bn1",
          "layer": "stage4_block2",
          "parameters": 1760,
          "input_shape": "[batch, 440, H, W]",
          "output_shape": "[batch, 440, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_158",
          "type": "BatchNorm2d",
          "name": "stage4_block2_bn2",
          "layer": "stage4_block2",
          "parameters": 1760,
          "input_shape": "[batch, 440, H, W]",
          "output_shape": "[batch, 440, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_159",
          "type": "BatchNorm2d",
          "name": "stage4_block2_bn3",
          "layer": "stage4_block2",
          "parameters": 7040,
          "input_shape": "[batch, 440, H, W]",
          "output_shape": "[batch, 440, H, W]"
        },
        {
          "op_id": "RegNet_Y_4GF_op_160",
          "type": "ReLU",
          "name": "stage4_block2_relu",
          "layer": "stage4_block2",
          "parameters": 0,
          "input_shape": "[batch, 440, H, W]",
          "output_shape": "[batch, 440, H, W]"
        }
      ],
      "parameters": 21000000
    },
    {
      "model_id": "BEiT_Base",
      "name": "BEiT-Base",
      "source": "huggingface/beit-base",
      "category": "CV",
      "architecture": "vit_style",
      "num_params": 86000000,
      "num_layers": 12,
      "hidden_size": 768,
      "vocab_size": 0,
      "intermediate_size": 3072,
      "operator_count": 123,
      "operators": [
        {
          "op_id": "BEiT_Base_op_1",
          "type": "Conv2d",
          "name": "patch_embed",
          "layer": "global",
          "parameters": 2073600000000,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "BEiT_Base_op_2",
          "type": "LayerNorm",
          "name": "norm",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_3",
          "type": "Linear",
          "name": "head",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_4",
          "type": "Linear",
          "name": "layer0_qkv",
          "layer": "layer_0",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_5",
          "type": "Linear",
          "name": "layer0_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_6",
          "type": "Linear",
          "name": "layer0_fc1",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_7",
          "type": "Linear",
          "name": "layer0_fc2",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_8",
          "type": "LayerNorm",
          "name": "layer0_norm1",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_9",
          "type": "LayerNorm",
          "name": "layer0_norm2",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_10",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BEiT_Base_op_11",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BEiT_Base_op_12",
          "type": "Dropout",
          "name": "layer0_attn_drop",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_13",
          "type": "Dropout",
          "name": "layer0_proj_drop",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_14",
          "type": "Linear",
          "name": "layer1_qkv",
          "layer": "layer_1",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_15",
          "type": "Linear",
          "name": "layer1_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_16",
          "type": "Linear",
          "name": "layer1_fc1",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_17",
          "type": "Linear",
          "name": "layer1_fc2",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_18",
          "type": "LayerNorm",
          "name": "layer1_norm1",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_19",
          "type": "LayerNorm",
          "name": "layer1_norm2",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_20",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BEiT_Base_op_21",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BEiT_Base_op_22",
          "type": "Dropout",
          "name": "layer1_attn_drop",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_23",
          "type": "Dropout",
          "name": "layer1_proj_drop",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_24",
          "type": "Linear",
          "name": "layer2_qkv",
          "layer": "layer_2",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_25",
          "type": "Linear",
          "name": "layer2_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_26",
          "type": "Linear",
          "name": "layer2_fc1",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_27",
          "type": "Linear",
          "name": "layer2_fc2",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_28",
          "type": "LayerNorm",
          "name": "layer2_norm1",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_29",
          "type": "LayerNorm",
          "name": "layer2_norm2",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_30",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BEiT_Base_op_31",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BEiT_Base_op_32",
          "type": "Dropout",
          "name": "layer2_attn_drop",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_33",
          "type": "Dropout",
          "name": "layer2_proj_drop",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_34",
          "type": "Linear",
          "name": "layer3_qkv",
          "layer": "layer_3",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_35",
          "type": "Linear",
          "name": "layer3_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_36",
          "type": "Linear",
          "name": "layer3_fc1",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_37",
          "type": "Linear",
          "name": "layer3_fc2",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_38",
          "type": "LayerNorm",
          "name": "layer3_norm1",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_39",
          "type": "LayerNorm",
          "name": "layer3_norm2",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_40",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BEiT_Base_op_41",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BEiT_Base_op_42",
          "type": "Dropout",
          "name": "layer3_attn_drop",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_43",
          "type": "Dropout",
          "name": "layer3_proj_drop",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_44",
          "type": "Linear",
          "name": "layer4_qkv",
          "layer": "layer_4",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_45",
          "type": "Linear",
          "name": "layer4_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_46",
          "type": "Linear",
          "name": "layer4_fc1",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_47",
          "type": "Linear",
          "name": "layer4_fc2",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_48",
          "type": "LayerNorm",
          "name": "layer4_norm1",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_49",
          "type": "LayerNorm",
          "name": "layer4_norm2",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_50",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BEiT_Base_op_51",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BEiT_Base_op_52",
          "type": "Dropout",
          "name": "layer4_attn_drop",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_53",
          "type": "Dropout",
          "name": "layer4_proj_drop",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_54",
          "type": "Linear",
          "name": "layer5_qkv",
          "layer": "layer_5",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_55",
          "type": "Linear",
          "name": "layer5_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_56",
          "type": "Linear",
          "name": "layer5_fc1",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_57",
          "type": "Linear",
          "name": "layer5_fc2",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_58",
          "type": "LayerNorm",
          "name": "layer5_norm1",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_59",
          "type": "LayerNorm",
          "name": "layer5_norm2",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_60",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BEiT_Base_op_61",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BEiT_Base_op_62",
          "type": "Dropout",
          "name": "layer5_attn_drop",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_63",
          "type": "Dropout",
          "name": "layer5_proj_drop",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_64",
          "type": "Linear",
          "name": "layer6_qkv",
          "layer": "layer_6",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_65",
          "type": "Linear",
          "name": "layer6_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_66",
          "type": "Linear",
          "name": "layer6_fc1",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_67",
          "type": "Linear",
          "name": "layer6_fc2",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_68",
          "type": "LayerNorm",
          "name": "layer6_norm1",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_69",
          "type": "LayerNorm",
          "name": "layer6_norm2",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_70",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BEiT_Base_op_71",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BEiT_Base_op_72",
          "type": "Dropout",
          "name": "layer6_attn_drop",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_73",
          "type": "Dropout",
          "name": "layer6_proj_drop",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_74",
          "type": "Linear",
          "name": "layer7_qkv",
          "layer": "layer_7",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_75",
          "type": "Linear",
          "name": "layer7_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_76",
          "type": "Linear",
          "name": "layer7_fc1",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_77",
          "type": "Linear",
          "name": "layer7_fc2",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_78",
          "type": "LayerNorm",
          "name": "layer7_norm1",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_79",
          "type": "LayerNorm",
          "name": "layer7_norm2",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_80",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BEiT_Base_op_81",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BEiT_Base_op_82",
          "type": "Dropout",
          "name": "layer7_attn_drop",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_83",
          "type": "Dropout",
          "name": "layer7_proj_drop",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_84",
          "type": "Linear",
          "name": "layer8_qkv",
          "layer": "layer_8",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_85",
          "type": "Linear",
          "name": "layer8_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_86",
          "type": "Linear",
          "name": "layer8_fc1",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_87",
          "type": "Linear",
          "name": "layer8_fc2",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_88",
          "type": "LayerNorm",
          "name": "layer8_norm1",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_89",
          "type": "LayerNorm",
          "name": "layer8_norm2",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_90",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BEiT_Base_op_91",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BEiT_Base_op_92",
          "type": "Dropout",
          "name": "layer8_attn_drop",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_93",
          "type": "Dropout",
          "name": "layer8_proj_drop",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_94",
          "type": "Linear",
          "name": "layer9_qkv",
          "layer": "layer_9",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_95",
          "type": "Linear",
          "name": "layer9_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_96",
          "type": "Linear",
          "name": "layer9_fc1",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_97",
          "type": "Linear",
          "name": "layer9_fc2",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_98",
          "type": "LayerNorm",
          "name": "layer9_norm1",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_99",
          "type": "LayerNorm",
          "name": "layer9_norm2",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_100",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BEiT_Base_op_101",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BEiT_Base_op_102",
          "type": "Dropout",
          "name": "layer9_attn_drop",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_103",
          "type": "Dropout",
          "name": "layer9_proj_drop",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_104",
          "type": "Linear",
          "name": "layer10_qkv",
          "layer": "layer_10",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_105",
          "type": "Linear",
          "name": "layer10_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_106",
          "type": "Linear",
          "name": "layer10_fc1",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_107",
          "type": "Linear",
          "name": "layer10_fc2",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_108",
          "type": "LayerNorm",
          "name": "layer10_norm1",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_109",
          "type": "LayerNorm",
          "name": "layer10_norm2",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_110",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BEiT_Base_op_111",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BEiT_Base_op_112",
          "type": "Dropout",
          "name": "layer10_attn_drop",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_113",
          "type": "Dropout",
          "name": "layer10_proj_drop",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_114",
          "type": "Linear",
          "name": "layer11_qkv",
          "layer": "layer_11",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_115",
          "type": "Linear",
          "name": "layer11_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_116",
          "type": "Linear",
          "name": "layer11_fc1",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_117",
          "type": "Linear",
          "name": "layer11_fc2",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BEiT_Base_op_118",
          "type": "LayerNorm",
          "name": "layer11_norm1",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_119",
          "type": "LayerNorm",
          "name": "layer11_norm2",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_120",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BEiT_Base_op_121",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BEiT_Base_op_122",
          "type": "Dropout",
          "name": "layer11_attn_drop",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BEiT_Base_op_123",
          "type": "Dropout",
          "name": "layer11_proj_drop",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        }
      ],
      "parameters": 86000000
    },
    {
      "model_id": "DeiT_Base",
      "name": "DeiT-Base",
      "source": "huggingface/deit-base",
      "category": "CV",
      "architecture": "vit_style",
      "num_params": 86000000,
      "num_layers": 12,
      "hidden_size": 768,
      "vocab_size": 0,
      "intermediate_size": 3072,
      "operator_count": 123,
      "operators": [
        {
          "op_id": "DeiT_Base_op_1",
          "type": "Conv2d",
          "name": "patch_embed",
          "layer": "global",
          "parameters": 2073600000000,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "DeiT_Base_op_2",
          "type": "LayerNorm",
          "name": "norm",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_3",
          "type": "Linear",
          "name": "head",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_4",
          "type": "Linear",
          "name": "layer0_qkv",
          "layer": "layer_0",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_5",
          "type": "Linear",
          "name": "layer0_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_6",
          "type": "Linear",
          "name": "layer0_fc1",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_7",
          "type": "Linear",
          "name": "layer0_fc2",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_8",
          "type": "LayerNorm",
          "name": "layer0_norm1",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_9",
          "type": "LayerNorm",
          "name": "layer0_norm2",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_10",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DeiT_Base_op_11",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DeiT_Base_op_12",
          "type": "Dropout",
          "name": "layer0_attn_drop",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_13",
          "type": "Dropout",
          "name": "layer0_proj_drop",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_14",
          "type": "Linear",
          "name": "layer1_qkv",
          "layer": "layer_1",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_15",
          "type": "Linear",
          "name": "layer1_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_16",
          "type": "Linear",
          "name": "layer1_fc1",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_17",
          "type": "Linear",
          "name": "layer1_fc2",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_18",
          "type": "LayerNorm",
          "name": "layer1_norm1",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_19",
          "type": "LayerNorm",
          "name": "layer1_norm2",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_20",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DeiT_Base_op_21",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DeiT_Base_op_22",
          "type": "Dropout",
          "name": "layer1_attn_drop",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_23",
          "type": "Dropout",
          "name": "layer1_proj_drop",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_24",
          "type": "Linear",
          "name": "layer2_qkv",
          "layer": "layer_2",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_25",
          "type": "Linear",
          "name": "layer2_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_26",
          "type": "Linear",
          "name": "layer2_fc1",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_27",
          "type": "Linear",
          "name": "layer2_fc2",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_28",
          "type": "LayerNorm",
          "name": "layer2_norm1",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_29",
          "type": "LayerNorm",
          "name": "layer2_norm2",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_30",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DeiT_Base_op_31",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DeiT_Base_op_32",
          "type": "Dropout",
          "name": "layer2_attn_drop",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_33",
          "type": "Dropout",
          "name": "layer2_proj_drop",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_34",
          "type": "Linear",
          "name": "layer3_qkv",
          "layer": "layer_3",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_35",
          "type": "Linear",
          "name": "layer3_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_36",
          "type": "Linear",
          "name": "layer3_fc1",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_37",
          "type": "Linear",
          "name": "layer3_fc2",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_38",
          "type": "LayerNorm",
          "name": "layer3_norm1",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_39",
          "type": "LayerNorm",
          "name": "layer3_norm2",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_40",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DeiT_Base_op_41",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DeiT_Base_op_42",
          "type": "Dropout",
          "name": "layer3_attn_drop",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_43",
          "type": "Dropout",
          "name": "layer3_proj_drop",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_44",
          "type": "Linear",
          "name": "layer4_qkv",
          "layer": "layer_4",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_45",
          "type": "Linear",
          "name": "layer4_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_46",
          "type": "Linear",
          "name": "layer4_fc1",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_47",
          "type": "Linear",
          "name": "layer4_fc2",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_48",
          "type": "LayerNorm",
          "name": "layer4_norm1",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_49",
          "type": "LayerNorm",
          "name": "layer4_norm2",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_50",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DeiT_Base_op_51",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DeiT_Base_op_52",
          "type": "Dropout",
          "name": "layer4_attn_drop",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_53",
          "type": "Dropout",
          "name": "layer4_proj_drop",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_54",
          "type": "Linear",
          "name": "layer5_qkv",
          "layer": "layer_5",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_55",
          "type": "Linear",
          "name": "layer5_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_56",
          "type": "Linear",
          "name": "layer5_fc1",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_57",
          "type": "Linear",
          "name": "layer5_fc2",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_58",
          "type": "LayerNorm",
          "name": "layer5_norm1",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_59",
          "type": "LayerNorm",
          "name": "layer5_norm2",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_60",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DeiT_Base_op_61",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DeiT_Base_op_62",
          "type": "Dropout",
          "name": "layer5_attn_drop",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_63",
          "type": "Dropout",
          "name": "layer5_proj_drop",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_64",
          "type": "Linear",
          "name": "layer6_qkv",
          "layer": "layer_6",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_65",
          "type": "Linear",
          "name": "layer6_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_66",
          "type": "Linear",
          "name": "layer6_fc1",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_67",
          "type": "Linear",
          "name": "layer6_fc2",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_68",
          "type": "LayerNorm",
          "name": "layer6_norm1",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_69",
          "type": "LayerNorm",
          "name": "layer6_norm2",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_70",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DeiT_Base_op_71",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DeiT_Base_op_72",
          "type": "Dropout",
          "name": "layer6_attn_drop",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_73",
          "type": "Dropout",
          "name": "layer6_proj_drop",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_74",
          "type": "Linear",
          "name": "layer7_qkv",
          "layer": "layer_7",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_75",
          "type": "Linear",
          "name": "layer7_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_76",
          "type": "Linear",
          "name": "layer7_fc1",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_77",
          "type": "Linear",
          "name": "layer7_fc2",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_78",
          "type": "LayerNorm",
          "name": "layer7_norm1",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_79",
          "type": "LayerNorm",
          "name": "layer7_norm2",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_80",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DeiT_Base_op_81",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DeiT_Base_op_82",
          "type": "Dropout",
          "name": "layer7_attn_drop",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_83",
          "type": "Dropout",
          "name": "layer7_proj_drop",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_84",
          "type": "Linear",
          "name": "layer8_qkv",
          "layer": "layer_8",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_85",
          "type": "Linear",
          "name": "layer8_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_86",
          "type": "Linear",
          "name": "layer8_fc1",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_87",
          "type": "Linear",
          "name": "layer8_fc2",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_88",
          "type": "LayerNorm",
          "name": "layer8_norm1",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_89",
          "type": "LayerNorm",
          "name": "layer8_norm2",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_90",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DeiT_Base_op_91",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DeiT_Base_op_92",
          "type": "Dropout",
          "name": "layer8_attn_drop",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_93",
          "type": "Dropout",
          "name": "layer8_proj_drop",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_94",
          "type": "Linear",
          "name": "layer9_qkv",
          "layer": "layer_9",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_95",
          "type": "Linear",
          "name": "layer9_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_96",
          "type": "Linear",
          "name": "layer9_fc1",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_97",
          "type": "Linear",
          "name": "layer9_fc2",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_98",
          "type": "LayerNorm",
          "name": "layer9_norm1",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_99",
          "type": "LayerNorm",
          "name": "layer9_norm2",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_100",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DeiT_Base_op_101",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DeiT_Base_op_102",
          "type": "Dropout",
          "name": "layer9_attn_drop",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_103",
          "type": "Dropout",
          "name": "layer9_proj_drop",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_104",
          "type": "Linear",
          "name": "layer10_qkv",
          "layer": "layer_10",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_105",
          "type": "Linear",
          "name": "layer10_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_106",
          "type": "Linear",
          "name": "layer10_fc1",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_107",
          "type": "Linear",
          "name": "layer10_fc2",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_108",
          "type": "LayerNorm",
          "name": "layer10_norm1",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_109",
          "type": "LayerNorm",
          "name": "layer10_norm2",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_110",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DeiT_Base_op_111",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DeiT_Base_op_112",
          "type": "Dropout",
          "name": "layer10_attn_drop",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_113",
          "type": "Dropout",
          "name": "layer10_proj_drop",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_114",
          "type": "Linear",
          "name": "layer11_qkv",
          "layer": "layer_11",
          "parameters": 1769472,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_115",
          "type": "Linear",
          "name": "layer11_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_116",
          "type": "Linear",
          "name": "layer11_fc1",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_117",
          "type": "Linear",
          "name": "layer11_fc2",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DeiT_Base_op_118",
          "type": "LayerNorm",
          "name": "layer11_norm1",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_119",
          "type": "LayerNorm",
          "name": "layer11_norm2",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_120",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DeiT_Base_op_121",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DeiT_Base_op_122",
          "type": "Dropout",
          "name": "layer11_attn_drop",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DeiT_Base_op_123",
          "type": "Dropout",
          "name": "layer11_proj_drop",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        }
      ],
      "parameters": 86000000
    },
    {
      "model_id": "BERT_Base",
      "name": "BERT-Base",
      "source": "huggingface/bert-base",
      "category": "NLP",
      "architecture": "bert_style",
      "num_params": 110000000,
      "num_layers": 12,
      "hidden_size": 768,
      "vocab_size": 30522,
      "intermediate_size": 3072,
      "operator_count": 150,
      "operators": [
        {
          "op_id": "BERT_Base_op_1",
          "type": "Embedding",
          "name": "word_embeddings",
          "layer": "global",
          "parameters": 23440896,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_2",
          "type": "Embedding",
          "name": "position_embeddings",
          "layer": "global",
          "parameters": 23440896,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_3",
          "type": "Embedding",
          "name": "token_type_embeddings",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_4",
          "type": "LayerNorm",
          "name": "embeddings_layernorm",
          "layer": "global",
          "parameters": 61044,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_5",
          "type": "Linear",
          "name": "pooler_dense",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_6",
          "type": "Tanh",
          "name": "pooler_activation",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_7",
          "type": "Linear",
          "name": "layer0_query",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_8",
          "type": "Linear",
          "name": "layer0_key",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_9",
          "type": "Linear",
          "name": "layer0_value",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_10",
          "type": "Linear",
          "name": "layer0_dense_attention",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_11",
          "type": "Linear",
          "name": "layer0_dense_intermediate",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_12",
          "type": "Linear",
          "name": "layer0_dense_output",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_13",
          "type": "LayerNorm",
          "name": "layer0_attention_layernorm",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_14",
          "type": "LayerNorm",
          "name": "layer0_output_layernorm",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_15",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Base_op_16",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BERT_Base_op_17",
          "type": "Dropout",
          "name": "layer0_attention_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_18",
          "type": "Dropout",
          "name": "layer0_output_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_19",
          "type": "Linear",
          "name": "layer1_query",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_20",
          "type": "Linear",
          "name": "layer1_key",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_21",
          "type": "Linear",
          "name": "layer1_value",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_22",
          "type": "Linear",
          "name": "layer1_dense_attention",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_23",
          "type": "Linear",
          "name": "layer1_dense_intermediate",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_24",
          "type": "Linear",
          "name": "layer1_dense_output",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_25",
          "type": "LayerNorm",
          "name": "layer1_attention_layernorm",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_26",
          "type": "LayerNorm",
          "name": "layer1_output_layernorm",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_27",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Base_op_28",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BERT_Base_op_29",
          "type": "Dropout",
          "name": "layer1_attention_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_30",
          "type": "Dropout",
          "name": "layer1_output_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_31",
          "type": "Linear",
          "name": "layer2_query",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_32",
          "type": "Linear",
          "name": "layer2_key",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_33",
          "type": "Linear",
          "name": "layer2_value",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_34",
          "type": "Linear",
          "name": "layer2_dense_attention",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_35",
          "type": "Linear",
          "name": "layer2_dense_intermediate",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_36",
          "type": "Linear",
          "name": "layer2_dense_output",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_37",
          "type": "LayerNorm",
          "name": "layer2_attention_layernorm",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_38",
          "type": "LayerNorm",
          "name": "layer2_output_layernorm",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_39",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Base_op_40",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BERT_Base_op_41",
          "type": "Dropout",
          "name": "layer2_attention_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_42",
          "type": "Dropout",
          "name": "layer2_output_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_43",
          "type": "Linear",
          "name": "layer3_query",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_44",
          "type": "Linear",
          "name": "layer3_key",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_45",
          "type": "Linear",
          "name": "layer3_value",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_46",
          "type": "Linear",
          "name": "layer3_dense_attention",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_47",
          "type": "Linear",
          "name": "layer3_dense_intermediate",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_48",
          "type": "Linear",
          "name": "layer3_dense_output",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_49",
          "type": "LayerNorm",
          "name": "layer3_attention_layernorm",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_50",
          "type": "LayerNorm",
          "name": "layer3_output_layernorm",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_51",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Base_op_52",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BERT_Base_op_53",
          "type": "Dropout",
          "name": "layer3_attention_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_54",
          "type": "Dropout",
          "name": "layer3_output_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_55",
          "type": "Linear",
          "name": "layer4_query",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_56",
          "type": "Linear",
          "name": "layer4_key",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_57",
          "type": "Linear",
          "name": "layer4_value",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_58",
          "type": "Linear",
          "name": "layer4_dense_attention",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_59",
          "type": "Linear",
          "name": "layer4_dense_intermediate",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_60",
          "type": "Linear",
          "name": "layer4_dense_output",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_61",
          "type": "LayerNorm",
          "name": "layer4_attention_layernorm",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_62",
          "type": "LayerNorm",
          "name": "layer4_output_layernorm",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_63",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Base_op_64",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BERT_Base_op_65",
          "type": "Dropout",
          "name": "layer4_attention_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_66",
          "type": "Dropout",
          "name": "layer4_output_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_67",
          "type": "Linear",
          "name": "layer5_query",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_68",
          "type": "Linear",
          "name": "layer5_key",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_69",
          "type": "Linear",
          "name": "layer5_value",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_70",
          "type": "Linear",
          "name": "layer5_dense_attention",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_71",
          "type": "Linear",
          "name": "layer5_dense_intermediate",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_72",
          "type": "Linear",
          "name": "layer5_dense_output",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_73",
          "type": "LayerNorm",
          "name": "layer5_attention_layernorm",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_74",
          "type": "LayerNorm",
          "name": "layer5_output_layernorm",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_75",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Base_op_76",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BERT_Base_op_77",
          "type": "Dropout",
          "name": "layer5_attention_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_78",
          "type": "Dropout",
          "name": "layer5_output_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_79",
          "type": "Linear",
          "name": "layer6_query",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_80",
          "type": "Linear",
          "name": "layer6_key",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_81",
          "type": "Linear",
          "name": "layer6_value",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_82",
          "type": "Linear",
          "name": "layer6_dense_attention",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_83",
          "type": "Linear",
          "name": "layer6_dense_intermediate",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_84",
          "type": "Linear",
          "name": "layer6_dense_output",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_85",
          "type": "LayerNorm",
          "name": "layer6_attention_layernorm",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_86",
          "type": "LayerNorm",
          "name": "layer6_output_layernorm",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_87",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Base_op_88",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BERT_Base_op_89",
          "type": "Dropout",
          "name": "layer6_attention_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_90",
          "type": "Dropout",
          "name": "layer6_output_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_91",
          "type": "Linear",
          "name": "layer7_query",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_92",
          "type": "Linear",
          "name": "layer7_key",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_93",
          "type": "Linear",
          "name": "layer7_value",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_94",
          "type": "Linear",
          "name": "layer7_dense_attention",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_95",
          "type": "Linear",
          "name": "layer7_dense_intermediate",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_96",
          "type": "Linear",
          "name": "layer7_dense_output",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_97",
          "type": "LayerNorm",
          "name": "layer7_attention_layernorm",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_98",
          "type": "LayerNorm",
          "name": "layer7_output_layernorm",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_99",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Base_op_100",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BERT_Base_op_101",
          "type": "Dropout",
          "name": "layer7_attention_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_102",
          "type": "Dropout",
          "name": "layer7_output_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_103",
          "type": "Linear",
          "name": "layer8_query",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_104",
          "type": "Linear",
          "name": "layer8_key",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_105",
          "type": "Linear",
          "name": "layer8_value",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_106",
          "type": "Linear",
          "name": "layer8_dense_attention",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_107",
          "type": "Linear",
          "name": "layer8_dense_intermediate",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_108",
          "type": "Linear",
          "name": "layer8_dense_output",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_109",
          "type": "LayerNorm",
          "name": "layer8_attention_layernorm",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_110",
          "type": "LayerNorm",
          "name": "layer8_output_layernorm",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_111",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Base_op_112",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BERT_Base_op_113",
          "type": "Dropout",
          "name": "layer8_attention_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_114",
          "type": "Dropout",
          "name": "layer8_output_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_115",
          "type": "Linear",
          "name": "layer9_query",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_116",
          "type": "Linear",
          "name": "layer9_key",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_117",
          "type": "Linear",
          "name": "layer9_value",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_118",
          "type": "Linear",
          "name": "layer9_dense_attention",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_119",
          "type": "Linear",
          "name": "layer9_dense_intermediate",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_120",
          "type": "Linear",
          "name": "layer9_dense_output",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_121",
          "type": "LayerNorm",
          "name": "layer9_attention_layernorm",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_122",
          "type": "LayerNorm",
          "name": "layer9_output_layernorm",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_123",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Base_op_124",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BERT_Base_op_125",
          "type": "Dropout",
          "name": "layer9_attention_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_126",
          "type": "Dropout",
          "name": "layer9_output_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_127",
          "type": "Linear",
          "name": "layer10_query",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_128",
          "type": "Linear",
          "name": "layer10_key",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_129",
          "type": "Linear",
          "name": "layer10_value",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_130",
          "type": "Linear",
          "name": "layer10_dense_attention",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_131",
          "type": "Linear",
          "name": "layer10_dense_intermediate",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_132",
          "type": "Linear",
          "name": "layer10_dense_output",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_133",
          "type": "LayerNorm",
          "name": "layer10_attention_layernorm",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_134",
          "type": "LayerNorm",
          "name": "layer10_output_layernorm",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_135",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Base_op_136",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BERT_Base_op_137",
          "type": "Dropout",
          "name": "layer10_attention_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_138",
          "type": "Dropout",
          "name": "layer10_output_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_139",
          "type": "Linear",
          "name": "layer11_query",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_140",
          "type": "Linear",
          "name": "layer11_key",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_141",
          "type": "Linear",
          "name": "layer11_value",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_142",
          "type": "Linear",
          "name": "layer11_dense_attention",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_143",
          "type": "Linear",
          "name": "layer11_dense_intermediate",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_144",
          "type": "Linear",
          "name": "layer11_dense_output",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Base_op_145",
          "type": "LayerNorm",
          "name": "layer11_attention_layernorm",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_146",
          "type": "LayerNorm",
          "name": "layer11_output_layernorm",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_147",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Base_op_148",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BERT_Base_op_149",
          "type": "Dropout",
          "name": "layer11_attention_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BERT_Base_op_150",
          "type": "Dropout",
          "name": "layer11_output_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        }
      ],
      "parameters": 110000000
    },
    {
      "model_id": "RoBERTa_Base",
      "name": "RoBERTa-Base",
      "source": "huggingface/roberta-base",
      "category": "NLP",
      "architecture": "bert_style",
      "num_params": 125000000,
      "num_layers": 12,
      "hidden_size": 768,
      "vocab_size": 50265,
      "intermediate_size": 3072,
      "operator_count": 150,
      "operators": [
        {
          "op_id": "RoBERTa_Base_op_1",
          "type": "Embedding",
          "name": "word_embeddings",
          "layer": "global",
          "parameters": 38603520,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_2",
          "type": "Embedding",
          "name": "position_embeddings",
          "layer": "global",
          "parameters": 38603520,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_3",
          "type": "Embedding",
          "name": "token_type_embeddings",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_4",
          "type": "LayerNorm",
          "name": "embeddings_layernorm",
          "layer": "global",
          "parameters": 100530,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_5",
          "type": "Linear",
          "name": "pooler_dense",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_6",
          "type": "Tanh",
          "name": "pooler_activation",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_7",
          "type": "Linear",
          "name": "layer0_query",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_8",
          "type": "Linear",
          "name": "layer0_key",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_9",
          "type": "Linear",
          "name": "layer0_value",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_10",
          "type": "Linear",
          "name": "layer0_dense_attention",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_11",
          "type": "Linear",
          "name": "layer0_dense_intermediate",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_12",
          "type": "Linear",
          "name": "layer0_dense_output",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_13",
          "type": "LayerNorm",
          "name": "layer0_attention_layernorm",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_14",
          "type": "LayerNorm",
          "name": "layer0_output_layernorm",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_15",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "RoBERTa_Base_op_16",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "RoBERTa_Base_op_17",
          "type": "Dropout",
          "name": "layer0_attention_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_18",
          "type": "Dropout",
          "name": "layer0_output_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_19",
          "type": "Linear",
          "name": "layer1_query",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_20",
          "type": "Linear",
          "name": "layer1_key",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_21",
          "type": "Linear",
          "name": "layer1_value",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_22",
          "type": "Linear",
          "name": "layer1_dense_attention",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_23",
          "type": "Linear",
          "name": "layer1_dense_intermediate",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_24",
          "type": "Linear",
          "name": "layer1_dense_output",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_25",
          "type": "LayerNorm",
          "name": "layer1_attention_layernorm",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_26",
          "type": "LayerNorm",
          "name": "layer1_output_layernorm",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_27",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "RoBERTa_Base_op_28",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "RoBERTa_Base_op_29",
          "type": "Dropout",
          "name": "layer1_attention_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_30",
          "type": "Dropout",
          "name": "layer1_output_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_31",
          "type": "Linear",
          "name": "layer2_query",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_32",
          "type": "Linear",
          "name": "layer2_key",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_33",
          "type": "Linear",
          "name": "layer2_value",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_34",
          "type": "Linear",
          "name": "layer2_dense_attention",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_35",
          "type": "Linear",
          "name": "layer2_dense_intermediate",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_36",
          "type": "Linear",
          "name": "layer2_dense_output",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_37",
          "type": "LayerNorm",
          "name": "layer2_attention_layernorm",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_38",
          "type": "LayerNorm",
          "name": "layer2_output_layernorm",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_39",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "RoBERTa_Base_op_40",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "RoBERTa_Base_op_41",
          "type": "Dropout",
          "name": "layer2_attention_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_42",
          "type": "Dropout",
          "name": "layer2_output_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_43",
          "type": "Linear",
          "name": "layer3_query",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_44",
          "type": "Linear",
          "name": "layer3_key",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_45",
          "type": "Linear",
          "name": "layer3_value",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_46",
          "type": "Linear",
          "name": "layer3_dense_attention",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_47",
          "type": "Linear",
          "name": "layer3_dense_intermediate",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_48",
          "type": "Linear",
          "name": "layer3_dense_output",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_49",
          "type": "LayerNorm",
          "name": "layer3_attention_layernorm",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_50",
          "type": "LayerNorm",
          "name": "layer3_output_layernorm",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_51",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "RoBERTa_Base_op_52",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "RoBERTa_Base_op_53",
          "type": "Dropout",
          "name": "layer3_attention_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_54",
          "type": "Dropout",
          "name": "layer3_output_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_55",
          "type": "Linear",
          "name": "layer4_query",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_56",
          "type": "Linear",
          "name": "layer4_key",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_57",
          "type": "Linear",
          "name": "layer4_value",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_58",
          "type": "Linear",
          "name": "layer4_dense_attention",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_59",
          "type": "Linear",
          "name": "layer4_dense_intermediate",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_60",
          "type": "Linear",
          "name": "layer4_dense_output",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_61",
          "type": "LayerNorm",
          "name": "layer4_attention_layernorm",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_62",
          "type": "LayerNorm",
          "name": "layer4_output_layernorm",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_63",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "RoBERTa_Base_op_64",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "RoBERTa_Base_op_65",
          "type": "Dropout",
          "name": "layer4_attention_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_66",
          "type": "Dropout",
          "name": "layer4_output_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_67",
          "type": "Linear",
          "name": "layer5_query",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_68",
          "type": "Linear",
          "name": "layer5_key",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_69",
          "type": "Linear",
          "name": "layer5_value",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_70",
          "type": "Linear",
          "name": "layer5_dense_attention",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_71",
          "type": "Linear",
          "name": "layer5_dense_intermediate",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_72",
          "type": "Linear",
          "name": "layer5_dense_output",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_73",
          "type": "LayerNorm",
          "name": "layer5_attention_layernorm",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_74",
          "type": "LayerNorm",
          "name": "layer5_output_layernorm",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_75",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "RoBERTa_Base_op_76",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "RoBERTa_Base_op_77",
          "type": "Dropout",
          "name": "layer5_attention_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_78",
          "type": "Dropout",
          "name": "layer5_output_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_79",
          "type": "Linear",
          "name": "layer6_query",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_80",
          "type": "Linear",
          "name": "layer6_key",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_81",
          "type": "Linear",
          "name": "layer6_value",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_82",
          "type": "Linear",
          "name": "layer6_dense_attention",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_83",
          "type": "Linear",
          "name": "layer6_dense_intermediate",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_84",
          "type": "Linear",
          "name": "layer6_dense_output",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_85",
          "type": "LayerNorm",
          "name": "layer6_attention_layernorm",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_86",
          "type": "LayerNorm",
          "name": "layer6_output_layernorm",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_87",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "RoBERTa_Base_op_88",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "RoBERTa_Base_op_89",
          "type": "Dropout",
          "name": "layer6_attention_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_90",
          "type": "Dropout",
          "name": "layer6_output_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_91",
          "type": "Linear",
          "name": "layer7_query",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_92",
          "type": "Linear",
          "name": "layer7_key",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_93",
          "type": "Linear",
          "name": "layer7_value",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_94",
          "type": "Linear",
          "name": "layer7_dense_attention",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_95",
          "type": "Linear",
          "name": "layer7_dense_intermediate",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_96",
          "type": "Linear",
          "name": "layer7_dense_output",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_97",
          "type": "LayerNorm",
          "name": "layer7_attention_layernorm",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_98",
          "type": "LayerNorm",
          "name": "layer7_output_layernorm",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_99",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "RoBERTa_Base_op_100",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "RoBERTa_Base_op_101",
          "type": "Dropout",
          "name": "layer7_attention_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_102",
          "type": "Dropout",
          "name": "layer7_output_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_103",
          "type": "Linear",
          "name": "layer8_query",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_104",
          "type": "Linear",
          "name": "layer8_key",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_105",
          "type": "Linear",
          "name": "layer8_value",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_106",
          "type": "Linear",
          "name": "layer8_dense_attention",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_107",
          "type": "Linear",
          "name": "layer8_dense_intermediate",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_108",
          "type": "Linear",
          "name": "layer8_dense_output",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_109",
          "type": "LayerNorm",
          "name": "layer8_attention_layernorm",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_110",
          "type": "LayerNorm",
          "name": "layer8_output_layernorm",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_111",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "RoBERTa_Base_op_112",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "RoBERTa_Base_op_113",
          "type": "Dropout",
          "name": "layer8_attention_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_114",
          "type": "Dropout",
          "name": "layer8_output_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_115",
          "type": "Linear",
          "name": "layer9_query",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_116",
          "type": "Linear",
          "name": "layer9_key",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_117",
          "type": "Linear",
          "name": "layer9_value",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_118",
          "type": "Linear",
          "name": "layer9_dense_attention",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_119",
          "type": "Linear",
          "name": "layer9_dense_intermediate",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_120",
          "type": "Linear",
          "name": "layer9_dense_output",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_121",
          "type": "LayerNorm",
          "name": "layer9_attention_layernorm",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_122",
          "type": "LayerNorm",
          "name": "layer9_output_layernorm",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_123",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "RoBERTa_Base_op_124",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "RoBERTa_Base_op_125",
          "type": "Dropout",
          "name": "layer9_attention_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_126",
          "type": "Dropout",
          "name": "layer9_output_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_127",
          "type": "Linear",
          "name": "layer10_query",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_128",
          "type": "Linear",
          "name": "layer10_key",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_129",
          "type": "Linear",
          "name": "layer10_value",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_130",
          "type": "Linear",
          "name": "layer10_dense_attention",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_131",
          "type": "Linear",
          "name": "layer10_dense_intermediate",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_132",
          "type": "Linear",
          "name": "layer10_dense_output",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_133",
          "type": "LayerNorm",
          "name": "layer10_attention_layernorm",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_134",
          "type": "LayerNorm",
          "name": "layer10_output_layernorm",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_135",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "RoBERTa_Base_op_136",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "RoBERTa_Base_op_137",
          "type": "Dropout",
          "name": "layer10_attention_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_138",
          "type": "Dropout",
          "name": "layer10_output_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_139",
          "type": "Linear",
          "name": "layer11_query",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_140",
          "type": "Linear",
          "name": "layer11_key",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_141",
          "type": "Linear",
          "name": "layer11_value",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_142",
          "type": "Linear",
          "name": "layer11_dense_attention",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_143",
          "type": "Linear",
          "name": "layer11_dense_intermediate",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_144",
          "type": "Linear",
          "name": "layer11_dense_output",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "RoBERTa_Base_op_145",
          "type": "LayerNorm",
          "name": "layer11_attention_layernorm",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_146",
          "type": "LayerNorm",
          "name": "layer11_output_layernorm",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_147",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "RoBERTa_Base_op_148",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "RoBERTa_Base_op_149",
          "type": "Dropout",
          "name": "layer11_attention_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "RoBERTa_Base_op_150",
          "type": "Dropout",
          "name": "layer11_output_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        }
      ],
      "parameters": 125000000
    },
    {
      "model_id": "T5_Base",
      "name": "T5-Base",
      "source": "huggingface/t5-base",
      "category": "NLP",
      "architecture": "t5_style",
      "num_params": 220000000,
      "num_layers": 12,
      "hidden_size": 768,
      "vocab_size": 32128,
      "intermediate_size": 3072,
      "operator_count": 134,
      "operators": [
        {
          "op_id": "T5_Base_op_1",
          "type": "Embedding",
          "name": "shared",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_2",
          "type": "RMSNorm",
          "name": "final_layer_norm",
          "layer": "global",
          "parameters": 768,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_3",
          "type": "Linear",
          "name": "layer0_q",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_4",
          "type": "Linear",
          "name": "layer0_k",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_5",
          "type": "Linear",
          "name": "layer0_v",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_6",
          "type": "Linear",
          "name": "layer0_o",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_7",
          "type": "Linear",
          "name": "layer0_wi_0",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_8",
          "type": "Linear",
          "name": "layer0_wi_1",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_9",
          "type": "Linear",
          "name": "layer0_wo",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_10",
          "type": "RMSNorm",
          "name": "layer0_layer_norm",
          "layer": "layer_0",
          "parameters": 768,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_11",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "T5_Base_op_12",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "T5_Base_op_13",
          "type": "Dropout",
          "name": "layer0_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_14",
          "type": "Linear",
          "name": "layer1_q",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_15",
          "type": "Linear",
          "name": "layer1_k",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_16",
          "type": "Linear",
          "name": "layer1_v",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_17",
          "type": "Linear",
          "name": "layer1_o",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_18",
          "type": "Linear",
          "name": "layer1_wi_0",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_19",
          "type": "Linear",
          "name": "layer1_wi_1",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_20",
          "type": "Linear",
          "name": "layer1_wo",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_21",
          "type": "RMSNorm",
          "name": "layer1_layer_norm",
          "layer": "layer_1",
          "parameters": 768,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_22",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "T5_Base_op_23",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "T5_Base_op_24",
          "type": "Dropout",
          "name": "layer1_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_25",
          "type": "Linear",
          "name": "layer2_q",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_26",
          "type": "Linear",
          "name": "layer2_k",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_27",
          "type": "Linear",
          "name": "layer2_v",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_28",
          "type": "Linear",
          "name": "layer2_o",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_29",
          "type": "Linear",
          "name": "layer2_wi_0",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_30",
          "type": "Linear",
          "name": "layer2_wi_1",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_31",
          "type": "Linear",
          "name": "layer2_wo",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_32",
          "type": "RMSNorm",
          "name": "layer2_layer_norm",
          "layer": "layer_2",
          "parameters": 768,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_33",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "T5_Base_op_34",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "T5_Base_op_35",
          "type": "Dropout",
          "name": "layer2_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_36",
          "type": "Linear",
          "name": "layer3_q",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_37",
          "type": "Linear",
          "name": "layer3_k",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_38",
          "type": "Linear",
          "name": "layer3_v",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_39",
          "type": "Linear",
          "name": "layer3_o",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_40",
          "type": "Linear",
          "name": "layer3_wi_0",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_41",
          "type": "Linear",
          "name": "layer3_wi_1",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_42",
          "type": "Linear",
          "name": "layer3_wo",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_43",
          "type": "RMSNorm",
          "name": "layer3_layer_norm",
          "layer": "layer_3",
          "parameters": 768,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_44",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "T5_Base_op_45",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "T5_Base_op_46",
          "type": "Dropout",
          "name": "layer3_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_47",
          "type": "Linear",
          "name": "layer4_q",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_48",
          "type": "Linear",
          "name": "layer4_k",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_49",
          "type": "Linear",
          "name": "layer4_v",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_50",
          "type": "Linear",
          "name": "layer4_o",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_51",
          "type": "Linear",
          "name": "layer4_wi_0",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_52",
          "type": "Linear",
          "name": "layer4_wi_1",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_53",
          "type": "Linear",
          "name": "layer4_wo",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_54",
          "type": "RMSNorm",
          "name": "layer4_layer_norm",
          "layer": "layer_4",
          "parameters": 768,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_55",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "T5_Base_op_56",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "T5_Base_op_57",
          "type": "Dropout",
          "name": "layer4_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_58",
          "type": "Linear",
          "name": "layer5_q",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_59",
          "type": "Linear",
          "name": "layer5_k",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_60",
          "type": "Linear",
          "name": "layer5_v",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_61",
          "type": "Linear",
          "name": "layer5_o",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_62",
          "type": "Linear",
          "name": "layer5_wi_0",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_63",
          "type": "Linear",
          "name": "layer5_wi_1",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_64",
          "type": "Linear",
          "name": "layer5_wo",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_65",
          "type": "RMSNorm",
          "name": "layer5_layer_norm",
          "layer": "layer_5",
          "parameters": 768,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_66",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "T5_Base_op_67",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "T5_Base_op_68",
          "type": "Dropout",
          "name": "layer5_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_69",
          "type": "Linear",
          "name": "layer6_q",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_70",
          "type": "Linear",
          "name": "layer6_k",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_71",
          "type": "Linear",
          "name": "layer6_v",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_72",
          "type": "Linear",
          "name": "layer6_o",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_73",
          "type": "Linear",
          "name": "layer6_wi_0",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_74",
          "type": "Linear",
          "name": "layer6_wi_1",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_75",
          "type": "Linear",
          "name": "layer6_wo",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_76",
          "type": "RMSNorm",
          "name": "layer6_layer_norm",
          "layer": "layer_6",
          "parameters": 768,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_77",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "T5_Base_op_78",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "T5_Base_op_79",
          "type": "Dropout",
          "name": "layer6_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_80",
          "type": "Linear",
          "name": "layer7_q",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_81",
          "type": "Linear",
          "name": "layer7_k",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_82",
          "type": "Linear",
          "name": "layer7_v",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_83",
          "type": "Linear",
          "name": "layer7_o",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_84",
          "type": "Linear",
          "name": "layer7_wi_0",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_85",
          "type": "Linear",
          "name": "layer7_wi_1",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_86",
          "type": "Linear",
          "name": "layer7_wo",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_87",
          "type": "RMSNorm",
          "name": "layer7_layer_norm",
          "layer": "layer_7",
          "parameters": 768,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_88",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "T5_Base_op_89",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "T5_Base_op_90",
          "type": "Dropout",
          "name": "layer7_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_91",
          "type": "Linear",
          "name": "layer8_q",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_92",
          "type": "Linear",
          "name": "layer8_k",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_93",
          "type": "Linear",
          "name": "layer8_v",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_94",
          "type": "Linear",
          "name": "layer8_o",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_95",
          "type": "Linear",
          "name": "layer8_wi_0",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_96",
          "type": "Linear",
          "name": "layer8_wi_1",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_97",
          "type": "Linear",
          "name": "layer8_wo",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_98",
          "type": "RMSNorm",
          "name": "layer8_layer_norm",
          "layer": "layer_8",
          "parameters": 768,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_99",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "T5_Base_op_100",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "T5_Base_op_101",
          "type": "Dropout",
          "name": "layer8_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_102",
          "type": "Linear",
          "name": "layer9_q",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_103",
          "type": "Linear",
          "name": "layer9_k",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_104",
          "type": "Linear",
          "name": "layer9_v",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_105",
          "type": "Linear",
          "name": "layer9_o",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_106",
          "type": "Linear",
          "name": "layer9_wi_0",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_107",
          "type": "Linear",
          "name": "layer9_wi_1",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_108",
          "type": "Linear",
          "name": "layer9_wo",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_109",
          "type": "RMSNorm",
          "name": "layer9_layer_norm",
          "layer": "layer_9",
          "parameters": 768,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_110",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "T5_Base_op_111",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "T5_Base_op_112",
          "type": "Dropout",
          "name": "layer9_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_113",
          "type": "Linear",
          "name": "layer10_q",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_114",
          "type": "Linear",
          "name": "layer10_k",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_115",
          "type": "Linear",
          "name": "layer10_v",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_116",
          "type": "Linear",
          "name": "layer10_o",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_117",
          "type": "Linear",
          "name": "layer10_wi_0",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_118",
          "type": "Linear",
          "name": "layer10_wi_1",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_119",
          "type": "Linear",
          "name": "layer10_wo",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_120",
          "type": "RMSNorm",
          "name": "layer10_layer_norm",
          "layer": "layer_10",
          "parameters": 768,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_121",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "T5_Base_op_122",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "T5_Base_op_123",
          "type": "Dropout",
          "name": "layer10_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_124",
          "type": "Linear",
          "name": "layer11_q",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_125",
          "type": "Linear",
          "name": "layer11_k",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_126",
          "type": "Linear",
          "name": "layer11_v",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_127",
          "type": "Linear",
          "name": "layer11_o",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_128",
          "type": "Linear",
          "name": "layer11_wi_0",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_129",
          "type": "Linear",
          "name": "layer11_wi_1",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_130",
          "type": "Linear",
          "name": "layer11_wo",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "T5_Base_op_131",
          "type": "RMSNorm",
          "name": "layer11_layer_norm",
          "layer": "layer_11",
          "parameters": 768,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "T5_Base_op_132",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "T5_Base_op_133",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "T5_Base_op_134",
          "type": "Dropout",
          "name": "layer11_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        }
      ],
      "parameters": 220000000
    },
    {
      "model_id": "DistilBERT",
      "name": "DistilBERT",
      "source": "huggingface/distilbert",
      "category": "NLP",
      "architecture": "bert_style",
      "num_params": 66000000,
      "num_layers": 6,
      "hidden_size": 768,
      "vocab_size": 30522,
      "intermediate_size": 3072,
      "operator_count": 78,
      "operators": [
        {
          "op_id": "DistilBERT_op_1",
          "type": "Embedding",
          "name": "word_embeddings",
          "layer": "global",
          "parameters": 23440896,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_2",
          "type": "Embedding",
          "name": "position_embeddings",
          "layer": "global",
          "parameters": 23440896,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_3",
          "type": "Embedding",
          "name": "token_type_embeddings",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_4",
          "type": "LayerNorm",
          "name": "embeddings_layernorm",
          "layer": "global",
          "parameters": 61044,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_5",
          "type": "Linear",
          "name": "pooler_dense",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_6",
          "type": "Tanh",
          "name": "pooler_activation",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_7",
          "type": "Linear",
          "name": "layer0_query",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_8",
          "type": "Linear",
          "name": "layer0_key",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_9",
          "type": "Linear",
          "name": "layer0_value",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_10",
          "type": "Linear",
          "name": "layer0_dense_attention",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_11",
          "type": "Linear",
          "name": "layer0_dense_intermediate",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_12",
          "type": "Linear",
          "name": "layer0_dense_output",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_13",
          "type": "LayerNorm",
          "name": "layer0_attention_layernorm",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_14",
          "type": "LayerNorm",
          "name": "layer0_output_layernorm",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_15",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DistilBERT_op_16",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DistilBERT_op_17",
          "type": "Dropout",
          "name": "layer0_attention_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_18",
          "type": "Dropout",
          "name": "layer0_output_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_19",
          "type": "Linear",
          "name": "layer1_query",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_20",
          "type": "Linear",
          "name": "layer1_key",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_21",
          "type": "Linear",
          "name": "layer1_value",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_22",
          "type": "Linear",
          "name": "layer1_dense_attention",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_23",
          "type": "Linear",
          "name": "layer1_dense_intermediate",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_24",
          "type": "Linear",
          "name": "layer1_dense_output",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_25",
          "type": "LayerNorm",
          "name": "layer1_attention_layernorm",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_26",
          "type": "LayerNorm",
          "name": "layer1_output_layernorm",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_27",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DistilBERT_op_28",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DistilBERT_op_29",
          "type": "Dropout",
          "name": "layer1_attention_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_30",
          "type": "Dropout",
          "name": "layer1_output_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_31",
          "type": "Linear",
          "name": "layer2_query",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_32",
          "type": "Linear",
          "name": "layer2_key",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_33",
          "type": "Linear",
          "name": "layer2_value",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_34",
          "type": "Linear",
          "name": "layer2_dense_attention",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_35",
          "type": "Linear",
          "name": "layer2_dense_intermediate",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_36",
          "type": "Linear",
          "name": "layer2_dense_output",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_37",
          "type": "LayerNorm",
          "name": "layer2_attention_layernorm",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_38",
          "type": "LayerNorm",
          "name": "layer2_output_layernorm",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_39",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DistilBERT_op_40",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DistilBERT_op_41",
          "type": "Dropout",
          "name": "layer2_attention_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_42",
          "type": "Dropout",
          "name": "layer2_output_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_43",
          "type": "Linear",
          "name": "layer3_query",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_44",
          "type": "Linear",
          "name": "layer3_key",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_45",
          "type": "Linear",
          "name": "layer3_value",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_46",
          "type": "Linear",
          "name": "layer3_dense_attention",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_47",
          "type": "Linear",
          "name": "layer3_dense_intermediate",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_48",
          "type": "Linear",
          "name": "layer3_dense_output",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_49",
          "type": "LayerNorm",
          "name": "layer3_attention_layernorm",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_50",
          "type": "LayerNorm",
          "name": "layer3_output_layernorm",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_51",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DistilBERT_op_52",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DistilBERT_op_53",
          "type": "Dropout",
          "name": "layer3_attention_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_54",
          "type": "Dropout",
          "name": "layer3_output_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_55",
          "type": "Linear",
          "name": "layer4_query",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_56",
          "type": "Linear",
          "name": "layer4_key",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_57",
          "type": "Linear",
          "name": "layer4_value",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_58",
          "type": "Linear",
          "name": "layer4_dense_attention",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_59",
          "type": "Linear",
          "name": "layer4_dense_intermediate",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_60",
          "type": "Linear",
          "name": "layer4_dense_output",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_61",
          "type": "LayerNorm",
          "name": "layer4_attention_layernorm",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_62",
          "type": "LayerNorm",
          "name": "layer4_output_layernorm",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_63",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DistilBERT_op_64",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DistilBERT_op_65",
          "type": "Dropout",
          "name": "layer4_attention_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_66",
          "type": "Dropout",
          "name": "layer4_output_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_67",
          "type": "Linear",
          "name": "layer5_query",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_68",
          "type": "Linear",
          "name": "layer5_key",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_69",
          "type": "Linear",
          "name": "layer5_value",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_70",
          "type": "Linear",
          "name": "layer5_dense_attention",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_71",
          "type": "Linear",
          "name": "layer5_dense_intermediate",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_72",
          "type": "Linear",
          "name": "layer5_dense_output",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_op_73",
          "type": "LayerNorm",
          "name": "layer5_attention_layernorm",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_74",
          "type": "LayerNorm",
          "name": "layer5_output_layernorm",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_75",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DistilBERT_op_76",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DistilBERT_op_77",
          "type": "Dropout",
          "name": "layer5_attention_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_op_78",
          "type": "Dropout",
          "name": "layer5_output_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        }
      ],
      "parameters": 66000000
    },
    {
      "model_id": "Whisper_Base",
      "name": "Whisper-Base",
      "source": "huggingface/whisper-base",
      "category": "Audio",
      "architecture": "whisper_style",
      "num_params": 74000000,
      "num_layers": 12,
      "hidden_size": 512,
      "vocab_size": 0,
      "intermediate_size": 2048,
      "operator_count": 124,
      "operators": [
        {
          "op_id": "Whisper_Base_op_1",
          "type": "Conv1d",
          "name": "conv1",
          "layer": "global",
          "parameters": 122880,
          "input_shape": "[batch, C_in, L]",
          "output_shape": "[batch, C_out, L']"
        },
        {
          "op_id": "Whisper_Base_op_2",
          "type": "Conv1d",
          "name": "conv2",
          "layer": "global",
          "parameters": 786432,
          "input_shape": "[batch, C_in, L]",
          "output_shape": "[batch, C_out, L']"
        },
        {
          "op_id": "Whisper_Base_op_3",
          "type": "Embedding",
          "name": "embed_positions",
          "layer": "global",
          "parameters": 768000,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_4",
          "type": "LayerNorm",
          "name": "layer_norm",
          "layer": "global",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_5",
          "type": "Linear",
          "name": "layer0_q_proj",
          "layer": "layer_0",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_6",
          "type": "Linear",
          "name": "layer0_k_proj",
          "layer": "layer_0",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_7",
          "type": "Linear",
          "name": "layer0_v_proj",
          "layer": "layer_0",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_8",
          "type": "Linear",
          "name": "layer0_out_proj",
          "layer": "layer_0",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_9",
          "type": "Linear",
          "name": "layer0_fc1",
          "layer": "layer_0",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_10",
          "type": "Linear",
          "name": "layer0_fc2",
          "layer": "layer_0",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_11",
          "type": "LayerNorm",
          "name": "layer0_self_attn_layer_norm",
          "layer": "layer_0",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_12",
          "type": "LayerNorm",
          "name": "layer0_final_layer_norm",
          "layer": "layer_0",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_13",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Whisper_Base_op_14",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Whisper_Base_op_15",
          "type": "Linear",
          "name": "layer1_q_proj",
          "layer": "layer_1",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_16",
          "type": "Linear",
          "name": "layer1_k_proj",
          "layer": "layer_1",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_17",
          "type": "Linear",
          "name": "layer1_v_proj",
          "layer": "layer_1",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_18",
          "type": "Linear",
          "name": "layer1_out_proj",
          "layer": "layer_1",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_19",
          "type": "Linear",
          "name": "layer1_fc1",
          "layer": "layer_1",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_20",
          "type": "Linear",
          "name": "layer1_fc2",
          "layer": "layer_1",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_21",
          "type": "LayerNorm",
          "name": "layer1_self_attn_layer_norm",
          "layer": "layer_1",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_22",
          "type": "LayerNorm",
          "name": "layer1_final_layer_norm",
          "layer": "layer_1",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_23",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Whisper_Base_op_24",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Whisper_Base_op_25",
          "type": "Linear",
          "name": "layer2_q_proj",
          "layer": "layer_2",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_26",
          "type": "Linear",
          "name": "layer2_k_proj",
          "layer": "layer_2",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_27",
          "type": "Linear",
          "name": "layer2_v_proj",
          "layer": "layer_2",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_28",
          "type": "Linear",
          "name": "layer2_out_proj",
          "layer": "layer_2",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_29",
          "type": "Linear",
          "name": "layer2_fc1",
          "layer": "layer_2",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_30",
          "type": "Linear",
          "name": "layer2_fc2",
          "layer": "layer_2",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_31",
          "type": "LayerNorm",
          "name": "layer2_self_attn_layer_norm",
          "layer": "layer_2",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_32",
          "type": "LayerNorm",
          "name": "layer2_final_layer_norm",
          "layer": "layer_2",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_33",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Whisper_Base_op_34",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Whisper_Base_op_35",
          "type": "Linear",
          "name": "layer3_q_proj",
          "layer": "layer_3",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_36",
          "type": "Linear",
          "name": "layer3_k_proj",
          "layer": "layer_3",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_37",
          "type": "Linear",
          "name": "layer3_v_proj",
          "layer": "layer_3",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_38",
          "type": "Linear",
          "name": "layer3_out_proj",
          "layer": "layer_3",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_39",
          "type": "Linear",
          "name": "layer3_fc1",
          "layer": "layer_3",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_40",
          "type": "Linear",
          "name": "layer3_fc2",
          "layer": "layer_3",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_41",
          "type": "LayerNorm",
          "name": "layer3_self_attn_layer_norm",
          "layer": "layer_3",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_42",
          "type": "LayerNorm",
          "name": "layer3_final_layer_norm",
          "layer": "layer_3",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_43",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Whisper_Base_op_44",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Whisper_Base_op_45",
          "type": "Linear",
          "name": "layer4_q_proj",
          "layer": "layer_4",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_46",
          "type": "Linear",
          "name": "layer4_k_proj",
          "layer": "layer_4",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_47",
          "type": "Linear",
          "name": "layer4_v_proj",
          "layer": "layer_4",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_48",
          "type": "Linear",
          "name": "layer4_out_proj",
          "layer": "layer_4",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_49",
          "type": "Linear",
          "name": "layer4_fc1",
          "layer": "layer_4",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_50",
          "type": "Linear",
          "name": "layer4_fc2",
          "layer": "layer_4",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_51",
          "type": "LayerNorm",
          "name": "layer4_self_attn_layer_norm",
          "layer": "layer_4",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_52",
          "type": "LayerNorm",
          "name": "layer4_final_layer_norm",
          "layer": "layer_4",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_53",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Whisper_Base_op_54",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Whisper_Base_op_55",
          "type": "Linear",
          "name": "layer5_q_proj",
          "layer": "layer_5",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_56",
          "type": "Linear",
          "name": "layer5_k_proj",
          "layer": "layer_5",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_57",
          "type": "Linear",
          "name": "layer5_v_proj",
          "layer": "layer_5",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_58",
          "type": "Linear",
          "name": "layer5_out_proj",
          "layer": "layer_5",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_59",
          "type": "Linear",
          "name": "layer5_fc1",
          "layer": "layer_5",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_60",
          "type": "Linear",
          "name": "layer5_fc2",
          "layer": "layer_5",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_61",
          "type": "LayerNorm",
          "name": "layer5_self_attn_layer_norm",
          "layer": "layer_5",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_62",
          "type": "LayerNorm",
          "name": "layer5_final_layer_norm",
          "layer": "layer_5",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_63",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Whisper_Base_op_64",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Whisper_Base_op_65",
          "type": "Linear",
          "name": "layer6_q_proj",
          "layer": "layer_6",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_66",
          "type": "Linear",
          "name": "layer6_k_proj",
          "layer": "layer_6",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_67",
          "type": "Linear",
          "name": "layer6_v_proj",
          "layer": "layer_6",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_68",
          "type": "Linear",
          "name": "layer6_out_proj",
          "layer": "layer_6",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_69",
          "type": "Linear",
          "name": "layer6_fc1",
          "layer": "layer_6",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_70",
          "type": "Linear",
          "name": "layer6_fc2",
          "layer": "layer_6",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_71",
          "type": "LayerNorm",
          "name": "layer6_self_attn_layer_norm",
          "layer": "layer_6",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_72",
          "type": "LayerNorm",
          "name": "layer6_final_layer_norm",
          "layer": "layer_6",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_73",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Whisper_Base_op_74",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Whisper_Base_op_75",
          "type": "Linear",
          "name": "layer7_q_proj",
          "layer": "layer_7",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_76",
          "type": "Linear",
          "name": "layer7_k_proj",
          "layer": "layer_7",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_77",
          "type": "Linear",
          "name": "layer7_v_proj",
          "layer": "layer_7",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_78",
          "type": "Linear",
          "name": "layer7_out_proj",
          "layer": "layer_7",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_79",
          "type": "Linear",
          "name": "layer7_fc1",
          "layer": "layer_7",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_80",
          "type": "Linear",
          "name": "layer7_fc2",
          "layer": "layer_7",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_81",
          "type": "LayerNorm",
          "name": "layer7_self_attn_layer_norm",
          "layer": "layer_7",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_82",
          "type": "LayerNorm",
          "name": "layer7_final_layer_norm",
          "layer": "layer_7",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_83",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Whisper_Base_op_84",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Whisper_Base_op_85",
          "type": "Linear",
          "name": "layer8_q_proj",
          "layer": "layer_8",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_86",
          "type": "Linear",
          "name": "layer8_k_proj",
          "layer": "layer_8",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_87",
          "type": "Linear",
          "name": "layer8_v_proj",
          "layer": "layer_8",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_88",
          "type": "Linear",
          "name": "layer8_out_proj",
          "layer": "layer_8",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_89",
          "type": "Linear",
          "name": "layer8_fc1",
          "layer": "layer_8",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_90",
          "type": "Linear",
          "name": "layer8_fc2",
          "layer": "layer_8",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_91",
          "type": "LayerNorm",
          "name": "layer8_self_attn_layer_norm",
          "layer": "layer_8",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_92",
          "type": "LayerNorm",
          "name": "layer8_final_layer_norm",
          "layer": "layer_8",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_93",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Whisper_Base_op_94",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Whisper_Base_op_95",
          "type": "Linear",
          "name": "layer9_q_proj",
          "layer": "layer_9",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_96",
          "type": "Linear",
          "name": "layer9_k_proj",
          "layer": "layer_9",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_97",
          "type": "Linear",
          "name": "layer9_v_proj",
          "layer": "layer_9",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_98",
          "type": "Linear",
          "name": "layer9_out_proj",
          "layer": "layer_9",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_99",
          "type": "Linear",
          "name": "layer9_fc1",
          "layer": "layer_9",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_100",
          "type": "Linear",
          "name": "layer9_fc2",
          "layer": "layer_9",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_101",
          "type": "LayerNorm",
          "name": "layer9_self_attn_layer_norm",
          "layer": "layer_9",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_102",
          "type": "LayerNorm",
          "name": "layer9_final_layer_norm",
          "layer": "layer_9",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_103",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Whisper_Base_op_104",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Whisper_Base_op_105",
          "type": "Linear",
          "name": "layer10_q_proj",
          "layer": "layer_10",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_106",
          "type": "Linear",
          "name": "layer10_k_proj",
          "layer": "layer_10",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_107",
          "type": "Linear",
          "name": "layer10_v_proj",
          "layer": "layer_10",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_108",
          "type": "Linear",
          "name": "layer10_out_proj",
          "layer": "layer_10",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_109",
          "type": "Linear",
          "name": "layer10_fc1",
          "layer": "layer_10",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_110",
          "type": "Linear",
          "name": "layer10_fc2",
          "layer": "layer_10",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_111",
          "type": "LayerNorm",
          "name": "layer10_self_attn_layer_norm",
          "layer": "layer_10",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_112",
          "type": "LayerNorm",
          "name": "layer10_final_layer_norm",
          "layer": "layer_10",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_113",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Whisper_Base_op_114",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "Whisper_Base_op_115",
          "type": "Linear",
          "name": "layer11_q_proj",
          "layer": "layer_11",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_116",
          "type": "Linear",
          "name": "layer11_k_proj",
          "layer": "layer_11",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_117",
          "type": "Linear",
          "name": "layer11_v_proj",
          "layer": "layer_11",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_118",
          "type": "Linear",
          "name": "layer11_out_proj",
          "layer": "layer_11",
          "parameters": 262144,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_119",
          "type": "Linear",
          "name": "layer11_fc1",
          "layer": "layer_11",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_120",
          "type": "Linear",
          "name": "layer11_fc2",
          "layer": "layer_11",
          "parameters": 1048576,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Whisper_Base_op_121",
          "type": "LayerNorm",
          "name": "layer11_self_attn_layer_norm",
          "layer": "layer_11",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_122",
          "type": "LayerNorm",
          "name": "layer11_final_layer_norm",
          "layer": "layer_11",
          "parameters": 1024,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "Whisper_Base_op_123",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Whisper_Base_op_124",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        }
      ],
      "parameters": 74000000
    },
    {
      "model_id": "Wav2Vec2_Base",
      "name": "Wav2Vec2-Base",
      "source": "huggingface/wav2vec2-base",
      "category": "Audio",
      "architecture": "bert_style",
      "num_params": 95000000,
      "num_layers": 12,
      "hidden_size": 768,
      "vocab_size": 32,
      "intermediate_size": 3072,
      "operator_count": 150,
      "operators": [
        {
          "op_id": "Wav2Vec2_Base_op_1",
          "type": "Embedding",
          "name": "word_embeddings",
          "layer": "global",
          "parameters": 24576,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_2",
          "type": "Embedding",
          "name": "position_embeddings",
          "layer": "global",
          "parameters": 24576,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_3",
          "type": "Embedding",
          "name": "token_type_embeddings",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_4",
          "type": "LayerNorm",
          "name": "embeddings_layernorm",
          "layer": "global",
          "parameters": 64,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_5",
          "type": "Linear",
          "name": "pooler_dense",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_6",
          "type": "Tanh",
          "name": "pooler_activation",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_7",
          "type": "Linear",
          "name": "layer0_query",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_8",
          "type": "Linear",
          "name": "layer0_key",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_9",
          "type": "Linear",
          "name": "layer0_value",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_10",
          "type": "Linear",
          "name": "layer0_dense_attention",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_11",
          "type": "Linear",
          "name": "layer0_dense_intermediate",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_12",
          "type": "Linear",
          "name": "layer0_dense_output",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_13",
          "type": "LayerNorm",
          "name": "layer0_attention_layernorm",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_14",
          "type": "LayerNorm",
          "name": "layer0_output_layernorm",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_15",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_16",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_17",
          "type": "Dropout",
          "name": "layer0_attention_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_18",
          "type": "Dropout",
          "name": "layer0_output_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_19",
          "type": "Linear",
          "name": "layer1_query",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_20",
          "type": "Linear",
          "name": "layer1_key",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_21",
          "type": "Linear",
          "name": "layer1_value",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_22",
          "type": "Linear",
          "name": "layer1_dense_attention",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_23",
          "type": "Linear",
          "name": "layer1_dense_intermediate",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_24",
          "type": "Linear",
          "name": "layer1_dense_output",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_25",
          "type": "LayerNorm",
          "name": "layer1_attention_layernorm",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_26",
          "type": "LayerNorm",
          "name": "layer1_output_layernorm",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_27",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_28",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_29",
          "type": "Dropout",
          "name": "layer1_attention_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_30",
          "type": "Dropout",
          "name": "layer1_output_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_31",
          "type": "Linear",
          "name": "layer2_query",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_32",
          "type": "Linear",
          "name": "layer2_key",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_33",
          "type": "Linear",
          "name": "layer2_value",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_34",
          "type": "Linear",
          "name": "layer2_dense_attention",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_35",
          "type": "Linear",
          "name": "layer2_dense_intermediate",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_36",
          "type": "Linear",
          "name": "layer2_dense_output",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_37",
          "type": "LayerNorm",
          "name": "layer2_attention_layernorm",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_38",
          "type": "LayerNorm",
          "name": "layer2_output_layernorm",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_39",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_40",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_41",
          "type": "Dropout",
          "name": "layer2_attention_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_42",
          "type": "Dropout",
          "name": "layer2_output_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_43",
          "type": "Linear",
          "name": "layer3_query",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_44",
          "type": "Linear",
          "name": "layer3_key",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_45",
          "type": "Linear",
          "name": "layer3_value",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_46",
          "type": "Linear",
          "name": "layer3_dense_attention",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_47",
          "type": "Linear",
          "name": "layer3_dense_intermediate",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_48",
          "type": "Linear",
          "name": "layer3_dense_output",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_49",
          "type": "LayerNorm",
          "name": "layer3_attention_layernorm",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_50",
          "type": "LayerNorm",
          "name": "layer3_output_layernorm",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_51",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_52",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_53",
          "type": "Dropout",
          "name": "layer3_attention_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_54",
          "type": "Dropout",
          "name": "layer3_output_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_55",
          "type": "Linear",
          "name": "layer4_query",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_56",
          "type": "Linear",
          "name": "layer4_key",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_57",
          "type": "Linear",
          "name": "layer4_value",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_58",
          "type": "Linear",
          "name": "layer4_dense_attention",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_59",
          "type": "Linear",
          "name": "layer4_dense_intermediate",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_60",
          "type": "Linear",
          "name": "layer4_dense_output",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_61",
          "type": "LayerNorm",
          "name": "layer4_attention_layernorm",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_62",
          "type": "LayerNorm",
          "name": "layer4_output_layernorm",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_63",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_64",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_65",
          "type": "Dropout",
          "name": "layer4_attention_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_66",
          "type": "Dropout",
          "name": "layer4_output_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_67",
          "type": "Linear",
          "name": "layer5_query",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_68",
          "type": "Linear",
          "name": "layer5_key",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_69",
          "type": "Linear",
          "name": "layer5_value",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_70",
          "type": "Linear",
          "name": "layer5_dense_attention",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_71",
          "type": "Linear",
          "name": "layer5_dense_intermediate",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_72",
          "type": "Linear",
          "name": "layer5_dense_output",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_73",
          "type": "LayerNorm",
          "name": "layer5_attention_layernorm",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_74",
          "type": "LayerNorm",
          "name": "layer5_output_layernorm",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_75",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_76",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_77",
          "type": "Dropout",
          "name": "layer5_attention_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_78",
          "type": "Dropout",
          "name": "layer5_output_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_79",
          "type": "Linear",
          "name": "layer6_query",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_80",
          "type": "Linear",
          "name": "layer6_key",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_81",
          "type": "Linear",
          "name": "layer6_value",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_82",
          "type": "Linear",
          "name": "layer6_dense_attention",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_83",
          "type": "Linear",
          "name": "layer6_dense_intermediate",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_84",
          "type": "Linear",
          "name": "layer6_dense_output",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_85",
          "type": "LayerNorm",
          "name": "layer6_attention_layernorm",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_86",
          "type": "LayerNorm",
          "name": "layer6_output_layernorm",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_87",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_88",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_89",
          "type": "Dropout",
          "name": "layer6_attention_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_90",
          "type": "Dropout",
          "name": "layer6_output_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_91",
          "type": "Linear",
          "name": "layer7_query",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_92",
          "type": "Linear",
          "name": "layer7_key",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_93",
          "type": "Linear",
          "name": "layer7_value",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_94",
          "type": "Linear",
          "name": "layer7_dense_attention",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_95",
          "type": "Linear",
          "name": "layer7_dense_intermediate",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_96",
          "type": "Linear",
          "name": "layer7_dense_output",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_97",
          "type": "LayerNorm",
          "name": "layer7_attention_layernorm",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_98",
          "type": "LayerNorm",
          "name": "layer7_output_layernorm",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_99",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_100",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_101",
          "type": "Dropout",
          "name": "layer7_attention_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_102",
          "type": "Dropout",
          "name": "layer7_output_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_103",
          "type": "Linear",
          "name": "layer8_query",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_104",
          "type": "Linear",
          "name": "layer8_key",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_105",
          "type": "Linear",
          "name": "layer8_value",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_106",
          "type": "Linear",
          "name": "layer8_dense_attention",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_107",
          "type": "Linear",
          "name": "layer8_dense_intermediate",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_108",
          "type": "Linear",
          "name": "layer8_dense_output",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_109",
          "type": "LayerNorm",
          "name": "layer8_attention_layernorm",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_110",
          "type": "LayerNorm",
          "name": "layer8_output_layernorm",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_111",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_112",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_113",
          "type": "Dropout",
          "name": "layer8_attention_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_114",
          "type": "Dropout",
          "name": "layer8_output_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_115",
          "type": "Linear",
          "name": "layer9_query",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_116",
          "type": "Linear",
          "name": "layer9_key",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_117",
          "type": "Linear",
          "name": "layer9_value",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_118",
          "type": "Linear",
          "name": "layer9_dense_attention",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_119",
          "type": "Linear",
          "name": "layer9_dense_intermediate",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_120",
          "type": "Linear",
          "name": "layer9_dense_output",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_121",
          "type": "LayerNorm",
          "name": "layer9_attention_layernorm",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_122",
          "type": "LayerNorm",
          "name": "layer9_output_layernorm",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_123",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_124",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_125",
          "type": "Dropout",
          "name": "layer9_attention_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_126",
          "type": "Dropout",
          "name": "layer9_output_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_127",
          "type": "Linear",
          "name": "layer10_query",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_128",
          "type": "Linear",
          "name": "layer10_key",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_129",
          "type": "Linear",
          "name": "layer10_value",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_130",
          "type": "Linear",
          "name": "layer10_dense_attention",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_131",
          "type": "Linear",
          "name": "layer10_dense_intermediate",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_132",
          "type": "Linear",
          "name": "layer10_dense_output",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_133",
          "type": "LayerNorm",
          "name": "layer10_attention_layernorm",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_134",
          "type": "LayerNorm",
          "name": "layer10_output_layernorm",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_135",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_136",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_137",
          "type": "Dropout",
          "name": "layer10_attention_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_138",
          "type": "Dropout",
          "name": "layer10_output_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_139",
          "type": "Linear",
          "name": "layer11_query",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_140",
          "type": "Linear",
          "name": "layer11_key",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_141",
          "type": "Linear",
          "name": "layer11_value",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_142",
          "type": "Linear",
          "name": "layer11_dense_attention",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_143",
          "type": "Linear",
          "name": "layer11_dense_intermediate",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_144",
          "type": "Linear",
          "name": "layer11_dense_output",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_145",
          "type": "LayerNorm",
          "name": "layer11_attention_layernorm",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_146",
          "type": "LayerNorm",
          "name": "layer11_output_layernorm",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_147",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_148",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_149",
          "type": "Dropout",
          "name": "layer11_attention_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "Wav2Vec2_Base_op_150",
          "type": "Dropout",
          "name": "layer11_output_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        }
      ],
      "parameters": 95000000
    },
    {
      "model_id": "CLIP_ViT_B_32",
      "name": "CLIP-ViT-B/32",
      "source": "huggingface/clip-vit-b/32",
      "category": "CV",
      "architecture": "clip_style",
      "num_params": 151000000,
      "num_layers": 12,
      "hidden_size": 768,
      "vocab_size": 49408,
      "intermediate_size": 3072,
      "operator_count": 127,
      "operators": [
        {
          "op_id": "CLIP_ViT_B_32_op_1",
          "type": "Conv2d",
          "name": "patch_embedding",
          "layer": "global",
          "parameters": 5624410669056,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_2",
          "type": "Embedding",
          "name": "token_embedding",
          "layer": "global",
          "parameters": 37945344,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_3",
          "type": "Embedding",
          "name": "position_embedding",
          "layer": "global",
          "parameters": 37945344,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_4",
          "type": "LayerNorm",
          "name": "pre_layernorm",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_5",
          "type": "LayerNorm",
          "name": "post_layernorm",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_6",
          "type": "Linear",
          "name": "visual_projection",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_7",
          "type": "Linear",
          "name": "text_projection",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_8",
          "type": "Linear",
          "name": "layer0_q_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_9",
          "type": "Linear",
          "name": "layer0_k_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_10",
          "type": "Linear",
          "name": "layer0_v_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_11",
          "type": "Linear",
          "name": "layer0_out_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_12",
          "type": "Linear",
          "name": "layer0_fc1",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_13",
          "type": "Linear",
          "name": "layer0_fc2",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_14",
          "type": "LayerNorm",
          "name": "layer0_layer_norm1",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_15",
          "type": "LayerNorm",
          "name": "layer0_layer_norm2",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_16",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_17",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_18",
          "type": "Linear",
          "name": "layer1_q_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_19",
          "type": "Linear",
          "name": "layer1_k_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_20",
          "type": "Linear",
          "name": "layer1_v_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_21",
          "type": "Linear",
          "name": "layer1_out_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_22",
          "type": "Linear",
          "name": "layer1_fc1",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_23",
          "type": "Linear",
          "name": "layer1_fc2",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_24",
          "type": "LayerNorm",
          "name": "layer1_layer_norm1",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_25",
          "type": "LayerNorm",
          "name": "layer1_layer_norm2",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_26",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_27",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_28",
          "type": "Linear",
          "name": "layer2_q_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_29",
          "type": "Linear",
          "name": "layer2_k_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_30",
          "type": "Linear",
          "name": "layer2_v_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_31",
          "type": "Linear",
          "name": "layer2_out_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_32",
          "type": "Linear",
          "name": "layer2_fc1",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_33",
          "type": "Linear",
          "name": "layer2_fc2",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_34",
          "type": "LayerNorm",
          "name": "layer2_layer_norm1",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_35",
          "type": "LayerNorm",
          "name": "layer2_layer_norm2",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_36",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_37",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_38",
          "type": "Linear",
          "name": "layer3_q_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_39",
          "type": "Linear",
          "name": "layer3_k_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_40",
          "type": "Linear",
          "name": "layer3_v_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_41",
          "type": "Linear",
          "name": "layer3_out_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_42",
          "type": "Linear",
          "name": "layer3_fc1",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_43",
          "type": "Linear",
          "name": "layer3_fc2",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_44",
          "type": "LayerNorm",
          "name": "layer3_layer_norm1",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_45",
          "type": "LayerNorm",
          "name": "layer3_layer_norm2",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_46",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_47",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_48",
          "type": "Linear",
          "name": "layer4_q_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_49",
          "type": "Linear",
          "name": "layer4_k_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_50",
          "type": "Linear",
          "name": "layer4_v_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_51",
          "type": "Linear",
          "name": "layer4_out_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_52",
          "type": "Linear",
          "name": "layer4_fc1",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_53",
          "type": "Linear",
          "name": "layer4_fc2",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_54",
          "type": "LayerNorm",
          "name": "layer4_layer_norm1",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_55",
          "type": "LayerNorm",
          "name": "layer4_layer_norm2",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_56",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_57",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_58",
          "type": "Linear",
          "name": "layer5_q_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_59",
          "type": "Linear",
          "name": "layer5_k_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_60",
          "type": "Linear",
          "name": "layer5_v_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_61",
          "type": "Linear",
          "name": "layer5_out_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_62",
          "type": "Linear",
          "name": "layer5_fc1",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_63",
          "type": "Linear",
          "name": "layer5_fc2",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_64",
          "type": "LayerNorm",
          "name": "layer5_layer_norm1",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_65",
          "type": "LayerNorm",
          "name": "layer5_layer_norm2",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_66",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_67",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_68",
          "type": "Linear",
          "name": "layer6_q_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_69",
          "type": "Linear",
          "name": "layer6_k_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_70",
          "type": "Linear",
          "name": "layer6_v_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_71",
          "type": "Linear",
          "name": "layer6_out_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_72",
          "type": "Linear",
          "name": "layer6_fc1",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_73",
          "type": "Linear",
          "name": "layer6_fc2",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_74",
          "type": "LayerNorm",
          "name": "layer6_layer_norm1",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_75",
          "type": "LayerNorm",
          "name": "layer6_layer_norm2",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_76",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_77",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_78",
          "type": "Linear",
          "name": "layer7_q_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_79",
          "type": "Linear",
          "name": "layer7_k_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_80",
          "type": "Linear",
          "name": "layer7_v_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_81",
          "type": "Linear",
          "name": "layer7_out_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_82",
          "type": "Linear",
          "name": "layer7_fc1",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_83",
          "type": "Linear",
          "name": "layer7_fc2",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_84",
          "type": "LayerNorm",
          "name": "layer7_layer_norm1",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_85",
          "type": "LayerNorm",
          "name": "layer7_layer_norm2",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_86",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_87",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_88",
          "type": "Linear",
          "name": "layer8_q_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_89",
          "type": "Linear",
          "name": "layer8_k_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_90",
          "type": "Linear",
          "name": "layer8_v_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_91",
          "type": "Linear",
          "name": "layer8_out_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_92",
          "type": "Linear",
          "name": "layer8_fc1",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_93",
          "type": "Linear",
          "name": "layer8_fc2",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_94",
          "type": "LayerNorm",
          "name": "layer8_layer_norm1",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_95",
          "type": "LayerNorm",
          "name": "layer8_layer_norm2",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_96",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_97",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_98",
          "type": "Linear",
          "name": "layer9_q_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_99",
          "type": "Linear",
          "name": "layer9_k_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_100",
          "type": "Linear",
          "name": "layer9_v_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_101",
          "type": "Linear",
          "name": "layer9_out_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_102",
          "type": "Linear",
          "name": "layer9_fc1",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_103",
          "type": "Linear",
          "name": "layer9_fc2",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_104",
          "type": "LayerNorm",
          "name": "layer9_layer_norm1",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_105",
          "type": "LayerNorm",
          "name": "layer9_layer_norm2",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_106",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_107",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_108",
          "type": "Linear",
          "name": "layer10_q_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_109",
          "type": "Linear",
          "name": "layer10_k_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_110",
          "type": "Linear",
          "name": "layer10_v_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_111",
          "type": "Linear",
          "name": "layer10_out_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_112",
          "type": "Linear",
          "name": "layer10_fc1",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_113",
          "type": "Linear",
          "name": "layer10_fc2",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_114",
          "type": "LayerNorm",
          "name": "layer10_layer_norm1",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_115",
          "type": "LayerNorm",
          "name": "layer10_layer_norm2",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_116",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_117",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_118",
          "type": "Linear",
          "name": "layer11_q_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_119",
          "type": "Linear",
          "name": "layer11_k_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_120",
          "type": "Linear",
          "name": "layer11_v_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_121",
          "type": "Linear",
          "name": "layer11_out_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_122",
          "type": "Linear",
          "name": "layer11_fc1",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_123",
          "type": "Linear",
          "name": "layer11_fc2",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_124",
          "type": "LayerNorm",
          "name": "layer11_layer_norm1",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_125",
          "type": "LayerNorm",
          "name": "layer11_layer_norm2",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_126",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "CLIP_ViT_B_32_op_127",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        }
      ],
      "parameters": 151000000
    },
    {
      "model_id": "BLIP_Base",
      "name": "BLIP-Base",
      "source": "huggingface/blip-base",
      "category": "Multimodal",
      "architecture": "clip_style",
      "num_params": 224000000,
      "num_layers": 12,
      "hidden_size": 768,
      "vocab_size": 30524,
      "intermediate_size": 3072,
      "operator_count": 127,
      "operators": [
        {
          "op_id": "BLIP_Base_op_1",
          "type": "Conv2d",
          "name": "patch_embedding",
          "layer": "global",
          "parameters": 2146670383104,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "BLIP_Base_op_2",
          "type": "Embedding",
          "name": "token_embedding",
          "layer": "global",
          "parameters": 23442432,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_3",
          "type": "Embedding",
          "name": "position_embedding",
          "layer": "global",
          "parameters": 23442432,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_4",
          "type": "LayerNorm",
          "name": "pre_layernorm",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_5",
          "type": "LayerNorm",
          "name": "post_layernorm",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_6",
          "type": "Linear",
          "name": "visual_projection",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_7",
          "type": "Linear",
          "name": "text_projection",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_8",
          "type": "Linear",
          "name": "layer0_q_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_9",
          "type": "Linear",
          "name": "layer0_k_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_10",
          "type": "Linear",
          "name": "layer0_v_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_11",
          "type": "Linear",
          "name": "layer0_out_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_12",
          "type": "Linear",
          "name": "layer0_fc1",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_13",
          "type": "Linear",
          "name": "layer0_fc2",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_14",
          "type": "LayerNorm",
          "name": "layer0_layer_norm1",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_15",
          "type": "LayerNorm",
          "name": "layer0_layer_norm2",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_16",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLIP_Base_op_17",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BLIP_Base_op_18",
          "type": "Linear",
          "name": "layer1_q_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_19",
          "type": "Linear",
          "name": "layer1_k_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_20",
          "type": "Linear",
          "name": "layer1_v_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_21",
          "type": "Linear",
          "name": "layer1_out_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_22",
          "type": "Linear",
          "name": "layer1_fc1",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_23",
          "type": "Linear",
          "name": "layer1_fc2",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_24",
          "type": "LayerNorm",
          "name": "layer1_layer_norm1",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_25",
          "type": "LayerNorm",
          "name": "layer1_layer_norm2",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_26",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLIP_Base_op_27",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BLIP_Base_op_28",
          "type": "Linear",
          "name": "layer2_q_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_29",
          "type": "Linear",
          "name": "layer2_k_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_30",
          "type": "Linear",
          "name": "layer2_v_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_31",
          "type": "Linear",
          "name": "layer2_out_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_32",
          "type": "Linear",
          "name": "layer2_fc1",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_33",
          "type": "Linear",
          "name": "layer2_fc2",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_34",
          "type": "LayerNorm",
          "name": "layer2_layer_norm1",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_35",
          "type": "LayerNorm",
          "name": "layer2_layer_norm2",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_36",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLIP_Base_op_37",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BLIP_Base_op_38",
          "type": "Linear",
          "name": "layer3_q_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_39",
          "type": "Linear",
          "name": "layer3_k_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_40",
          "type": "Linear",
          "name": "layer3_v_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_41",
          "type": "Linear",
          "name": "layer3_out_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_42",
          "type": "Linear",
          "name": "layer3_fc1",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_43",
          "type": "Linear",
          "name": "layer3_fc2",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_44",
          "type": "LayerNorm",
          "name": "layer3_layer_norm1",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_45",
          "type": "LayerNorm",
          "name": "layer3_layer_norm2",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_46",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLIP_Base_op_47",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BLIP_Base_op_48",
          "type": "Linear",
          "name": "layer4_q_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_49",
          "type": "Linear",
          "name": "layer4_k_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_50",
          "type": "Linear",
          "name": "layer4_v_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_51",
          "type": "Linear",
          "name": "layer4_out_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_52",
          "type": "Linear",
          "name": "layer4_fc1",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_53",
          "type": "Linear",
          "name": "layer4_fc2",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_54",
          "type": "LayerNorm",
          "name": "layer4_layer_norm1",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_55",
          "type": "LayerNorm",
          "name": "layer4_layer_norm2",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_56",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLIP_Base_op_57",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BLIP_Base_op_58",
          "type": "Linear",
          "name": "layer5_q_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_59",
          "type": "Linear",
          "name": "layer5_k_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_60",
          "type": "Linear",
          "name": "layer5_v_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_61",
          "type": "Linear",
          "name": "layer5_out_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_62",
          "type": "Linear",
          "name": "layer5_fc1",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_63",
          "type": "Linear",
          "name": "layer5_fc2",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_64",
          "type": "LayerNorm",
          "name": "layer5_layer_norm1",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_65",
          "type": "LayerNorm",
          "name": "layer5_layer_norm2",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_66",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLIP_Base_op_67",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BLIP_Base_op_68",
          "type": "Linear",
          "name": "layer6_q_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_69",
          "type": "Linear",
          "name": "layer6_k_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_70",
          "type": "Linear",
          "name": "layer6_v_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_71",
          "type": "Linear",
          "name": "layer6_out_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_72",
          "type": "Linear",
          "name": "layer6_fc1",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_73",
          "type": "Linear",
          "name": "layer6_fc2",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_74",
          "type": "LayerNorm",
          "name": "layer6_layer_norm1",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_75",
          "type": "LayerNorm",
          "name": "layer6_layer_norm2",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_76",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLIP_Base_op_77",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BLIP_Base_op_78",
          "type": "Linear",
          "name": "layer7_q_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_79",
          "type": "Linear",
          "name": "layer7_k_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_80",
          "type": "Linear",
          "name": "layer7_v_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_81",
          "type": "Linear",
          "name": "layer7_out_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_82",
          "type": "Linear",
          "name": "layer7_fc1",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_83",
          "type": "Linear",
          "name": "layer7_fc2",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_84",
          "type": "LayerNorm",
          "name": "layer7_layer_norm1",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_85",
          "type": "LayerNorm",
          "name": "layer7_layer_norm2",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_86",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLIP_Base_op_87",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BLIP_Base_op_88",
          "type": "Linear",
          "name": "layer8_q_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_89",
          "type": "Linear",
          "name": "layer8_k_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_90",
          "type": "Linear",
          "name": "layer8_v_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_91",
          "type": "Linear",
          "name": "layer8_out_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_92",
          "type": "Linear",
          "name": "layer8_fc1",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_93",
          "type": "Linear",
          "name": "layer8_fc2",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_94",
          "type": "LayerNorm",
          "name": "layer8_layer_norm1",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_95",
          "type": "LayerNorm",
          "name": "layer8_layer_norm2",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_96",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLIP_Base_op_97",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BLIP_Base_op_98",
          "type": "Linear",
          "name": "layer9_q_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_99",
          "type": "Linear",
          "name": "layer9_k_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_100",
          "type": "Linear",
          "name": "layer9_v_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_101",
          "type": "Linear",
          "name": "layer9_out_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_102",
          "type": "Linear",
          "name": "layer9_fc1",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_103",
          "type": "Linear",
          "name": "layer9_fc2",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_104",
          "type": "LayerNorm",
          "name": "layer9_layer_norm1",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_105",
          "type": "LayerNorm",
          "name": "layer9_layer_norm2",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_106",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLIP_Base_op_107",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BLIP_Base_op_108",
          "type": "Linear",
          "name": "layer10_q_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_109",
          "type": "Linear",
          "name": "layer10_k_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_110",
          "type": "Linear",
          "name": "layer10_v_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_111",
          "type": "Linear",
          "name": "layer10_out_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_112",
          "type": "Linear",
          "name": "layer10_fc1",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_113",
          "type": "Linear",
          "name": "layer10_fc2",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_114",
          "type": "LayerNorm",
          "name": "layer10_layer_norm1",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_115",
          "type": "LayerNorm",
          "name": "layer10_layer_norm2",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_116",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLIP_Base_op_117",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "BLIP_Base_op_118",
          "type": "Linear",
          "name": "layer11_q_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_119",
          "type": "Linear",
          "name": "layer11_k_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_120",
          "type": "Linear",
          "name": "layer11_v_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_121",
          "type": "Linear",
          "name": "layer11_out_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_122",
          "type": "Linear",
          "name": "layer11_fc1",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_123",
          "type": "Linear",
          "name": "layer11_fc2",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BLIP_Base_op_124",
          "type": "LayerNorm",
          "name": "layer11_layer_norm1",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_125",
          "type": "LayerNorm",
          "name": "layer11_layer_norm2",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "BLIP_Base_op_126",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BLIP_Base_op_127",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        }
      ],
      "parameters": 224000000
    },
    {
      "model_id": "SigLIP_Base",
      "name": "SigLIP-Base",
      "source": "huggingface/siglip-base",
      "category": "Multimodal",
      "architecture": "clip_style",
      "num_params": 200000000,
      "num_layers": 12,
      "hidden_size": 768,
      "vocab_size": 32000,
      "intermediate_size": 3072,
      "operator_count": 127,
      "operators": [
        {
          "op_id": "SigLIP_Base_op_1",
          "type": "Conv2d",
          "name": "patch_embedding",
          "layer": "global",
          "parameters": 2359296000000,
          "input_shape": "[batch, C_in, H, W]",
          "output_shape": "[batch, C_out, H', W']"
        },
        {
          "op_id": "SigLIP_Base_op_2",
          "type": "Embedding",
          "name": "token_embedding",
          "layer": "global",
          "parameters": 24576000,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_3",
          "type": "Embedding",
          "name": "position_embedding",
          "layer": "global",
          "parameters": 24576000,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_4",
          "type": "LayerNorm",
          "name": "pre_layernorm",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_5",
          "type": "LayerNorm",
          "name": "post_layernorm",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_6",
          "type": "Linear",
          "name": "visual_projection",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_7",
          "type": "Linear",
          "name": "text_projection",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_8",
          "type": "Linear",
          "name": "layer0_q_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_9",
          "type": "Linear",
          "name": "layer0_k_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_10",
          "type": "Linear",
          "name": "layer0_v_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_11",
          "type": "Linear",
          "name": "layer0_out_proj",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_12",
          "type": "Linear",
          "name": "layer0_fc1",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_13",
          "type": "Linear",
          "name": "layer0_fc2",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_14",
          "type": "LayerNorm",
          "name": "layer0_layer_norm1",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_15",
          "type": "LayerNorm",
          "name": "layer0_layer_norm2",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_16",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "SigLIP_Base_op_17",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "SigLIP_Base_op_18",
          "type": "Linear",
          "name": "layer1_q_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_19",
          "type": "Linear",
          "name": "layer1_k_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_20",
          "type": "Linear",
          "name": "layer1_v_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_21",
          "type": "Linear",
          "name": "layer1_out_proj",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_22",
          "type": "Linear",
          "name": "layer1_fc1",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_23",
          "type": "Linear",
          "name": "layer1_fc2",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_24",
          "type": "LayerNorm",
          "name": "layer1_layer_norm1",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_25",
          "type": "LayerNorm",
          "name": "layer1_layer_norm2",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_26",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "SigLIP_Base_op_27",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "SigLIP_Base_op_28",
          "type": "Linear",
          "name": "layer2_q_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_29",
          "type": "Linear",
          "name": "layer2_k_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_30",
          "type": "Linear",
          "name": "layer2_v_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_31",
          "type": "Linear",
          "name": "layer2_out_proj",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_32",
          "type": "Linear",
          "name": "layer2_fc1",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_33",
          "type": "Linear",
          "name": "layer2_fc2",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_34",
          "type": "LayerNorm",
          "name": "layer2_layer_norm1",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_35",
          "type": "LayerNorm",
          "name": "layer2_layer_norm2",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_36",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "SigLIP_Base_op_37",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "SigLIP_Base_op_38",
          "type": "Linear",
          "name": "layer3_q_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_39",
          "type": "Linear",
          "name": "layer3_k_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_40",
          "type": "Linear",
          "name": "layer3_v_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_41",
          "type": "Linear",
          "name": "layer3_out_proj",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_42",
          "type": "Linear",
          "name": "layer3_fc1",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_43",
          "type": "Linear",
          "name": "layer3_fc2",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_44",
          "type": "LayerNorm",
          "name": "layer3_layer_norm1",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_45",
          "type": "LayerNorm",
          "name": "layer3_layer_norm2",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_46",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "SigLIP_Base_op_47",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "SigLIP_Base_op_48",
          "type": "Linear",
          "name": "layer4_q_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_49",
          "type": "Linear",
          "name": "layer4_k_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_50",
          "type": "Linear",
          "name": "layer4_v_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_51",
          "type": "Linear",
          "name": "layer4_out_proj",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_52",
          "type": "Linear",
          "name": "layer4_fc1",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_53",
          "type": "Linear",
          "name": "layer4_fc2",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_54",
          "type": "LayerNorm",
          "name": "layer4_layer_norm1",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_55",
          "type": "LayerNorm",
          "name": "layer4_layer_norm2",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_56",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "SigLIP_Base_op_57",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "SigLIP_Base_op_58",
          "type": "Linear",
          "name": "layer5_q_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_59",
          "type": "Linear",
          "name": "layer5_k_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_60",
          "type": "Linear",
          "name": "layer5_v_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_61",
          "type": "Linear",
          "name": "layer5_out_proj",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_62",
          "type": "Linear",
          "name": "layer5_fc1",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_63",
          "type": "Linear",
          "name": "layer5_fc2",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_64",
          "type": "LayerNorm",
          "name": "layer5_layer_norm1",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_65",
          "type": "LayerNorm",
          "name": "layer5_layer_norm2",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_66",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "SigLIP_Base_op_67",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "SigLIP_Base_op_68",
          "type": "Linear",
          "name": "layer6_q_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_69",
          "type": "Linear",
          "name": "layer6_k_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_70",
          "type": "Linear",
          "name": "layer6_v_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_71",
          "type": "Linear",
          "name": "layer6_out_proj",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_72",
          "type": "Linear",
          "name": "layer6_fc1",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_73",
          "type": "Linear",
          "name": "layer6_fc2",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_74",
          "type": "LayerNorm",
          "name": "layer6_layer_norm1",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_75",
          "type": "LayerNorm",
          "name": "layer6_layer_norm2",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_76",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "SigLIP_Base_op_77",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "SigLIP_Base_op_78",
          "type": "Linear",
          "name": "layer7_q_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_79",
          "type": "Linear",
          "name": "layer7_k_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_80",
          "type": "Linear",
          "name": "layer7_v_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_81",
          "type": "Linear",
          "name": "layer7_out_proj",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_82",
          "type": "Linear",
          "name": "layer7_fc1",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_83",
          "type": "Linear",
          "name": "layer7_fc2",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_84",
          "type": "LayerNorm",
          "name": "layer7_layer_norm1",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_85",
          "type": "LayerNorm",
          "name": "layer7_layer_norm2",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_86",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "SigLIP_Base_op_87",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "SigLIP_Base_op_88",
          "type": "Linear",
          "name": "layer8_q_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_89",
          "type": "Linear",
          "name": "layer8_k_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_90",
          "type": "Linear",
          "name": "layer8_v_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_91",
          "type": "Linear",
          "name": "layer8_out_proj",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_92",
          "type": "Linear",
          "name": "layer8_fc1",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_93",
          "type": "Linear",
          "name": "layer8_fc2",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_94",
          "type": "LayerNorm",
          "name": "layer8_layer_norm1",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_95",
          "type": "LayerNorm",
          "name": "layer8_layer_norm2",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_96",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "SigLIP_Base_op_97",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "SigLIP_Base_op_98",
          "type": "Linear",
          "name": "layer9_q_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_99",
          "type": "Linear",
          "name": "layer9_k_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_100",
          "type": "Linear",
          "name": "layer9_v_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_101",
          "type": "Linear",
          "name": "layer9_out_proj",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_102",
          "type": "Linear",
          "name": "layer9_fc1",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_103",
          "type": "Linear",
          "name": "layer9_fc2",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_104",
          "type": "LayerNorm",
          "name": "layer9_layer_norm1",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_105",
          "type": "LayerNorm",
          "name": "layer9_layer_norm2",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_106",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "SigLIP_Base_op_107",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "SigLIP_Base_op_108",
          "type": "Linear",
          "name": "layer10_q_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_109",
          "type": "Linear",
          "name": "layer10_k_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_110",
          "type": "Linear",
          "name": "layer10_v_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_111",
          "type": "Linear",
          "name": "layer10_out_proj",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_112",
          "type": "Linear",
          "name": "layer10_fc1",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_113",
          "type": "Linear",
          "name": "layer10_fc2",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_114",
          "type": "LayerNorm",
          "name": "layer10_layer_norm1",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_115",
          "type": "LayerNorm",
          "name": "layer10_layer_norm2",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_116",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "SigLIP_Base_op_117",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "SigLIP_Base_op_118",
          "type": "Linear",
          "name": "layer11_q_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_119",
          "type": "Linear",
          "name": "layer11_k_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_120",
          "type": "Linear",
          "name": "layer11_v_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_121",
          "type": "Linear",
          "name": "layer11_out_proj",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_122",
          "type": "Linear",
          "name": "layer11_fc1",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_123",
          "type": "Linear",
          "name": "layer11_fc2",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "SigLIP_Base_op_124",
          "type": "LayerNorm",
          "name": "layer11_layer_norm1",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_125",
          "type": "LayerNorm",
          "name": "layer11_layer_norm2",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "SigLIP_Base_op_126",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "SigLIP_Base_op_127",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        }
      ],
      "parameters": 86000000
    },
    {
      "model_id": "DistilBERT_Base",
      "name": "DistilBERT-Base",
      "source": "huggingface/distilbert-base",
      "category": "NLP",
      "architecture": "bert_style",
      "num_params": 66000000,
      "num_layers": 6,
      "hidden_size": 768,
      "vocab_size": 30522,
      "intermediate_size": 3072,
      "operator_count": 78,
      "operators": [
        {
          "op_id": "DistilBERT_Base_op_1",
          "type": "Embedding",
          "name": "word_embeddings",
          "layer": "global",
          "parameters": 23440896,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_2",
          "type": "Embedding",
          "name": "position_embeddings",
          "layer": "global",
          "parameters": 23440896,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_3",
          "type": "Embedding",
          "name": "token_type_embeddings",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_4",
          "type": "LayerNorm",
          "name": "embeddings_layernorm",
          "layer": "global",
          "parameters": 61044,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_5",
          "type": "Linear",
          "name": "pooler_dense",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_6",
          "type": "Tanh",
          "name": "pooler_activation",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_7",
          "type": "Linear",
          "name": "layer0_query",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_8",
          "type": "Linear",
          "name": "layer0_key",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_9",
          "type": "Linear",
          "name": "layer0_value",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_10",
          "type": "Linear",
          "name": "layer0_dense_attention",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_11",
          "type": "Linear",
          "name": "layer0_dense_intermediate",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_12",
          "type": "Linear",
          "name": "layer0_dense_output",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_13",
          "type": "LayerNorm",
          "name": "layer0_attention_layernorm",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_14",
          "type": "LayerNorm",
          "name": "layer0_output_layernorm",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_15",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DistilBERT_Base_op_16",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DistilBERT_Base_op_17",
          "type": "Dropout",
          "name": "layer0_attention_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_18",
          "type": "Dropout",
          "name": "layer0_output_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_19",
          "type": "Linear",
          "name": "layer1_query",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_20",
          "type": "Linear",
          "name": "layer1_key",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_21",
          "type": "Linear",
          "name": "layer1_value",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_22",
          "type": "Linear",
          "name": "layer1_dense_attention",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_23",
          "type": "Linear",
          "name": "layer1_dense_intermediate",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_24",
          "type": "Linear",
          "name": "layer1_dense_output",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_25",
          "type": "LayerNorm",
          "name": "layer1_attention_layernorm",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_26",
          "type": "LayerNorm",
          "name": "layer1_output_layernorm",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_27",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DistilBERT_Base_op_28",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DistilBERT_Base_op_29",
          "type": "Dropout",
          "name": "layer1_attention_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_30",
          "type": "Dropout",
          "name": "layer1_output_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_31",
          "type": "Linear",
          "name": "layer2_query",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_32",
          "type": "Linear",
          "name": "layer2_key",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_33",
          "type": "Linear",
          "name": "layer2_value",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_34",
          "type": "Linear",
          "name": "layer2_dense_attention",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_35",
          "type": "Linear",
          "name": "layer2_dense_intermediate",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_36",
          "type": "Linear",
          "name": "layer2_dense_output",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_37",
          "type": "LayerNorm",
          "name": "layer2_attention_layernorm",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_38",
          "type": "LayerNorm",
          "name": "layer2_output_layernorm",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_39",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DistilBERT_Base_op_40",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DistilBERT_Base_op_41",
          "type": "Dropout",
          "name": "layer2_attention_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_42",
          "type": "Dropout",
          "name": "layer2_output_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_43",
          "type": "Linear",
          "name": "layer3_query",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_44",
          "type": "Linear",
          "name": "layer3_key",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_45",
          "type": "Linear",
          "name": "layer3_value",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_46",
          "type": "Linear",
          "name": "layer3_dense_attention",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_47",
          "type": "Linear",
          "name": "layer3_dense_intermediate",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_48",
          "type": "Linear",
          "name": "layer3_dense_output",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_49",
          "type": "LayerNorm",
          "name": "layer3_attention_layernorm",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_50",
          "type": "LayerNorm",
          "name": "layer3_output_layernorm",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_51",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DistilBERT_Base_op_52",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DistilBERT_Base_op_53",
          "type": "Dropout",
          "name": "layer3_attention_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_54",
          "type": "Dropout",
          "name": "layer3_output_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_55",
          "type": "Linear",
          "name": "layer4_query",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_56",
          "type": "Linear",
          "name": "layer4_key",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_57",
          "type": "Linear",
          "name": "layer4_value",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_58",
          "type": "Linear",
          "name": "layer4_dense_attention",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_59",
          "type": "Linear",
          "name": "layer4_dense_intermediate",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_60",
          "type": "Linear",
          "name": "layer4_dense_output",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_61",
          "type": "LayerNorm",
          "name": "layer4_attention_layernorm",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_62",
          "type": "LayerNorm",
          "name": "layer4_output_layernorm",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_63",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DistilBERT_Base_op_64",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DistilBERT_Base_op_65",
          "type": "Dropout",
          "name": "layer4_attention_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_66",
          "type": "Dropout",
          "name": "layer4_output_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_67",
          "type": "Linear",
          "name": "layer5_query",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_68",
          "type": "Linear",
          "name": "layer5_key",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_69",
          "type": "Linear",
          "name": "layer5_value",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_70",
          "type": "Linear",
          "name": "layer5_dense_attention",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_71",
          "type": "Linear",
          "name": "layer5_dense_intermediate",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_72",
          "type": "Linear",
          "name": "layer5_dense_output",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "DistilBERT_Base_op_73",
          "type": "LayerNorm",
          "name": "layer5_attention_layernorm",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_74",
          "type": "LayerNorm",
          "name": "layer5_output_layernorm",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_75",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "DistilBERT_Base_op_76",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "DistilBERT_Base_op_77",
          "type": "Dropout",
          "name": "layer5_attention_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "DistilBERT_Base_op_78",
          "type": "Dropout",
          "name": "layer5_output_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        }
      ],
      "parameters": 66000000
    },
    {
      "model_id": "ALBERT_Base",
      "name": "ALBERT-Base",
      "source": "huggingface/albert-base",
      "category": "NLP",
      "architecture": "bert_style",
      "num_params": 12000000,
      "num_layers": 12,
      "hidden_size": 768,
      "vocab_size": 30000,
      "intermediate_size": 3072,
      "operator_count": 150,
      "operators": [
        {
          "op_id": "ALBERT_Base_op_1",
          "type": "Embedding",
          "name": "word_embeddings",
          "layer": "global",
          "parameters": 23040000,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_2",
          "type": "Embedding",
          "name": "position_embeddings",
          "layer": "global",
          "parameters": 23040000,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_3",
          "type": "Embedding",
          "name": "token_type_embeddings",
          "layer": "global",
          "parameters": 1536,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_4",
          "type": "LayerNorm",
          "name": "embeddings_layernorm",
          "layer": "global",
          "parameters": 60000,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_5",
          "type": "Linear",
          "name": "pooler_dense",
          "layer": "global",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_6",
          "type": "Tanh",
          "name": "pooler_activation",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_7",
          "type": "Linear",
          "name": "layer0_query",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_8",
          "type": "Linear",
          "name": "layer0_key",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_9",
          "type": "Linear",
          "name": "layer0_value",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_10",
          "type": "Linear",
          "name": "layer0_dense_attention",
          "layer": "layer_0",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_11",
          "type": "Linear",
          "name": "layer0_dense_intermediate",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_12",
          "type": "Linear",
          "name": "layer0_dense_output",
          "layer": "layer_0",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_13",
          "type": "LayerNorm",
          "name": "layer0_attention_layernorm",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_14",
          "type": "LayerNorm",
          "name": "layer0_output_layernorm",
          "layer": "layer_0",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_15",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ALBERT_Base_op_16",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ALBERT_Base_op_17",
          "type": "Dropout",
          "name": "layer0_attention_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_18",
          "type": "Dropout",
          "name": "layer0_output_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_19",
          "type": "Linear",
          "name": "layer1_query",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_20",
          "type": "Linear",
          "name": "layer1_key",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_21",
          "type": "Linear",
          "name": "layer1_value",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_22",
          "type": "Linear",
          "name": "layer1_dense_attention",
          "layer": "layer_1",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_23",
          "type": "Linear",
          "name": "layer1_dense_intermediate",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_24",
          "type": "Linear",
          "name": "layer1_dense_output",
          "layer": "layer_1",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_25",
          "type": "LayerNorm",
          "name": "layer1_attention_layernorm",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_26",
          "type": "LayerNorm",
          "name": "layer1_output_layernorm",
          "layer": "layer_1",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_27",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ALBERT_Base_op_28",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ALBERT_Base_op_29",
          "type": "Dropout",
          "name": "layer1_attention_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_30",
          "type": "Dropout",
          "name": "layer1_output_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_31",
          "type": "Linear",
          "name": "layer2_query",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_32",
          "type": "Linear",
          "name": "layer2_key",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_33",
          "type": "Linear",
          "name": "layer2_value",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_34",
          "type": "Linear",
          "name": "layer2_dense_attention",
          "layer": "layer_2",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_35",
          "type": "Linear",
          "name": "layer2_dense_intermediate",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_36",
          "type": "Linear",
          "name": "layer2_dense_output",
          "layer": "layer_2",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_37",
          "type": "LayerNorm",
          "name": "layer2_attention_layernorm",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_38",
          "type": "LayerNorm",
          "name": "layer2_output_layernorm",
          "layer": "layer_2",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_39",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ALBERT_Base_op_40",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ALBERT_Base_op_41",
          "type": "Dropout",
          "name": "layer2_attention_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_42",
          "type": "Dropout",
          "name": "layer2_output_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_43",
          "type": "Linear",
          "name": "layer3_query",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_44",
          "type": "Linear",
          "name": "layer3_key",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_45",
          "type": "Linear",
          "name": "layer3_value",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_46",
          "type": "Linear",
          "name": "layer3_dense_attention",
          "layer": "layer_3",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_47",
          "type": "Linear",
          "name": "layer3_dense_intermediate",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_48",
          "type": "Linear",
          "name": "layer3_dense_output",
          "layer": "layer_3",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_49",
          "type": "LayerNorm",
          "name": "layer3_attention_layernorm",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_50",
          "type": "LayerNorm",
          "name": "layer3_output_layernorm",
          "layer": "layer_3",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_51",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ALBERT_Base_op_52",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ALBERT_Base_op_53",
          "type": "Dropout",
          "name": "layer3_attention_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_54",
          "type": "Dropout",
          "name": "layer3_output_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_55",
          "type": "Linear",
          "name": "layer4_query",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_56",
          "type": "Linear",
          "name": "layer4_key",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_57",
          "type": "Linear",
          "name": "layer4_value",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_58",
          "type": "Linear",
          "name": "layer4_dense_attention",
          "layer": "layer_4",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_59",
          "type": "Linear",
          "name": "layer4_dense_intermediate",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_60",
          "type": "Linear",
          "name": "layer4_dense_output",
          "layer": "layer_4",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_61",
          "type": "LayerNorm",
          "name": "layer4_attention_layernorm",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_62",
          "type": "LayerNorm",
          "name": "layer4_output_layernorm",
          "layer": "layer_4",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_63",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ALBERT_Base_op_64",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ALBERT_Base_op_65",
          "type": "Dropout",
          "name": "layer4_attention_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_66",
          "type": "Dropout",
          "name": "layer4_output_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_67",
          "type": "Linear",
          "name": "layer5_query",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_68",
          "type": "Linear",
          "name": "layer5_key",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_69",
          "type": "Linear",
          "name": "layer5_value",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_70",
          "type": "Linear",
          "name": "layer5_dense_attention",
          "layer": "layer_5",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_71",
          "type": "Linear",
          "name": "layer5_dense_intermediate",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_72",
          "type": "Linear",
          "name": "layer5_dense_output",
          "layer": "layer_5",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_73",
          "type": "LayerNorm",
          "name": "layer5_attention_layernorm",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_74",
          "type": "LayerNorm",
          "name": "layer5_output_layernorm",
          "layer": "layer_5",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_75",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ALBERT_Base_op_76",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ALBERT_Base_op_77",
          "type": "Dropout",
          "name": "layer5_attention_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_78",
          "type": "Dropout",
          "name": "layer5_output_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_79",
          "type": "Linear",
          "name": "layer6_query",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_80",
          "type": "Linear",
          "name": "layer6_key",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_81",
          "type": "Linear",
          "name": "layer6_value",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_82",
          "type": "Linear",
          "name": "layer6_dense_attention",
          "layer": "layer_6",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_83",
          "type": "Linear",
          "name": "layer6_dense_intermediate",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_84",
          "type": "Linear",
          "name": "layer6_dense_output",
          "layer": "layer_6",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_85",
          "type": "LayerNorm",
          "name": "layer6_attention_layernorm",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_86",
          "type": "LayerNorm",
          "name": "layer6_output_layernorm",
          "layer": "layer_6",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_87",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ALBERT_Base_op_88",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ALBERT_Base_op_89",
          "type": "Dropout",
          "name": "layer6_attention_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_90",
          "type": "Dropout",
          "name": "layer6_output_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_91",
          "type": "Linear",
          "name": "layer7_query",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_92",
          "type": "Linear",
          "name": "layer7_key",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_93",
          "type": "Linear",
          "name": "layer7_value",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_94",
          "type": "Linear",
          "name": "layer7_dense_attention",
          "layer": "layer_7",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_95",
          "type": "Linear",
          "name": "layer7_dense_intermediate",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_96",
          "type": "Linear",
          "name": "layer7_dense_output",
          "layer": "layer_7",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_97",
          "type": "LayerNorm",
          "name": "layer7_attention_layernorm",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_98",
          "type": "LayerNorm",
          "name": "layer7_output_layernorm",
          "layer": "layer_7",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_99",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ALBERT_Base_op_100",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ALBERT_Base_op_101",
          "type": "Dropout",
          "name": "layer7_attention_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_102",
          "type": "Dropout",
          "name": "layer7_output_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_103",
          "type": "Linear",
          "name": "layer8_query",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_104",
          "type": "Linear",
          "name": "layer8_key",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_105",
          "type": "Linear",
          "name": "layer8_value",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_106",
          "type": "Linear",
          "name": "layer8_dense_attention",
          "layer": "layer_8",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_107",
          "type": "Linear",
          "name": "layer8_dense_intermediate",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_108",
          "type": "Linear",
          "name": "layer8_dense_output",
          "layer": "layer_8",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_109",
          "type": "LayerNorm",
          "name": "layer8_attention_layernorm",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_110",
          "type": "LayerNorm",
          "name": "layer8_output_layernorm",
          "layer": "layer_8",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_111",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ALBERT_Base_op_112",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ALBERT_Base_op_113",
          "type": "Dropout",
          "name": "layer8_attention_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_114",
          "type": "Dropout",
          "name": "layer8_output_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_115",
          "type": "Linear",
          "name": "layer9_query",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_116",
          "type": "Linear",
          "name": "layer9_key",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_117",
          "type": "Linear",
          "name": "layer9_value",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_118",
          "type": "Linear",
          "name": "layer9_dense_attention",
          "layer": "layer_9",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_119",
          "type": "Linear",
          "name": "layer9_dense_intermediate",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_120",
          "type": "Linear",
          "name": "layer9_dense_output",
          "layer": "layer_9",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_121",
          "type": "LayerNorm",
          "name": "layer9_attention_layernorm",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_122",
          "type": "LayerNorm",
          "name": "layer9_output_layernorm",
          "layer": "layer_9",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_123",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ALBERT_Base_op_124",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ALBERT_Base_op_125",
          "type": "Dropout",
          "name": "layer9_attention_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_126",
          "type": "Dropout",
          "name": "layer9_output_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_127",
          "type": "Linear",
          "name": "layer10_query",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_128",
          "type": "Linear",
          "name": "layer10_key",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_129",
          "type": "Linear",
          "name": "layer10_value",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_130",
          "type": "Linear",
          "name": "layer10_dense_attention",
          "layer": "layer_10",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_131",
          "type": "Linear",
          "name": "layer10_dense_intermediate",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_132",
          "type": "Linear",
          "name": "layer10_dense_output",
          "layer": "layer_10",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_133",
          "type": "LayerNorm",
          "name": "layer10_attention_layernorm",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_134",
          "type": "LayerNorm",
          "name": "layer10_output_layernorm",
          "layer": "layer_10",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_135",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ALBERT_Base_op_136",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ALBERT_Base_op_137",
          "type": "Dropout",
          "name": "layer10_attention_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_138",
          "type": "Dropout",
          "name": "layer10_output_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_139",
          "type": "Linear",
          "name": "layer11_query",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_140",
          "type": "Linear",
          "name": "layer11_key",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_141",
          "type": "Linear",
          "name": "layer11_value",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_142",
          "type": "Linear",
          "name": "layer11_dense_attention",
          "layer": "layer_11",
          "parameters": 589824,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_143",
          "type": "Linear",
          "name": "layer11_dense_intermediate",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_144",
          "type": "Linear",
          "name": "layer11_dense_output",
          "layer": "layer_11",
          "parameters": 2359296,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "ALBERT_Base_op_145",
          "type": "LayerNorm",
          "name": "layer11_attention_layernorm",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_146",
          "type": "LayerNorm",
          "name": "layer11_output_layernorm",
          "layer": "layer_11",
          "parameters": 1536,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_147",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "ALBERT_Base_op_148",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 3072]",
          "output_shape": "[batch, seq, 3072]"
        },
        {
          "op_id": "ALBERT_Base_op_149",
          "type": "Dropout",
          "name": "layer11_attention_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        },
        {
          "op_id": "ALBERT_Base_op_150",
          "type": "Dropout",
          "name": "layer11_output_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 768]",
          "output_shape": "[batch, seq, 768]"
        }
      ],
      "parameters": 12000000
    },
    {
      "model_id": "GPT_Neo_1.3B",
      "name": "GPT-Neo-1.3B",
      "source": "huggingface/gpt-neo-1.3b",
      "category": "LLM",
      "architecture": "gpt2_style",
      "num_params": 1300000000,
      "num_layers": 24,
      "hidden_size": 2048,
      "vocab_size": 50257,
      "intermediate_size": 8192,
      "operator_count": 243,
      "operators": [
        {
          "op_id": "GPT_Neo_1.3B_op_1",
          "type": "Embedding",
          "name": "wte",
          "layer": "global",
          "parameters": 4194304,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_2",
          "type": "Embedding",
          "name": "wpe",
          "layer": "global",
          "parameters": 4194304,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_3",
          "type": "LayerNorm",
          "name": "ln_f",
          "layer": "global",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_4",
          "type": "Linear",
          "name": "layer0_c_attn",
          "layer": "layer_0",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_5",
          "type": "Linear",
          "name": "layer0_c_proj",
          "layer": "layer_0",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_6",
          "type": "Linear",
          "name": "layer0_c_fc",
          "layer": "layer_0",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_7",
          "type": "Linear",
          "name": "layer0_c_proj_mlp",
          "layer": "layer_0",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_8",
          "type": "LayerNorm",
          "name": "layer0_ln_1",
          "layer": "layer_0",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_9",
          "type": "LayerNorm",
          "name": "layer0_ln_2",
          "layer": "layer_0",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_10",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_11",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_12",
          "type": "Dropout",
          "name": "layer0_attn_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_13",
          "type": "Dropout",
          "name": "layer0_resid_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_14",
          "type": "Linear",
          "name": "layer1_c_attn",
          "layer": "layer_1",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_15",
          "type": "Linear",
          "name": "layer1_c_proj",
          "layer": "layer_1",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_16",
          "type": "Linear",
          "name": "layer1_c_fc",
          "layer": "layer_1",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_17",
          "type": "Linear",
          "name": "layer1_c_proj_mlp",
          "layer": "layer_1",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_18",
          "type": "LayerNorm",
          "name": "layer1_ln_1",
          "layer": "layer_1",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_19",
          "type": "LayerNorm",
          "name": "layer1_ln_2",
          "layer": "layer_1",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_20",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_21",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_22",
          "type": "Dropout",
          "name": "layer1_attn_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_23",
          "type": "Dropout",
          "name": "layer1_resid_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_24",
          "type": "Linear",
          "name": "layer2_c_attn",
          "layer": "layer_2",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_25",
          "type": "Linear",
          "name": "layer2_c_proj",
          "layer": "layer_2",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_26",
          "type": "Linear",
          "name": "layer2_c_fc",
          "layer": "layer_2",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_27",
          "type": "Linear",
          "name": "layer2_c_proj_mlp",
          "layer": "layer_2",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_28",
          "type": "LayerNorm",
          "name": "layer2_ln_1",
          "layer": "layer_2",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_29",
          "type": "LayerNorm",
          "name": "layer2_ln_2",
          "layer": "layer_2",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_30",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_31",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_32",
          "type": "Dropout",
          "name": "layer2_attn_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_33",
          "type": "Dropout",
          "name": "layer2_resid_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_34",
          "type": "Linear",
          "name": "layer3_c_attn",
          "layer": "layer_3",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_35",
          "type": "Linear",
          "name": "layer3_c_proj",
          "layer": "layer_3",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_36",
          "type": "Linear",
          "name": "layer3_c_fc",
          "layer": "layer_3",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_37",
          "type": "Linear",
          "name": "layer3_c_proj_mlp",
          "layer": "layer_3",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_38",
          "type": "LayerNorm",
          "name": "layer3_ln_1",
          "layer": "layer_3",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_39",
          "type": "LayerNorm",
          "name": "layer3_ln_2",
          "layer": "layer_3",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_40",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_41",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_42",
          "type": "Dropout",
          "name": "layer3_attn_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_43",
          "type": "Dropout",
          "name": "layer3_resid_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_44",
          "type": "Linear",
          "name": "layer4_c_attn",
          "layer": "layer_4",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_45",
          "type": "Linear",
          "name": "layer4_c_proj",
          "layer": "layer_4",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_46",
          "type": "Linear",
          "name": "layer4_c_fc",
          "layer": "layer_4",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_47",
          "type": "Linear",
          "name": "layer4_c_proj_mlp",
          "layer": "layer_4",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_48",
          "type": "LayerNorm",
          "name": "layer4_ln_1",
          "layer": "layer_4",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_49",
          "type": "LayerNorm",
          "name": "layer4_ln_2",
          "layer": "layer_4",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_50",
          "type": "Softmax",
          "name": "layer4_attention_softmax",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_51",
          "type": "GELU",
          "name": "layer4_activation",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_52",
          "type": "Dropout",
          "name": "layer4_attn_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_53",
          "type": "Dropout",
          "name": "layer4_resid_dropout",
          "layer": "layer_4",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_54",
          "type": "Linear",
          "name": "layer5_c_attn",
          "layer": "layer_5",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_55",
          "type": "Linear",
          "name": "layer5_c_proj",
          "layer": "layer_5",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_56",
          "type": "Linear",
          "name": "layer5_c_fc",
          "layer": "layer_5",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_57",
          "type": "Linear",
          "name": "layer5_c_proj_mlp",
          "layer": "layer_5",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_58",
          "type": "LayerNorm",
          "name": "layer5_ln_1",
          "layer": "layer_5",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_59",
          "type": "LayerNorm",
          "name": "layer5_ln_2",
          "layer": "layer_5",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_60",
          "type": "Softmax",
          "name": "layer5_attention_softmax",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_61",
          "type": "GELU",
          "name": "layer5_activation",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_62",
          "type": "Dropout",
          "name": "layer5_attn_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_63",
          "type": "Dropout",
          "name": "layer5_resid_dropout",
          "layer": "layer_5",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_64",
          "type": "Linear",
          "name": "layer6_c_attn",
          "layer": "layer_6",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_65",
          "type": "Linear",
          "name": "layer6_c_proj",
          "layer": "layer_6",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_66",
          "type": "Linear",
          "name": "layer6_c_fc",
          "layer": "layer_6",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_67",
          "type": "Linear",
          "name": "layer6_c_proj_mlp",
          "layer": "layer_6",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_68",
          "type": "LayerNorm",
          "name": "layer6_ln_1",
          "layer": "layer_6",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_69",
          "type": "LayerNorm",
          "name": "layer6_ln_2",
          "layer": "layer_6",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_70",
          "type": "Softmax",
          "name": "layer6_attention_softmax",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_71",
          "type": "GELU",
          "name": "layer6_activation",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_72",
          "type": "Dropout",
          "name": "layer6_attn_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_73",
          "type": "Dropout",
          "name": "layer6_resid_dropout",
          "layer": "layer_6",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_74",
          "type": "Linear",
          "name": "layer7_c_attn",
          "layer": "layer_7",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_75",
          "type": "Linear",
          "name": "layer7_c_proj",
          "layer": "layer_7",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_76",
          "type": "Linear",
          "name": "layer7_c_fc",
          "layer": "layer_7",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_77",
          "type": "Linear",
          "name": "layer7_c_proj_mlp",
          "layer": "layer_7",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_78",
          "type": "LayerNorm",
          "name": "layer7_ln_1",
          "layer": "layer_7",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_79",
          "type": "LayerNorm",
          "name": "layer7_ln_2",
          "layer": "layer_7",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_80",
          "type": "Softmax",
          "name": "layer7_attention_softmax",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_81",
          "type": "GELU",
          "name": "layer7_activation",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_82",
          "type": "Dropout",
          "name": "layer7_attn_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_83",
          "type": "Dropout",
          "name": "layer7_resid_dropout",
          "layer": "layer_7",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_84",
          "type": "Linear",
          "name": "layer8_c_attn",
          "layer": "layer_8",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_85",
          "type": "Linear",
          "name": "layer8_c_proj",
          "layer": "layer_8",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_86",
          "type": "Linear",
          "name": "layer8_c_fc",
          "layer": "layer_8",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_87",
          "type": "Linear",
          "name": "layer8_c_proj_mlp",
          "layer": "layer_8",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_88",
          "type": "LayerNorm",
          "name": "layer8_ln_1",
          "layer": "layer_8",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_89",
          "type": "LayerNorm",
          "name": "layer8_ln_2",
          "layer": "layer_8",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_90",
          "type": "Softmax",
          "name": "layer8_attention_softmax",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_91",
          "type": "GELU",
          "name": "layer8_activation",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_92",
          "type": "Dropout",
          "name": "layer8_attn_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_93",
          "type": "Dropout",
          "name": "layer8_resid_dropout",
          "layer": "layer_8",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_94",
          "type": "Linear",
          "name": "layer9_c_attn",
          "layer": "layer_9",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_95",
          "type": "Linear",
          "name": "layer9_c_proj",
          "layer": "layer_9",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_96",
          "type": "Linear",
          "name": "layer9_c_fc",
          "layer": "layer_9",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_97",
          "type": "Linear",
          "name": "layer9_c_proj_mlp",
          "layer": "layer_9",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_98",
          "type": "LayerNorm",
          "name": "layer9_ln_1",
          "layer": "layer_9",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_99",
          "type": "LayerNorm",
          "name": "layer9_ln_2",
          "layer": "layer_9",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_100",
          "type": "Softmax",
          "name": "layer9_attention_softmax",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_101",
          "type": "GELU",
          "name": "layer9_activation",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_102",
          "type": "Dropout",
          "name": "layer9_attn_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_103",
          "type": "Dropout",
          "name": "layer9_resid_dropout",
          "layer": "layer_9",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_104",
          "type": "Linear",
          "name": "layer10_c_attn",
          "layer": "layer_10",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_105",
          "type": "Linear",
          "name": "layer10_c_proj",
          "layer": "layer_10",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_106",
          "type": "Linear",
          "name": "layer10_c_fc",
          "layer": "layer_10",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_107",
          "type": "Linear",
          "name": "layer10_c_proj_mlp",
          "layer": "layer_10",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_108",
          "type": "LayerNorm",
          "name": "layer10_ln_1",
          "layer": "layer_10",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_109",
          "type": "LayerNorm",
          "name": "layer10_ln_2",
          "layer": "layer_10",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_110",
          "type": "Softmax",
          "name": "layer10_attention_softmax",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_111",
          "type": "GELU",
          "name": "layer10_activation",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_112",
          "type": "Dropout",
          "name": "layer10_attn_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_113",
          "type": "Dropout",
          "name": "layer10_resid_dropout",
          "layer": "layer_10",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_114",
          "type": "Linear",
          "name": "layer11_c_attn",
          "layer": "layer_11",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_115",
          "type": "Linear",
          "name": "layer11_c_proj",
          "layer": "layer_11",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_116",
          "type": "Linear",
          "name": "layer11_c_fc",
          "layer": "layer_11",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_117",
          "type": "Linear",
          "name": "layer11_c_proj_mlp",
          "layer": "layer_11",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_118",
          "type": "LayerNorm",
          "name": "layer11_ln_1",
          "layer": "layer_11",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_119",
          "type": "LayerNorm",
          "name": "layer11_ln_2",
          "layer": "layer_11",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_120",
          "type": "Softmax",
          "name": "layer11_attention_softmax",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_121",
          "type": "GELU",
          "name": "layer11_activation",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_122",
          "type": "Dropout",
          "name": "layer11_attn_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_123",
          "type": "Dropout",
          "name": "layer11_resid_dropout",
          "layer": "layer_11",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_124",
          "type": "Linear",
          "name": "layer12_c_attn",
          "layer": "layer_12",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_125",
          "type": "Linear",
          "name": "layer12_c_proj",
          "layer": "layer_12",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_126",
          "type": "Linear",
          "name": "layer12_c_fc",
          "layer": "layer_12",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_127",
          "type": "Linear",
          "name": "layer12_c_proj_mlp",
          "layer": "layer_12",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_128",
          "type": "LayerNorm",
          "name": "layer12_ln_1",
          "layer": "layer_12",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_129",
          "type": "LayerNorm",
          "name": "layer12_ln_2",
          "layer": "layer_12",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_130",
          "type": "Softmax",
          "name": "layer12_attention_softmax",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_131",
          "type": "GELU",
          "name": "layer12_activation",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_132",
          "type": "Dropout",
          "name": "layer12_attn_dropout",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_133",
          "type": "Dropout",
          "name": "layer12_resid_dropout",
          "layer": "layer_12",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_134",
          "type": "Linear",
          "name": "layer13_c_attn",
          "layer": "layer_13",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_135",
          "type": "Linear",
          "name": "layer13_c_proj",
          "layer": "layer_13",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_136",
          "type": "Linear",
          "name": "layer13_c_fc",
          "layer": "layer_13",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_137",
          "type": "Linear",
          "name": "layer13_c_proj_mlp",
          "layer": "layer_13",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_138",
          "type": "LayerNorm",
          "name": "layer13_ln_1",
          "layer": "layer_13",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_139",
          "type": "LayerNorm",
          "name": "layer13_ln_2",
          "layer": "layer_13",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_140",
          "type": "Softmax",
          "name": "layer13_attention_softmax",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_141",
          "type": "GELU",
          "name": "layer13_activation",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_142",
          "type": "Dropout",
          "name": "layer13_attn_dropout",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_143",
          "type": "Dropout",
          "name": "layer13_resid_dropout",
          "layer": "layer_13",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_144",
          "type": "Linear",
          "name": "layer14_c_attn",
          "layer": "layer_14",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_145",
          "type": "Linear",
          "name": "layer14_c_proj",
          "layer": "layer_14",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_146",
          "type": "Linear",
          "name": "layer14_c_fc",
          "layer": "layer_14",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_147",
          "type": "Linear",
          "name": "layer14_c_proj_mlp",
          "layer": "layer_14",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_148",
          "type": "LayerNorm",
          "name": "layer14_ln_1",
          "layer": "layer_14",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_149",
          "type": "LayerNorm",
          "name": "layer14_ln_2",
          "layer": "layer_14",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_150",
          "type": "Softmax",
          "name": "layer14_attention_softmax",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_151",
          "type": "GELU",
          "name": "layer14_activation",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_152",
          "type": "Dropout",
          "name": "layer14_attn_dropout",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_153",
          "type": "Dropout",
          "name": "layer14_resid_dropout",
          "layer": "layer_14",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_154",
          "type": "Linear",
          "name": "layer15_c_attn",
          "layer": "layer_15",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_155",
          "type": "Linear",
          "name": "layer15_c_proj",
          "layer": "layer_15",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_156",
          "type": "Linear",
          "name": "layer15_c_fc",
          "layer": "layer_15",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_157",
          "type": "Linear",
          "name": "layer15_c_proj_mlp",
          "layer": "layer_15",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_158",
          "type": "LayerNorm",
          "name": "layer15_ln_1",
          "layer": "layer_15",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_159",
          "type": "LayerNorm",
          "name": "layer15_ln_2",
          "layer": "layer_15",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_160",
          "type": "Softmax",
          "name": "layer15_attention_softmax",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_161",
          "type": "GELU",
          "name": "layer15_activation",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_162",
          "type": "Dropout",
          "name": "layer15_attn_dropout",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_163",
          "type": "Dropout",
          "name": "layer15_resid_dropout",
          "layer": "layer_15",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_164",
          "type": "Linear",
          "name": "layer16_c_attn",
          "layer": "layer_16",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_165",
          "type": "Linear",
          "name": "layer16_c_proj",
          "layer": "layer_16",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_166",
          "type": "Linear",
          "name": "layer16_c_fc",
          "layer": "layer_16",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_167",
          "type": "Linear",
          "name": "layer16_c_proj_mlp",
          "layer": "layer_16",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_168",
          "type": "LayerNorm",
          "name": "layer16_ln_1",
          "layer": "layer_16",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_169",
          "type": "LayerNorm",
          "name": "layer16_ln_2",
          "layer": "layer_16",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_170",
          "type": "Softmax",
          "name": "layer16_attention_softmax",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_171",
          "type": "GELU",
          "name": "layer16_activation",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_172",
          "type": "Dropout",
          "name": "layer16_attn_dropout",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_173",
          "type": "Dropout",
          "name": "layer16_resid_dropout",
          "layer": "layer_16",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_174",
          "type": "Linear",
          "name": "layer17_c_attn",
          "layer": "layer_17",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_175",
          "type": "Linear",
          "name": "layer17_c_proj",
          "layer": "layer_17",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_176",
          "type": "Linear",
          "name": "layer17_c_fc",
          "layer": "layer_17",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_177",
          "type": "Linear",
          "name": "layer17_c_proj_mlp",
          "layer": "layer_17",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_178",
          "type": "LayerNorm",
          "name": "layer17_ln_1",
          "layer": "layer_17",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_179",
          "type": "LayerNorm",
          "name": "layer17_ln_2",
          "layer": "layer_17",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_180",
          "type": "Softmax",
          "name": "layer17_attention_softmax",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_181",
          "type": "GELU",
          "name": "layer17_activation",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_182",
          "type": "Dropout",
          "name": "layer17_attn_dropout",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_183",
          "type": "Dropout",
          "name": "layer17_resid_dropout",
          "layer": "layer_17",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_184",
          "type": "Linear",
          "name": "layer18_c_attn",
          "layer": "layer_18",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_185",
          "type": "Linear",
          "name": "layer18_c_proj",
          "layer": "layer_18",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_186",
          "type": "Linear",
          "name": "layer18_c_fc",
          "layer": "layer_18",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_187",
          "type": "Linear",
          "name": "layer18_c_proj_mlp",
          "layer": "layer_18",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_188",
          "type": "LayerNorm",
          "name": "layer18_ln_1",
          "layer": "layer_18",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_189",
          "type": "LayerNorm",
          "name": "layer18_ln_2",
          "layer": "layer_18",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_190",
          "type": "Softmax",
          "name": "layer18_attention_softmax",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_191",
          "type": "GELU",
          "name": "layer18_activation",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_192",
          "type": "Dropout",
          "name": "layer18_attn_dropout",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_193",
          "type": "Dropout",
          "name": "layer18_resid_dropout",
          "layer": "layer_18",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_194",
          "type": "Linear",
          "name": "layer19_c_attn",
          "layer": "layer_19",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_195",
          "type": "Linear",
          "name": "layer19_c_proj",
          "layer": "layer_19",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_196",
          "type": "Linear",
          "name": "layer19_c_fc",
          "layer": "layer_19",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_197",
          "type": "Linear",
          "name": "layer19_c_proj_mlp",
          "layer": "layer_19",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_198",
          "type": "LayerNorm",
          "name": "layer19_ln_1",
          "layer": "layer_19",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_199",
          "type": "LayerNorm",
          "name": "layer19_ln_2",
          "layer": "layer_19",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_200",
          "type": "Softmax",
          "name": "layer19_attention_softmax",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_201",
          "type": "GELU",
          "name": "layer19_activation",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_202",
          "type": "Dropout",
          "name": "layer19_attn_dropout",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_203",
          "type": "Dropout",
          "name": "layer19_resid_dropout",
          "layer": "layer_19",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_204",
          "type": "Linear",
          "name": "layer20_c_attn",
          "layer": "layer_20",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_205",
          "type": "Linear",
          "name": "layer20_c_proj",
          "layer": "layer_20",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_206",
          "type": "Linear",
          "name": "layer20_c_fc",
          "layer": "layer_20",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_207",
          "type": "Linear",
          "name": "layer20_c_proj_mlp",
          "layer": "layer_20",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_208",
          "type": "LayerNorm",
          "name": "layer20_ln_1",
          "layer": "layer_20",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_209",
          "type": "LayerNorm",
          "name": "layer20_ln_2",
          "layer": "layer_20",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_210",
          "type": "Softmax",
          "name": "layer20_attention_softmax",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_211",
          "type": "GELU",
          "name": "layer20_activation",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_212",
          "type": "Dropout",
          "name": "layer20_attn_dropout",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_213",
          "type": "Dropout",
          "name": "layer20_resid_dropout",
          "layer": "layer_20",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_214",
          "type": "Linear",
          "name": "layer21_c_attn",
          "layer": "layer_21",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_215",
          "type": "Linear",
          "name": "layer21_c_proj",
          "layer": "layer_21",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_216",
          "type": "Linear",
          "name": "layer21_c_fc",
          "layer": "layer_21",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_217",
          "type": "Linear",
          "name": "layer21_c_proj_mlp",
          "layer": "layer_21",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_218",
          "type": "LayerNorm",
          "name": "layer21_ln_1",
          "layer": "layer_21",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_219",
          "type": "LayerNorm",
          "name": "layer21_ln_2",
          "layer": "layer_21",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_220",
          "type": "Softmax",
          "name": "layer21_attention_softmax",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_221",
          "type": "GELU",
          "name": "layer21_activation",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_222",
          "type": "Dropout",
          "name": "layer21_attn_dropout",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_223",
          "type": "Dropout",
          "name": "layer21_resid_dropout",
          "layer": "layer_21",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_224",
          "type": "Linear",
          "name": "layer22_c_attn",
          "layer": "layer_22",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_225",
          "type": "Linear",
          "name": "layer22_c_proj",
          "layer": "layer_22",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_226",
          "type": "Linear",
          "name": "layer22_c_fc",
          "layer": "layer_22",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_227",
          "type": "Linear",
          "name": "layer22_c_proj_mlp",
          "layer": "layer_22",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_228",
          "type": "LayerNorm",
          "name": "layer22_ln_1",
          "layer": "layer_22",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_229",
          "type": "LayerNorm",
          "name": "layer22_ln_2",
          "layer": "layer_22",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_230",
          "type": "Softmax",
          "name": "layer22_attention_softmax",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_231",
          "type": "GELU",
          "name": "layer22_activation",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_232",
          "type": "Dropout",
          "name": "layer22_attn_dropout",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_233",
          "type": "Dropout",
          "name": "layer22_resid_dropout",
          "layer": "layer_22",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_234",
          "type": "Linear",
          "name": "layer23_c_attn",
          "layer": "layer_23",
          "parameters": 12582912,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_235",
          "type": "Linear",
          "name": "layer23_c_proj",
          "layer": "layer_23",
          "parameters": 4194304,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_236",
          "type": "Linear",
          "name": "layer23_c_fc",
          "layer": "layer_23",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_237",
          "type": "Linear",
          "name": "layer23_c_proj_mlp",
          "layer": "layer_23",
          "parameters": 16777216,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_238",
          "type": "LayerNorm",
          "name": "layer23_ln_1",
          "layer": "layer_23",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_239",
          "type": "LayerNorm",
          "name": "layer23_ln_2",
          "layer": "layer_23",
          "parameters": 4096,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_240",
          "type": "Softmax",
          "name": "layer23_attention_softmax",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_241",
          "type": "GELU",
          "name": "layer23_activation",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 8192]",
          "output_shape": "[batch, seq, 8192]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_242",
          "type": "Dropout",
          "name": "layer23_attn_dropout",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        },
        {
          "op_id": "GPT_Neo_1.3B_op_243",
          "type": "Dropout",
          "name": "layer23_resid_dropout",
          "layer": "layer_23",
          "parameters": 0,
          "input_shape": "[batch, seq, 2048]",
          "output_shape": "[batch, seq, 2048]"
        }
      ],
      "parameters": 1300000000
    },
    {
      "model_id": "BERT_Tiny",
      "name": "BERT-Tiny",
      "source": "huggingface/bert-tiny",
      "category": "NLP",
      "architecture": "bert_style",
      "num_params": 4400000,
      "num_layers": 2,
      "hidden_size": 128,
      "vocab_size": 30522,
      "intermediate_size": 512,
      "operator_count": 30,
      "operators": [
        {
          "op_id": "BERT_Tiny_op_1",
          "type": "Embedding",
          "name": "word_embeddings",
          "layer": "global",
          "parameters": 3906816,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 128]"
        },
        {
          "op_id": "BERT_Tiny_op_2",
          "type": "Embedding",
          "name": "position_embeddings",
          "layer": "global",
          "parameters": 3906816,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 128]"
        },
        {
          "op_id": "BERT_Tiny_op_3",
          "type": "Embedding",
          "name": "token_type_embeddings",
          "layer": "global",
          "parameters": 256,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 128]"
        },
        {
          "op_id": "BERT_Tiny_op_4",
          "type": "LayerNorm",
          "name": "embeddings_layernorm",
          "layer": "global",
          "parameters": 61044,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, 128]"
        },
        {
          "op_id": "BERT_Tiny_op_5",
          "type": "Linear",
          "name": "pooler_dense",
          "layer": "global",
          "parameters": 16384,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Tiny_op_6",
          "type": "Tanh",
          "name": "pooler_activation",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, 128]"
        },
        {
          "op_id": "BERT_Tiny_op_7",
          "type": "Linear",
          "name": "layer0_query",
          "layer": "layer_0",
          "parameters": 16384,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Tiny_op_8",
          "type": "Linear",
          "name": "layer0_key",
          "layer": "layer_0",
          "parameters": 16384,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Tiny_op_9",
          "type": "Linear",
          "name": "layer0_value",
          "layer": "layer_0",
          "parameters": 16384,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Tiny_op_10",
          "type": "Linear",
          "name": "layer0_dense_attention",
          "layer": "layer_0",
          "parameters": 16384,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Tiny_op_11",
          "type": "Linear",
          "name": "layer0_dense_intermediate",
          "layer": "layer_0",
          "parameters": 65536,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Tiny_op_12",
          "type": "Linear",
          "name": "layer0_dense_output",
          "layer": "layer_0",
          "parameters": 65536,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Tiny_op_13",
          "type": "LayerNorm",
          "name": "layer0_attention_layernorm",
          "layer": "layer_0",
          "parameters": 256,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, 128]"
        },
        {
          "op_id": "BERT_Tiny_op_14",
          "type": "LayerNorm",
          "name": "layer0_output_layernorm",
          "layer": "layer_0",
          "parameters": 256,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, 128]"
        },
        {
          "op_id": "BERT_Tiny_op_15",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Tiny_op_16",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "BERT_Tiny_op_17",
          "type": "Dropout",
          "name": "layer0_attention_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, 128]"
        },
        {
          "op_id": "BERT_Tiny_op_18",
          "type": "Dropout",
          "name": "layer0_output_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, 128]"
        },
        {
          "op_id": "BERT_Tiny_op_19",
          "type": "Linear",
          "name": "layer1_query",
          "layer": "layer_1",
          "parameters": 16384,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Tiny_op_20",
          "type": "Linear",
          "name": "layer1_key",
          "layer": "layer_1",
          "parameters": 16384,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Tiny_op_21",
          "type": "Linear",
          "name": "layer1_value",
          "layer": "layer_1",
          "parameters": 16384,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Tiny_op_22",
          "type": "Linear",
          "name": "layer1_dense_attention",
          "layer": "layer_1",
          "parameters": 16384,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Tiny_op_23",
          "type": "Linear",
          "name": "layer1_dense_intermediate",
          "layer": "layer_1",
          "parameters": 65536,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Tiny_op_24",
          "type": "Linear",
          "name": "layer1_dense_output",
          "layer": "layer_1",
          "parameters": 65536,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Tiny_op_25",
          "type": "LayerNorm",
          "name": "layer1_attention_layernorm",
          "layer": "layer_1",
          "parameters": 256,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, 128]"
        },
        {
          "op_id": "BERT_Tiny_op_26",
          "type": "LayerNorm",
          "name": "layer1_output_layernorm",
          "layer": "layer_1",
          "parameters": 256,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, 128]"
        },
        {
          "op_id": "BERT_Tiny_op_27",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Tiny_op_28",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 512]",
          "output_shape": "[batch, seq, 512]"
        },
        {
          "op_id": "BERT_Tiny_op_29",
          "type": "Dropout",
          "name": "layer1_attention_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, 128]"
        },
        {
          "op_id": "BERT_Tiny_op_30",
          "type": "Dropout",
          "name": "layer1_output_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 128]",
          "output_shape": "[batch, seq, 128]"
        }
      ],
      "parameters": 4400000
    },
    {
      "model_id": "BERT_Mini",
      "name": "BERT-Mini",
      "source": "huggingface/bert-mini",
      "category": "NLP",
      "architecture": "bert_style",
      "num_params": 11300000,
      "num_layers": 4,
      "hidden_size": 256,
      "vocab_size": 30522,
      "intermediate_size": 1024,
      "operator_count": 54,
      "operators": [
        {
          "op_id": "BERT_Mini_op_1",
          "type": "Embedding",
          "name": "word_embeddings",
          "layer": "global",
          "parameters": 7813632,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_2",
          "type": "Embedding",
          "name": "position_embeddings",
          "layer": "global",
          "parameters": 7813632,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_3",
          "type": "Embedding",
          "name": "token_type_embeddings",
          "layer": "global",
          "parameters": 512,
          "input_shape": "[batch, seq]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_4",
          "type": "LayerNorm",
          "name": "embeddings_layernorm",
          "layer": "global",
          "parameters": 61044,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_5",
          "type": "Linear",
          "name": "pooler_dense",
          "layer": "global",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_6",
          "type": "Tanh",
          "name": "pooler_activation",
          "layer": "global",
          "parameters": 0,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_7",
          "type": "Linear",
          "name": "layer0_query",
          "layer": "layer_0",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_8",
          "type": "Linear",
          "name": "layer0_key",
          "layer": "layer_0",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_9",
          "type": "Linear",
          "name": "layer0_value",
          "layer": "layer_0",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_10",
          "type": "Linear",
          "name": "layer0_dense_attention",
          "layer": "layer_0",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_11",
          "type": "Linear",
          "name": "layer0_dense_intermediate",
          "layer": "layer_0",
          "parameters": 262144,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_12",
          "type": "Linear",
          "name": "layer0_dense_output",
          "layer": "layer_0",
          "parameters": 262144,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_13",
          "type": "LayerNorm",
          "name": "layer0_attention_layernorm",
          "layer": "layer_0",
          "parameters": 512,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_14",
          "type": "LayerNorm",
          "name": "layer0_output_layernorm",
          "layer": "layer_0",
          "parameters": 512,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_15",
          "type": "Softmax",
          "name": "layer0_attention_softmax",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Mini_op_16",
          "type": "GELU",
          "name": "layer0_activation",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BERT_Mini_op_17",
          "type": "Dropout",
          "name": "layer0_attention_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_18",
          "type": "Dropout",
          "name": "layer0_output_dropout",
          "layer": "layer_0",
          "parameters": 0,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_19",
          "type": "Linear",
          "name": "layer1_query",
          "layer": "layer_1",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_20",
          "type": "Linear",
          "name": "layer1_key",
          "layer": "layer_1",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_21",
          "type": "Linear",
          "name": "layer1_value",
          "layer": "layer_1",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_22",
          "type": "Linear",
          "name": "layer1_dense_attention",
          "layer": "layer_1",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_23",
          "type": "Linear",
          "name": "layer1_dense_intermediate",
          "layer": "layer_1",
          "parameters": 262144,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_24",
          "type": "Linear",
          "name": "layer1_dense_output",
          "layer": "layer_1",
          "parameters": 262144,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_25",
          "type": "LayerNorm",
          "name": "layer1_attention_layernorm",
          "layer": "layer_1",
          "parameters": 512,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_26",
          "type": "LayerNorm",
          "name": "layer1_output_layernorm",
          "layer": "layer_1",
          "parameters": 512,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_27",
          "type": "Softmax",
          "name": "layer1_attention_softmax",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Mini_op_28",
          "type": "GELU",
          "name": "layer1_activation",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BERT_Mini_op_29",
          "type": "Dropout",
          "name": "layer1_attention_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_30",
          "type": "Dropout",
          "name": "layer1_output_dropout",
          "layer": "layer_1",
          "parameters": 0,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_31",
          "type": "Linear",
          "name": "layer2_query",
          "layer": "layer_2",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_32",
          "type": "Linear",
          "name": "layer2_key",
          "layer": "layer_2",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_33",
          "type": "Linear",
          "name": "layer2_value",
          "layer": "layer_2",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_34",
          "type": "Linear",
          "name": "layer2_dense_attention",
          "layer": "layer_2",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_35",
          "type": "Linear",
          "name": "layer2_dense_intermediate",
          "layer": "layer_2",
          "parameters": 262144,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_36",
          "type": "Linear",
          "name": "layer2_dense_output",
          "layer": "layer_2",
          "parameters": 262144,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_37",
          "type": "LayerNorm",
          "name": "layer2_attention_layernorm",
          "layer": "layer_2",
          "parameters": 512,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_38",
          "type": "LayerNorm",
          "name": "layer2_output_layernorm",
          "layer": "layer_2",
          "parameters": 512,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_39",
          "type": "Softmax",
          "name": "layer2_attention_softmax",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Mini_op_40",
          "type": "GELU",
          "name": "layer2_activation",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BERT_Mini_op_41",
          "type": "Dropout",
          "name": "layer2_attention_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_42",
          "type": "Dropout",
          "name": "layer2_output_dropout",
          "layer": "layer_2",
          "parameters": 0,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_43",
          "type": "Linear",
          "name": "layer3_query",
          "layer": "layer_3",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_44",
          "type": "Linear",
          "name": "layer3_key",
          "layer": "layer_3",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_45",
          "type": "Linear",
          "name": "layer3_value",
          "layer": "layer_3",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_46",
          "type": "Linear",
          "name": "layer3_dense_attention",
          "layer": "layer_3",
          "parameters": 65536,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_47",
          "type": "Linear",
          "name": "layer3_dense_intermediate",
          "layer": "layer_3",
          "parameters": 262144,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_48",
          "type": "Linear",
          "name": "layer3_dense_output",
          "layer": "layer_3",
          "parameters": 262144,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, out_features]"
        },
        {
          "op_id": "BERT_Mini_op_49",
          "type": "LayerNorm",
          "name": "layer3_attention_layernorm",
          "layer": "layer_3",
          "parameters": 512,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_50",
          "type": "LayerNorm",
          "name": "layer3_output_layernorm",
          "layer": "layer_3",
          "parameters": 512,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_51",
          "type": "Softmax",
          "name": "layer3_attention_softmax",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, heads, seq, seq]",
          "output_shape": "[batch, heads, seq, seq]"
        },
        {
          "op_id": "BERT_Mini_op_52",
          "type": "GELU",
          "name": "layer3_activation",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 1024]",
          "output_shape": "[batch, seq, 1024]"
        },
        {
          "op_id": "BERT_Mini_op_53",
          "type": "Dropout",
          "name": "layer3_attention_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        },
        {
          "op_id": "BERT_Mini_op_54",
          "type": "Dropout",
          "name": "layer3_output_dropout",
          "layer": "layer_3",
          "parameters": 0,
          "input_shape": "[batch, seq, 256]",
          "output_shape": "[batch, seq, 256]"
        }
      ],
      "parameters": 11300000
    }
  ]
}